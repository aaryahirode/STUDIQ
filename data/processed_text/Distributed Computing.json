{
  "subject": "Distributed Computing",
  "files": [
    {
      "filename": "BE_Comps_DC_Week2.pdf",
      "path": "data/materials\\Distributed Computing\\BE_Comps_DC_Week2.pdf",
      "text": "Subject Name: DISTRIBUTED COMPUTING\nUnit No: 1 Unit Name: Introduction to\ndistributed System\nFaculty Name:\nMrs.BhavanaAlte\nIndex\n\n\n\n\n\n\n\n\n\nmiddleware system\n\n\n\n\nUnit No: 1 Unit name:Introduction to Distributed Systems\nLectureNo:4\nHardware concept,Software\nconcept\nHardware Concepts\nDistributed system consist of multiple CPUs and memory, there are several different\nways the hardware can be organized in terms of how they are interconnected and\nhow they communicate.\n\n\nMULTIPROCESSOR\nMULTICO\nMPUTER\nHardware Concepts Classification\n-\nMulti Processor and Multi Computer\nMulti Processor-Those have shared memory(Single Physical address space)\nMulti Computer-has its own private memory\n-\nBus and Switched\nBus-Single network, backplane, cable that connect all the machine.\nSwitch-There are individual wires from machine to machine with many different\nwiring pattern\n-\nHomogenous and Heterogeneous (Distinction for only Multicomputer)\nHomogenous-use same technology for interconnection, same access to private\nmemory( generally working on same problem)\nHeterogeneous-Different\nindependent\ncomputer\nconnected\nthrough\ndifferent\nnetwork through switch\n\n\nA bus-based multiprocessor\n-\nDirect access to shared memory\n-\nConnected to a common bus\n-\nMemory should maintain coherent (Consistent)property\n-\nWith increase of CPU performance drop\n-\nTo increase performance, High-speed cache memory is used between CPU and\nbus.\n-\nIt store recent data\n-\nCan lead to incoherent memory and need replication mechanisms.\nMultiprocessors (1)\n\n\nMultiprocessors (2)\n\n\nA Switch-based multiprocessor\n- for more than 256 CPU\n- Memory is divided in modules\na) A crossbar switch-Each CPU and each memory\nare connected.\n- Switch closed when one CPU access the memory.\nRest CPU waits for switch to open.\n- For n CPU and n memory,n^2 switches are\nneeded.(network is limited to less value of n)\nb) An omega switching network\n- In fig network contain 2X2 switch, each having 2\ninput and 2 output\n- Disadvantage- There may be several switching\nstages between the CPU\nIn CPU-CPU communication, the volume of traffic is of lower magnitude than for\nCPU-Memory traffic.\n-\nSystem Area Networks(SANs) ‚Äìthe nodes are mounted in a big rack and\nconnected through single, high-performance network.\n-\nBus-based Multicomputer(20-100 nodes)- Connected through shared multi-\naccess network such as Fast Ethernet.(message is broadcast )\n-\nSwitch-based Multicomputer- (message is routed)\na)\nGrid: 2-dimensional network\nb)\nHypercube: n-dimensional cube , each vertex is CPU\nHomogeneous Multicomputer Systems\n\n\nAn overview of\n-DOS (Distributed Operating Systems)\n-NOS (Network Operating Systems)\n-Middleware\nSoftware Concepts\n\n\nComputer architectures consisting of interconnected, multiple processors are\nbasically of two types:\n-\nTightly coupled systems\n-\nLoosely coupled systems\nComputer Architecture\n\n\n-\nIn these systems, there is a single system wide primary memory (address space)\nthat is shared by all the processors.\n-\nIf any processor writes, for example, the value 100 to the memory location x, any\nother processor subsequently reading from location x will get the value 100.\n-\nTherefore, in these systems, any communication between the processors usually\ntakes place through the shared memory.\nTightly Coupled System\n\n\n\n-\nIn these systems, the processors do not share memory, and each processor has\nits own local memory.\n-\nIf a processor writes the value 100 to the memory location x, this write operation\nwill only change the contents of its local memory and will not affect the contents\nof the memory.\n-\nIn these systems, all physical communication between the processors is done by\npassing messages across the network that interconnects the processors.\nLoosely coupled systems\n\n\n\nSoftware Concepts\nSystem\nDescription\nMain Goal\nDistributed\nOperating\nSystem (DOS)\nTightly-coupled operating system for multi-\nprocessors and homogeneous\nmulticomputers\nHide and manage\nhardware resources\nNetwork\nOperating\nSystem (NOS)\nLoosely-coupled operating system for\nheterogeneous multicomputers (LAN and\nWAN)\nOffer local services\nto remote clients\nMiddleware\nAdditional layer atop of NOS implementing\ngeneral-purpose services\nProvide distribution\ntransparency\n\n\n1. Distributed Operating Systems (1)\n1.11\n\n\n-\nDOS functionality is same as traditional OS for uniprocessor system, except that\nthey handle multiple CPUs.\n-\nOS should have full control on how the hardware resource are used and shared\nUniprocessor Operating System\n1. Distributed Operating Systems (2)\n\n\n-\nAccess to Shared Memory\n-\nHigh performance\n-\nNo of CPUs are transparent to application\nMultiprocessor Operating System\nGeneral structure of a homogeneous multicomputers operating system\n1. Distributed Operating Systems (3)\n1.14\n\n\nMulticomputer Operating Systems\nAlternatives for blocking and buffering in message passing.\nS1: Block sender when buffer is full\nS2: Message being sent\nS3: Message is arrived\nS4: Message is sent to receiver\n1. Distributed Operating Systems (4)\n\n\nRelation between blocking, buffering, and reliable communications.\n1. Distributed Operating Systems (5)\nSynchronization point\nSend buffer\nReliable comm.\nguaranteed?\nBlock sender until buffer not full\nYes\nNot necessary\nBlock sender until message sent\nNo\nNot necessary\nBlock sender until message received\nNo\nNecessary\nBlock sender until message delivered\nNo\nNecessary\n\n\n-\nProgramming\na\nmulticomputer\nis\nmuch\nharder\nthan\nprogramming\nmultiprocessor.\n-\nFor communication to access shared data process in Multiprocessor used\nsemaphore and monitor for synchronization which is easier than message\npassing.\n-\nBuffering, blocking and reliable communication make things worse.\n-\nTo avoid this researcher provide virtual shared memory to multicomputer.\n-\nWhere user access large virtual address space which lead to page-based\ndistributed shared memory(DSM)\n1. Distributed Operating Systems (6)\n\n\na)\nPages (of size 4KB or 8KB) of address space\ndistributed among four machines\nb)\nCPU1 Reference instruction 0,2,5,9, the\nreferences are done locally\nc)\nSituation after CPU 1 references page 10 is\nnot present locally, a trap occurs the OS\nfetch the page containing the address and\nmove it from CPU2 to CPU1\nd)\nThis is normal paging concept, except that\nremote RAM is used instead of local disk.\ne)\nSituation if page 10 is read only and\nreplication is used to increase\nperformance.\n1. Distributed Operating Systems (7)\n\n\nDistributed Shared Memory Systems\n-\nIssue with DSM is to decide how large page should be.\n-\nIf page size is large, total number of transfer will be reduce which increase performance\n-\nA page contain two independent processes, OS need to consequently transfer the page\nbetween two processor Known as False sharing\n1. Distributed Operating Systems (8)\n1.18\n\n\n-\nHeterogeneous system\n-\nServices Provided by NOS\n-\nUser can remotely login to another machine\n-\nTransfer of file\n-\nGlobal file server accessible to all\n2. Network Operating System (1)\n1-19\n\n\n-\nTwo clients and a server in a network operating system.\n-\nFile server maintain hierarchical file system each with a root directory containing\nsubdirectories and files.\n2. Network Operating System (2)\n1-20\n\n\nDifferent clients may mount the servers in different places.\n2. Network Operating System (3)\n1.21\n\n\n-\nDOS provide full transparency\n-\nNOS lacks in maintaining transparency\n-\nDisadvantage of NOS\n-\nIts hard to login to every machine in network\n-\nCant use same password for every machine\n-\nProtection against malicious attack\n-\nAdvantage of NOS\n-\nEasy to add or remove machine/Server\n-\nEasy to communicate existence of new machine to other\n-\nNeither DOS or NOS qualifies as Distributed system definition\n-\nWe need Openness and Scalability of NOS\n-\nAnd Transparency of DOS\nDifference between DOS and NOS\n\n\n3. Middleware(1)\n\n\nIn an open middleware-based distributed system, the protocols used by each\nmiddleware layer should be the same, as well as the interfaces they offer to\napplications.\nMiddleware\n\n\nThere are many services offered by middleware system\n1.Naming service\n2.Persistence service\n3.Message service\n4.Querying service\n5.Concurrency service\n6.Security service\nServices offered by Middleware System\n\nMiddleware system\n\nComparison between Systems\nItem\nDistributed OS\nNetwork\nOS\nMiddleware-\nbased OS\nMultiproc.\nMulticomp.\nDegree of transparency\nVery High\nHigh\nLow\nHigh\nSame OS on all nodes\nYes\nYes\nNo\nNo\nNumber of copies of OS\n\nN\nN\nN\nBasis for\ncommunication\nShared\nmemory\nMessages\nFiles\nModel specific\nResource management\nGlobal,\ncentral\nGlobal,\ndistributed\nPer node\nPer node\nScalability\nNo\nModerately\nYes\nVaries\nOpenness\nClosed\nClosed\nOpen\nOpen\n\n\nThank You"
    },
    {
      "filename": "BE_Comps_DC_Week6_L21_L24.pdf",
      "path": "data/materials\\Distributed Computing\\BE_Comps_DC_Week6_L21_L24.pdf",
      "text": "Subject Name: Distributed Computing\nCourse Code: CSC802\nUnit No: 03\nUnit Name: Synchronization\nFaculty Name Ms. BhavanaAlte\nUnit No 3: Synchronization\nLecture No: 21 Ricart‚Äì\nAgrawala‚ÄòsAlgorithm\nThe Ricart-Agrawala algorithm is an optimization of Lamport's algorithm that\ndispenses with RELEASE messages by cleverly merging them with REPLY\nmessages. In this algorithm also, ‚àÄi : 1 ‚â§i ‚â§N : Ri = {S1,S2,...,SN}.\n\nLecture No: 21 Ricart‚ÄìAgrawala‚ÄòsAlgorithm\nRicart-AgrawalaAlgorithm\nRicart-AgrawalaAlgorithm\nAlgorithm\nRequesting the critical section.\n1.\nWhen a site Si wants to enter the CS, it sends a timestamped\nREQUEST message to all the sites in its request set.\n2.\nWhen site Sj receives a REQUEST message from site Si, it sends\na REPLY message to site Si if site Sj is neither requesting nor\nexecuting the CS or if site Sj is requesting and S'is request's\ntimestamp is smaller than Sj's own request's timestamp. The\nrequest is deferred otherwise.\nExecuting the critical section\n1.\nSite Si enters the CS after it has received REPLY messages from\nall the sites in its request set.\nReleasing the critical section\n1. When site Si exits the CS, it sends REPLY messages to all the\ndeferred requests.\n\nLecture No: 21 Ricart‚ÄìAgrawala‚ÄòsAlgorithm\nRicart-AgrawalaAlgorithm\n\nLecture No: 21 Ricart‚ÄìAgrawala‚ÄòsAlgorithm\nRicart-Agrawala: Example\n(2,1)\nS1\n(2,1)\nS2\nS3\nS2\n(1,2)\nS3\nStep 2:\nS1\n\nLecture No: 21 Ricart‚ÄìAgrawala‚ÄòsAlgorithm\nStep 1:\nS2 enters CS\nRicart-Agrawala: Example‚Ä¶\n(2,1)\nS2\nS3\nStep 3:\nS1\nS1 enters CS\nS2 leaves CS\n\nLecture No: 21 Ricart‚ÄìAgrawala‚ÄòsAlgorithm\nPerformance:\n2(N-1) messages per CS execution. (N-1) REQUEST +(N-1)\nREPLY.\nSynchronization delay: T.\nOptimization:\nWhen Si receives REPLY message from Sj -> authorization to\naccess CS till\nSj sends a REQUEST message and Si sends a REPLY\nmessage.\nAccess CS repeatedly till then.\nA site requests permission from dynamically varying setof\nsites: 0 to 2(N-1) messages.\n\nLecture No: 21 Ricart‚ÄìAgrawala‚ÄòsAlgorithm\nRicart-Agrawala: Performance\nUnit No 3: Synchronization\nLecture No: 22\nMaekawa‚Äòs Algorithm\nMaekawa‚ÄôsAlgorithm\n- In Maekawa‚Äôs algorithm, a process request permission,\nonly from a subset of sites instead of all sites.\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\n- To\nprevent\ntwo\nprocess\nto\nobtain\nall\nnecessary\npermissions for CS, these subsets of process must have\noverlaps.\n- Each site receiving request message serves as a\nmediator that ensures only one process under its watch\ncan be given permission for CS at a time.\nThe Construction of request sets. The request sets for\nprocess in Maekawa's algorithm are constructed to satisfy\nthe following conditions:\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\nM1: (‚àÄi ‚àÄj : i‚â†j, 1 ‚â§ i, j ‚â§ N :: Ri ‚à© Rj ‚â† Œ¶ ).\nM2: ((‚àÄi : 1 ‚â§ i ‚â§ N:: Pi ‚ààRi)\nM3: (((‚àÄi : 1 ‚â§ i ‚â§ N:: |Ri | = K)\nM4: Any process Pj is contained in K number of Ris, 1 ‚â§ i, j\n‚â§N.\nMaekawa‚ÄôsAlgorithm\nThe data structure used by each process Pi\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\n1. The request-deferred queue, RDi\n2. A variable called ‚Äòvoted‚Äô=False is set initially\nVoted is set to true when a reply is sent indicating that it\nhas already granted permission to a process in its quorum.\nMaekawa‚ÄôsAlgorithm\nMaekawa‚ÄôsAlgorithm\nP1\nP2\nP3\nP4\nN=4 R=4 K=2\nR1={P1,P2}\nR2={P2,P3}\nR3={P3,P4}\nR4={P4,P1}\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\nMaekawa‚ÄôsAlgorithm\nN=4 R=4 K=3\nR1={P1,P2,P4}\nR2={P2,P1,P3}\nR3={P3,P2,P4}\nR4={P4,P1,P3}\nP1\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\nP2\nP3\nP4\nMaekawa‚ÄôsAlgorithm\nThe Algorithm\nRequesting the critical section.\n1.\nA proces Pi requests access to the CS by sendingREQUEST(i)\nmessages to all the process in its request set Ri.\n2.\nWhen a proces Pj receives the REQUEST(i) message, it sends a\nREPLY(j) message to Pi, provided it hasn't sent a REPLY message\nto a process from the time it received the last RELEASE message.\nOtherwise, it queues up the REQUEST for later consideration.\nExecuting the critical section.\n1.\nproces Pi accesses the CS only after receiving REPLY messages\nfrom all the process in Ri.\nReleasing the critical section.\n1.\nAfter the execution of the CS is over, site Pi sends RELEASE(i)\nmessage to all the process in Ri.\n2.\nWhen a proces Pj receives a RELEASE(i) message from proces Pi,\nit sends a REPLY message to the next process waiting in the\nqueue and deletes that entry from the queue. If the queue isempty,\nthen proces updates its state to reflect that the process has not\nsent out any REPLY message.\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\nPerformance\nSynchronization delay: 2T\nMessages: 3\nmessages)\nDeadlocks\nùëÅ(one each for REQUEST, REPLY,RELEASE\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\nMessage deliveries are not ordered which will lead to deadlock\nMaekawa‚Äôs Algorithm...\nPi yields to a request if that has a smaller time stamp.\nA site suspects a deadlock when it is locked by a request\nwith a higher time stamp (lower priority).\nDeadlock handling messages:\nFAILED: from Pi to Pj -> Pi has granted permission to higher\npriority request.\nINQUIRE: from Pi to Pj -> Pi would like to know Pj has succeeded\nin locking all sites in Pj‚Äôs request set.\nYIELD: from Pi to Pj -> Pi is returning permission to Pj so that Pj\ncan yield to a higher priority request.\n\nLecture No: 22 Maekawa‚ÄòsAlgorithm\nHandling Deadlocks\nUnit No 3: Synchronization\nLecture No: 23\nToken Based Algorithms:\nSuzuki-Kasami‚Äòs Broardcast\nAlgorithms\n-if a process wants to enter the critical section, and it does not have the token,\nit broadcasts a request message to all other processes in the system\n-the process that has the token will then send it to the requesting process\n-However, if it is in CS, it gets to finish before sending the token\n-a process holding the token can continuously enter the critical section until the\ntoken is requested\n-request vector at process i :\n-RNi [k] contains the largest sequence number received from process k in a\nrequest message\n-token consists of vector and a queue:\n-LN[k] contains the sequence number of the latest executed request from\nprocess k\n-Q is the queue of requesting process\n\n\nSuzuki-Kasami‚Äôs BroadcastAlgorithm\n-requesting the CS:\n-when a process i wants to enter the CS, if it does not have the token, it:\n-increments its sequence number RNi [i]\n-sends a request message containing new sequence number to all\nprocesses in the system\n-when a process k receives the request(i,sn) message, it:\n-Sets RNk [i] to MAX(RNk [i],sn)\n-if process k has the token and is not in CS (i.e., is not using token),\nand if RNk [i] == LN[i]+1 (indicating an outstanding request)\nit sends the token to process i\n-executing the CS:\n-a process enters the CS when it acquires the token\n\n\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs BroadcastAlgorithm\n(cont.)\n-releasing the CS:\n-when a process i leaves the CS, it:\n-sets LN[i] of the token equal to RNi [i]\n-for every process k whose ID is not in the token queue Q, it appends its\nID to Q if RNi [k] == LN[k]+1\n-if the token queue Q is nonempty after this update, it deletes the process\nID at the head of Q and sends the token to that process\n-evaluation:\n-N messages required to enter CS\n-otherwise (N-1) requests, 1 reply\n-note, no messages needed if process holds the token\n-synchronization delay ‚Äì T\n\n\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs BroadcastAlgorithm\n(cont.)\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs Broadcast Algorithm:\nExample\n\n\nUnit No 3: Synchronization\nLecture No: 24\nToken Based Algorithms:\nSuzuki-Kasami‚Äòs Broardcast\nAlgorithms\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs Broadcast Algorithm:\nExample\n\n\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs Broadcast Algorithm:\nExample\n\n\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs Broadcast Algorithm:\nExample\n\n\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs Broadcast Algorithm:\nExample\n\n\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs Broadcast Algorithm:\nExample\n\n\nDistributed Mutual Exclusion: Suzuki-Kasami‚Äôs Broadcast Algorithm:\nExample\n\n\nThank You"
    },
    {
      "filename": "DC_Week1.pdf",
      "path": "data/materials\\Distributed Computing\\DC_Week1.pdf",
      "text": "Subject Name: DISTRIBUTED COMPUTING\nUnit No: 1 Unit Name: Introduction to\ndistributed System\nFaculty Name:\nMrs. . HarshaSaxena\nMrs.BhavanaAlte\nMs.DhanashriBhosale\nIndex\n\n\n\n\n\n\n\n\n\noffered by middleware system\n\n\n\n\nUnit No: 1\nUnit name:Introduction to Distributed Systems\nLectureNo:1\nIntroduction to Distributed\nIntroduction to Distributed\nSystems\nComputer architectures consisting of interconnected, multiple processors are basically\nof two types:\n-\nTightly coupled systems\nComputer Architecture\n-\nLoosely coupled systems\n\n\n-\nIn these systems, there is a single system wide primary memory (address space) that\nis shared by all the processors.\n-\nIf any processor writes, for example, the value 100 to the memory location x, any\nother processor subsequently reading from location x will get the value 100.\n-\nTherefore, in these systems, any communication between the processors usually\ntakes place through the shared memory.\nTightly Coupled System\n\n\n\n-\nIn these systems, the processors do not share memory, and each processor has its\nown local memory.\n-\nIf a processor writes the value 100 to the memory location x, this write operation\nwill only change the contents of its local memory and will not affect the contents of\nthe memory.\n-\nIn these systems, all physical communication between the processors is done by\npassing messages across the network that interconnects the processors.\nLoosely coupled systems\npassing messages across the network that interconnects the processors.\n\n\n\nA distributed system is:\nA collection of independent computers that appears to its users as a single\ncoherent system.\nDefinition of a Distributed System\n-\nDefinition meaning: The first part deal with Hardware: the machine are\nautonomous. A second aspect deal with Software: the user (be they people\nor programs) think they are dealing with a single system.\n-\nThe operating systems used for Distributed computing systems can be\nbroadly classified into two type‚Äôs network operating systems and distributed\noperating systems.\n\n\nÔÇ∑\nTightly coupled systems are referred to as parallel processing systems, and loosely\ncoupled systems are referred to as distributed computing systems, or simply distributed\nsystems.\nÔÇ∑\n(No of systems) In contrast to the tightly coupled systems, the processor of distributed\ncomputing systems can be located far from each other to cover a wider geographical\narea. Furthermore, in tightly coupled systems, the number of processors that can be\nusefully deployed is usually small and limited by the bandwidth of the shared memory.\nThis is not the case with distributed computing systems that are more freely expandable\nDifference between tightly coupled and loosely coupled\nmultiprocessor systems\nThis is not the case with distributed computing systems that are more freely expandable\nand can have an almost unlimited number of processors.\nÔÇ∑\nIn short, a distributed computing system is basically a collection of processors\ninterconnected by a communication network in which each processor has its own local\nmemory and other peripherals, and the communication between any two processors of\nthe system takes place by message passing over the communication network.\nÔÇ∑\nFor a particular processor, its own resources are local, whereas the other processors and\ntheir resources are remote. Together, a processor and its resources are usually referred to\nas a node or site or machine of the distributed computing system.\n\n\nDifference between tightly coupled and loosely coupled\nmultiprocessor systems\nS.No\nParallel system\nDistributed System\n\nMemory\nTightly coupled shared\nmemory UMA,NUMA\nDistributed memory.\nMessage passing, RPC\nand/or DSM\n\nControl\nGlobal clock control\nSIMD MIMD\nNo global clock control.\nSynchronization\n\n\nalgorithms needed\n\nMain focus\nPerformance scientific\ncomputing\nPerformance (cost and\nscalability) Reliability/\nAvailability.\nInformation/ Resource\nsharing.\nEvolution of Distributed Computing System\nÔÇ∑\nComputer systems are undergoing a revolution. From 1945, when the modem Computer\nera began, until about 1985, computers were large and expensive. Even minicomputers\ncost at least tens of thousands of dollars each. As a result, most organizations had only a\nhandful of computers, and for lack of a way to connect them, these operated\nindependently from one another. Starting around the mid-1980s, however, two advances\nin technology began to change that situation.\n\n\nÔÇ∑\nThe first was the development of powerful microprocessors. Initially, these were 8-bit\nmachines, but soon 16-, 32-, and 64-bit CPUs became common. Many of these had the\ncomputing power of a mainframe (i.e., large) computer, but for a fraction of the price.\nThe amount of improvement that has occurred in computer technology in the past half\ncentury is truly staggering and totally unprecedented in other industries. From a machine\nthat cost 10 million dollars and executed 1 instruction per second. We have come to\nmachines that cost 1000 dollars and are able to execute 1 billion instructions per second,\na price/performance gain of 1013.\nEvolution of Distributed Computing System\n-\nThe second development was the invention of high-speed computer networks. Local-area\nnetworks or LANs allow hundreds of machines within a building to be connected in such a\nway that small amounts of information can be transferred between machines in a few\nmicroseconds or so. Larger amounts of data can be Distributed Computing become popular\nwith the difficulties of centralized processing in mainframe use.\n-\nWith mainframe software architectures all components are within a central host computer.\nUsers interact with the host through a terminal that captures keystrokes and sends that\n\n\nUsers interact with the host through a terminal that captures keystrokes and sends that\ninformation to the host. In the last decade however, mainframes have found a new use as a\nserver in distributed client/server architectures (Edelstein 1994). The original PC networks\n(which have largely superseded mainframes) were based on file sharing architectures,\nwhere the server transfers files from a shared location to a desktop environment.\nÔÇ∑\nIn the 1990s, PC LAN (local area network) computing changed because the capacity of\nthe file sharing was strained as the number of online users grew and graphical user\ninterfaces (GUIs) became popular (making mainframe and terminal displays appear out of\ndate).\nEvolution of Distributed Computing System\nÔÇ∑\nThe next major step in distributed computing came with separation of software\narchitecture into 2 or 3 tiers.\nÔÇ∑\nWith two tier client-server architectures, the GUI is usually located in the user's\ndesktop environment and the database management services are usually in a server\nthat is a more powerful machine that services many clients. Processing management\nis split between the user system interface environment and the database management\nserver environment. The two tier client/server architecture is a good solution for\n\n\nserver environment. The two tier client/server architecture is a good solution for\nlocally distributed computing when work groups are defined as a dozen to 100\npeople interacting on a LAN simultaneously. However, when the number of users\nexceeds 100, performance begins to deteriorate and the architecture is also difficult\nto scale.\nÔÇ∑\nThe three tier architecture (also referred to as the multi-tier architecture) emerged to\novercome the limitations of the two tier architecture. In the three tier architecture, a\nmiddle tier was added between the user system interface client environment and the\ndatabase management server environment.\nEvolution of Distributed\nComputing System\n1. Minicomputer\nModel\n-\nThe minicomputer model is a simple extension of the centralized time sharing system, a\n\n\ndistributed computing system based on this model consists of a few minicomputers (they\nmay be large supercomputers as well) interconnected by a communication network. Each\nminicomputer usually has multiple users simultaneously logged on to it. For this, several\ninteractive terminals are connected to each minicomputer.\n-\nEach user is logged on to one specific minicomputer, with remote access to other\nminicomputers. The network allows a user to access remote resources that are available on\nsome machine other than the one on to which the user is currently logged.\nEvolution of Distributed\nComputing System\n1. Minicomputer\nModel\n-\nThe minicomputer model may be used when resource sharing (Such as sharing of\n\n\n-\nThe minicomputer model may be used when resource sharing (Such as sharing of\ninformation databases of different types, with each type of database located on a different\nmachine) with remote users is desired.\n-\nThe early ARPAnet is an example of a distributed computing system based on the\nminicomputer model.\nEvolution of Distributed\nComputing System\n2. Workstation\nModel:\nÔÇ∑\na distributed computing system based on the workstation model consists of several\n\n\nÔÇ∑\na distributed computing system based on the workstation model consists of several\nworkstations interconnected by a communication network. A company‚Äôs office or a\nuniversity department may have several workstations scattered throughout a building or\ncampus, each workstation equipped with its own disk and serving as a single-user\ncomputer.\nÔÇ∑\nIt has been often found that in such an environment, at any one time (especially at night),\na significant proportion of the workstations are idle (not being used), resulting in the\nwaste of large amounts of CPU time.\nEvolution of Distributed\nComputing System\n2. Workstation\nModel:\nÔÇ∑\nTherefore, the idea of the workstation model is to interconnect all these workstations by a\nhigh speed LAN so that idle workstations may be used to process jobs of users who are\n\n\nhigh speed LAN so that idle workstations may be used to process jobs of users who are\nlogged onto other workstations and do not have sufficient processing power at their own\nworkstations to get their jobs processed efficiently.\nÔÇ∑\nIn this model, a user logs onto one of the workstations called his or her ‚Äúhome‚Äù\nworkstation and submits jobs for execution. When the system finds that the user‚Äôs\nworkstation does not have sufficient processing power for executing the processes of the\nsubmitted jobs efficiently, it transfers one or more of the process from the user‚Äôs\nworkstation to some other workstation that is currently idle and gets the process executed\nthere, and finally the result of execution is returned to the user‚Äôs workstation.\nEvolution of Distributed Computing System\n3. The workstation ‚Äì\nserver model\n\n\n-\nFor a number of reasons, such as higher reliability and better scalability, multiple servers\nare often used for managing the resources of a particular type in a distributed computing\nsystem.\n-\nFor better overall system performance, the local disk of a diskful workstation is normally\nused for such purposes as storage of temporary files, storage of unshared files, and storage\nof shared files that are rarely changed, paging activity in virtual-memory management, and\nchanging of remotely accessed data.\nEvolution of Distributed\nComputing System\n4. Processor Pool\nModel:\n-\nThe processor ‚Äì pool model is based on the observation that most of the time a user does not\nneed any computing power but once in a while he or she may need a very large amount of\n\n\nneed any computing power but once in a while he or she may need a very large amount of\ncomputing power for a short time. (e.g., when recompiling a program consisting of a large\nnumber of files after changing a basic shared declaration).\n-\nTherefore, unlike the workstation ‚Äì server model in which a processor is allocated to each\nuser, in the processor-pool model the processors are pooled together to be shared by the users\nas needed.\n-\nThe pool of processors consists of a large number of microcomputers and minicomputers\nattached to the network. Each processor in the pool has its own memory to load and run a\nsystem program or an application program of the distributed computing system.\nDefinition of a Distributed System\nA distributed system organized as middleware. The middleware layer extends over\nmultiple machines, and offers each application the same interface.\n1.1\n\n\nDefinition of a Distributed System\nFig. shows four networked computers and three applications, of which application B is\ndistributed across computers 2 and 3. Each application is offered the same interface.\nThe distributed system provides the means for components of a single distributed\napplication to communicate with each other, but also to let different applications\ncommunicate. At the same time, it hides, as best and reasonable as possible, the\ndifferences in hardware and operating systems from each application.\n\n\nUnit No: 1 Unit name:Introduction to Distributed Systems\nLectureNo:2\nIssues,Goals of distributed\nIssues,Goals of distributed\nsystem\n1. Support of heterogeneity\n2. Making Resources Accessible\n3. Transparency\n4. Openness\n5. Scalability\n6. Security\nIssues\n6. Security\n7. Lack of failure-handling\nmechanism\n8. Concurrency\n9. Quality of service\n\n\n1. Support of heterogeneity\nThe distributed system is nothing but a collection of computers which may have\ndifferent hardware operating system platforms, network protocols and languages for\nimplementation. Some of the distributed systems do not support heterogeneity as they\nare either platform-dependent or programming language dependent.\n2. Making Resources Accessible\nÔÇ∑\nThe main goal/issue of a distributed system is to make it easy for the users (and\nIssues\nÔÇ∑\nThe main goal/issue of a distributed system is to make it easy for the users (and\napplications) to access remote resources, and to share them in a controlled and efficient\nway.\nÔÇ∑\nResources can be just about anything, but typical examples include things like printers,\ncomputers, storage facilities, data, files, Web pages, and networks, to name just a few.\nÔÇ∑\nThere are many reasons for wanting to share resources. One obvious reason is that of\neconomics. For example, it is cheaper to let a printer be shared by several users in a\nsmall office than having to buy and maintain a separate printer for each user. Likewise, it\nmakes economic sense to share costly resources such as supercomputers, high-\nperformance storage systems, imagesetters, and other expensive peripherals\n\n\nIssues\n3. Transparency\nAn important issue of distributed system is to hide the fact that its processes and resources\nare physically distributed across multiple computers. A distributed system that is able to\npresent itself to user and application as if it were only a single computer system is said to\nbe transparent. There are eight types of transparencies in a distributed system:\n\n\n4. Openness\nÔÇ∑\nAnother important goal/issue of distributed systems is openness.\nÔÇ∑\nThere is need to define comm\non syntax, semantics and implementation details with specification or document\ndescribing key interface details to provide openness.\nÔÇ∑\nAn open distributed system is a system that offers services according to standard rules that describe the syntax and\nsemantics of those services.\nÔÇ∑\nFor example, in computer networks, standard rules govern the format, contents, and meaning of messages sent and\nreceived. Such rules are formalized in protocols.\nÔÇ∑\nIn distributed systems services are generally specified through interfaces, which are often described in an Interface\nDefinition Language (IDL).\nIssues\nDefinition Language (IDL).\n5. Scalability\nÔÇ∑\nScalability of a system can be measured along at least three different dimensions.\nÔÇ∑\nFirst, a system can be scalable with respect to its size, meaning that we can easily add more users and resources to\nthe system.\nÔÇ∑\nSecond, a geographical scalable system is one in which the users and resources may lie far apart.\nÔÇ∑\nThird, a system can be administratively scalable, meaning that it can still be easy to manage even if it spans many\nindependent administrative organizations. Unfortunately, a system that is scalable in one or more of these\ndimensions often exhibits some loss of performance as the system scales up.\n\n\n6. Security\nIn distributed system, most of the communication occur between the client and the servers.\nTherefore, vulnerability in security may bottleneck the server.\nDenial of service is most common security attack in distributed system, where large number\nof continous request are sent to the server such that intended user will not get access to it.\nSecurity can be achieved by\nÔÇ∑\nAuthentication is used to validate the user access by the means of username and password.\nÔÇ∑\nAuthorization is used to check the authority, policies and the role of access.\nIssues\nÔÇ∑\nAuthorization is used to check the authority, policies and the role of access.\nÔÇ∑\nUse of message encryption and decryption technique for security\n7. Lack of failure-handling mechanism\nMost of the distributed systems have lack of a built-in fault-tolerant mechanism, wherein the\nfailure detection and recovery is difficult in real time. The faults in the distributed system are\nusually partial, where some component fail and other may function, thus leading to the generation\nof a wrong output. Therefore, it is difficult to handle failure in the system.\n\n\n8. Concurrency\nIn a distributed system, multiple users who are spatially separated use the system concurrently. In such duration, it is economical to share the\nsystem resources (hardware or software) among the concurrently executing user processes. However since the number of available resources in\na computing system is restricted, one user process must necessarily influence the action of other concurrently executing user processes, as it\ncompetes for resources. For example, concurrent updates to the same file by two different processes should be prevented. Concurrency means\nthat each user has a feeling that he or she is the sole user of the system and other users do not exist in the system. For providing concurrency,\nthe resource sharing mechanisms of the distributed operating system must have the following four properties:\n1. An event-ordering property ensures that all access requests to various system resources are properly ordered to provide a consistent view to\nall users of the system.\nIssues\n2. A mutual-exclusion property ensures that at any time at most one process accesses a shared resource, which must not be used simultaneously\nby multiple processes if program operation is to be correct.\n3. A no-starvation property ensures that if every process that is granted a resource, which must not be used simultaneously by multiple\nprocesses, eventually releases it, every request for that resource is eventually granted.\n4. A no-deadlock property ensures that a situation will never occur in which competing processes prevent their mutual progress even though no\nsingle one requests more resources than available in the system\n9. Quality of service\nThe quality of service is a measure of defining the performance of functional services in the system by the client in terms of reliability, security\nand performance.\n\n\n-\nSupport of heterogeneous hardware and software.\n-\nResources are easily accessible across the network.\n-\nIt should hide the fact about the resources in terms of transparencies.\n-\nDistributed system should be scalable.\nGoals\n-\nDistributed system should be scalable.\n-\nThe system follows open standards so that they use standard syntax and\nsemantics.\n-\nThe system should be independent of geographical locations of servers and\neasily manageable or the administrators\n-\nThe system should be capable to detect and recover from failure, it should be\nfault tolerance and robust.\n\n\nUnit No: 1 Unit name:Introduction to Distributed Systems\nLectureNo:3\nTypes and Models of\nTypes and Models of\nDistributed System\nBased on functionality the distributed system is broadly classified into three\ntypes:\nTypes of distributed system\nTypes of Distributed System\n\n\nDistributed\nComputing\nSystem\nDistributed\nInformation\nSystem\nDistributed\nPervasive\nSystem\n-Distributed Computing Systems\n-Clusters\n-Grids\n-Distributed Information Systems\n-Transaction Processing Systems\nTypes of Distributed Systems\n-Transaction Processing Systems\n-Enterprise Application Integration\n-Distributed Embedded Systems\n-Home systems\n-Health care systems\n-Sensor networks\n\n\n1. Distributed Computing system\n-\nThe distributed system uses a group of computers that share a common\ncomputation problem among them so as to generate an efficient results in\nshort time span.\n-\nThe time is an important entity in\ndistributed computing, which can be\nreduced by increasing the number of computing nodes.\nTypes of distributed system\n-\nThe system uses special kind of software application to manage different\ntask performed on multiple computer simultaneously\n-\nIt help in the interaction of different computer through the exchange of\nmessage.\n-\nIt has two subclasses- cluster computing and grid computing\n\n\nCluster computing\n-\nCluster computing is a form of distributed computing where a group of\ncomputers are linked together in a network to perform a single task and act\nlike single entity.\n-\nhigh availability and load balancing are the key feature of cluster computing.\n-\nThey ensure that the computing system will always be available by creating\nTypes of distributed system\n-\nThey ensure that the computing system will always be available by creating\nredundancy in the network so that the system never fails.\n-\nDisadvantage\n-\nIt only supports homogeneous infrastructure only.\n( Same OS, Hardware, Networks)\n\nsystem\n\n-High Performance Clusters (HPC)\n-run large parallel programs\n-Scientific, military, engineering apps; e.g., weather modeling\n-Load Balancing Clusters\nCluster Types & Uses\n-Front end processor distributes incoming requests\n-High Availability Clusters (HA)\n-Provide redundancy ‚Äì back up systems\n-May be more fault tolerant than large mainframes\n\n\nTypes of distributed system\nCluster computing\n\nsystem\n\n-Linux-based\n-Master-slave paradigm\n-One processor is the master; allocates tasks to other processors,\nmaintains batch queue of submitted jobs, handles interface to users\n-Master has libraries to handle message-based communication or other\nfeatures (the middleware).\nClusters ‚Äì Beowulf model\nfeatures (the middleware).\n\n\nFigure 1-6. An example of a cluster computing system.\nCluster Computing Systems\nFigure 1-6. An example of a (Beowolf) cluster computing\nsystem\n\n-A characteristic feature of cluster computing is its homogeneity.\n-In most cases, the computers in a cluster are largely the same, they all have\nthe same operating system, and are all connected through the same network.\n-In contrast, grid computing systems have a high degree of heterogeneity:\n--- no assumptions are made concerning hardware, operating systems,\nnetworks, administrative domains, security policies, etc\nGrid Computing\nnetworks, administrative domains, security policies, etc\n-Grids support virtual organizations:\n-Resources from different organizations are brought together to allow the\ncollaboration of a group of people or institutions. Such a collaboration is\nrealized in the form of a virtual organization.\n-People belonging to the same virtual organization have access rights to the\nresources that are provided to that organization.\n\n\nGrid Computing\n-\nGrid computing is another\ncomputing\nform\nwhere\nheterogeneity\nis\nthe\nkey\nadvantage.\n-\nIt\nallows\nmultiple\ncomputers\nwith\ndiverse\n\nsystem\n\ncomputers\nwith\ndiverse\nhardware ,operating system\nand\nnetwork\nto\nsolve\ncomputation problem.\n-\nAll the computer in a grid\nnetwork have a common\ngoal and work on single\ntask.\nA Proposed Architecture for Grid Systems*\n-\nFabric layer: interfaces to local\nresources at a specific site\n-\nConnectivity layer: protocols to\nsupport usage of multiple\nresources for a single application;\ne.g., access a remote resource or\ntransfer data between resources;\nand protocols to provide security\n-\nResource layer manages a single\nresource, using functions supplied\nResource layer manages a single\nresource, using functions supplied\nby the connectivity layer\n-\nCollective layer: resource\ndiscovery, allocation, scheduling,\netc.\n-\nApplications: use the grid\nresources\n-\nThe collective, connectivity and\nresource layers together form the\nmiddleware layer for a grid\nFigure : A layered architecture for grid\ncomputing systems\n\nFigure 1-8. Example primitives for transactions.\nTransaction Processing Systems\n\n-Let us concentrate on database applications\n-Operations on a database are usually carried out in the form of transactions\n-Obey the ACID properties:\nAtomic:\nall or nothing\nConsistent:\ninvariants are preserved\nIsolated: Concurrent\nTransaction Processing Systems\nIsolated: Concurrent\nDurable:\ncommitted operations can‚Äôt be undone\n\n\nsystem\n-So far, transactions have been defined on a single database.\n-A nested transaction is constructed from a number of sub transactions\nNested Transaction\nFig: A nested transaction.\n\n\nsystem\n-Enterprise application integration is an integration framework composed of a\ncollection\nof\ntechnologies\nand\nservices\nwhich\nform\na\nmiddleware\nor\n\"middleware framework\" to enable integration of systems and applications\nacross an enterprise.\n-Many\ntypes\nof\nbusiness\nsoftware\nsuch\nas\nsupply\nchain\nmanagement applications, ERP systems, CRM applications for managing\ncustomers,\nbusiness\nintelligence\napplications,\npayroll,\nand\nhuman\nEnterprise Application Integration\ncustomers,\nbusiness\nintelligence\napplications,\npayroll,\nand\nhuman\nresources systems typically cannot communicate with one another in order to\nshare data or business rules.\n-Enterprise application integration is the process of linking such applications\nwithin a single organization together in order to simplify and automate business\nprocesses\n-Applications can be linked either at the back-end via APIs and the front-end\n(GUI).\n\n\nsystem\nData\nintegration:\nEnsures\nthat\ninformation\nin\nmultiple\nsystems\nis\nkept\nconsistent. This is also known as enterprise information integration (EII).\nVendor independence: Extracts business policies or rules from applications and\nimplements them in the EAI system, so that even if one of the business\napplications is replaced with a different vendor's application, the business rules\ndo not have to be re-implemented.\nEAI can be used for different purposes:\ndo not have to be re-implemented.\nCommon facade: An EAI system can front-end a cluster of applications,\nproviding a single consistent access interface to these applications and\nshielding users from having to learn to use different software packages.\n\n\nsystem\n-The main idea was that existing applications could directly exchange\ninformation\n-Several types of communication middleware exist.\n----Remote Procedure Call (RPC)\n----Remote Method Invocation (RMI)\n-RPC and RMI have the disadvantage that the caller and callee both need to be\nEnterprise Application Integration\n-RPC and RMI have the disadvantage that the caller and callee both need to be\nup and running at the time of communication.\n-In addition, they need to know exactly how to refer to each other.\n-This tight coupling is often experienced as a serious drawback, and has led to\nwhat is known as message-oriented middleware\n\n-In this case, applications simply send messages to logical contact points, often\ndescribed by means of a subject.\n-Applications can indicate their interest for a specific type of message, after\nwhich the communication middleware will take care that those messages are\ndelivered to those application.\nMessage-oriented middleware\n-These so-called publish/subscribe systems form an important and expanding\nclass of distributed systems.\n\nThe first two types of systems are characterized by their stability: nodes and\nnetwork connections are more or less fixed\n-Three requirements for pervasive applications:\n1. Embrace contextual changes.\n2. Encourage ad hoc composition.\nDistributed Pervasive Systems\n2. Encourage ad hoc composition.\n3. Recognize sharing as the default.\nThis type of system is likely to incorporate small, battery-powered, mobile\ndevices\nHome systems\nElectronic health care systems ‚Äì patient monitoring\nSensor networks ‚Äì data collection, surveillance\n\nBuilt around one or more PCs, but can also include other electronic devices:\nAutomatic control of lighting, sprinkler systems, alarm systems, etc.\nNetwork enabled appliances\nPDAs and smart phones, etc.\nHome System\nHome System\n\nFigure 1-12. Monitoring a person in a pervasive electronic\nhealth care system, using (a) a local hub or (b) a\ncontinuous wireless connection.\nElectronic Health Care Systems\n\nA collection of geographically distributed nodes consisting of a comm. device, a\npower source, some kind of sensor, a small processor‚Ä¶\nPurpose: to collectively monitor sensory data (temperature, sound, moisture\netc.,) and transmit the data to a base station\n‚Äúsmart environment‚Äù ‚Äì the nodes may do some rudimentary processing of the\ndata in addition to their communication responsibilities.\nSensor Networks\n\nFigure 1-13. Organizing a sensor network database, while\nstoring and processing data (a) only at the operator‚Äôs site\nor ‚Ä¶\nSensor Networks\n\nFigure 1-13. Organizing a sensor network database, while\nstoring and processing data ‚Ä¶ or (b) only at the sensors.\nSensor Networks\n\nDistributed computing systems ‚Äì our main emphasis\nDistributed information systems ‚Äì we will talk about some aspects of them\nDistributed pervasive systems ‚Äì not so much\n****\nSummary ‚Äì Types of Systems\nThe distributed system models are used to express functionalities between the\ncomponents have been categorized into four sub models\nDistributed System Models\nDistributed System\nModels\n\nmodels\n\nArchitectural\nModel\nInteraction\nmodel\nFault model\nSecurity\nmodel\n1.Architectural Model\n-\nAn architecture model of distributed system defines the way in which the\ncomponents of the system interact with each other and mapped onto an\nunderlying network of computer.\n-\nThe client-server and peer to peer model are the parts of an architectural model.\nThe client server model is widely used architecture in most of the distributed\ntechnologies, where client process interact with the server process to get access\nDistributed System Models\ntechnologies, where client process interact with the server process to get access\nthe shared resources.\n-\nUse RPC/RMI for communication\n-\nIts is based on request/reply protocol implemented with send/receive primitives.\nIn the peer to peer model all computers in the network get same privileges and run\nsame program with the same set of interfaces.\n-\nDisadvantage of P2P are the lack of scalability and high complexity\n\nmodels\n\nDistributed System Models\nClient-Server Model\n\nmodels\n\nDistributed System Models\npeer-to-peer Model\n\nmodels\n\n2. Interaction Model\n-\nThe interaction model deals with type of interaction pattern used in the\ncommunication.\n-\nThe two variants of the interaction model are synchronous distributed\nsystem and asynchronous distributed system.\nDistributed System Models\n\nmodels\n\nIn synchronous distributed system ,both client and the server should actively\nparticipate in the communication process\nMain feature:\nExecution\nSynchronous distributed system\nLower and upper bound on the execution time of process is set\nCommunication\nOrdered Message Delivery\nClocks\n\nmodels\n\nTransmitted messages are received within known bounded time\nCommunication channel between two machines should deliver messages in FIFO order\nEach node has logical clock and it should be synchronized\nIn asynchronous distributed system, the sender and receiver both need not\nto be activated at a time for communication because no time bound is recorded\nhere.\nMain feature:\nExecution\nAsynchronous distributed system\nNo bound Lower and upper bound on the execution time of process is set\nCommunication\nOrdered Message Delivery\nClocks\n\nmodels\n\nNo bounded on transmission delay (No assumption on speed,load,etc\nMessage can be delayed for an arbitrary period of time\nClocks of different nodes in a distributed system can drift apart\nBlocking communication (synchronous)\n‚Äì Send blocks until message is actually sent\n‚Äì Receive blocks until message is actually received\nNon-blocking communication (asynchronous)\n‚Äì Send returns immediately\nDistributed System Models\n‚Äì Send returns immediately\n‚Äì Return does not block either\n\n\nmodels\n-\nIn synchronous distributed system ,both client and the server should\nactively participate in the communication process, where the time to\nexecute each step of a process records the lower and upper bounds.\nBlocking communication (synchronous)\n‚Äì Send blocks until message is actually sent\n‚Äì Receive blocks until message is actually received\n‚Äì Receive blocks until message is actually received\n-\nIn asynchronous distributed system, the sender and receiver both need\nnot to be activated at a time for communication because no time bound is\nrecorded here.\nNon-blocking communication (asynchronous)\n‚Äì Send returns immediately\n‚Äì Return does not block either\n\n\nmodels\nDistributed System Models\nBlocking Vs Non- Blocking\n\nmodels\n\n3. Fault Model\nIn distributed system, failure may occur with both process and communication\nchannels.\nThere are three types of fault-omission fault, arbitrary fault and timing fault.\n1.Omission fault-:the action is performed or omitted if fault occurs.\nDistributed System Models\n1.Omission fault-:the action is performed or omitted if fault occurs.\n2.Arbitrary fault-:describes the worst possible fault semantics where any type of\nerror may occur.\n3.Timing Fault-:time limit is exceeded.\n\nmodels\n\n4. Security Model\n-\nThe security model is intended to provide security to the shared resource,\nprocess and channel used for their interaction.\n-\nIt Provide security to the shared object by means of encryption and\ndecryption.\nDistributed System Models\ndecryption.\n-\nThe various cryptographic algorithms are used for encrypting and decrypting\nobjects and processes by using different key algorithm.\n-\nCombination of authentication and encryptions is widely used to secure the\ncommunication channel.\n\nmodels\n\nThank You\nThank You"
    },
    {
      "filename": "DC_Week2.pdf",
      "path": "data/materials\\Distributed Computing\\DC_Week2.pdf",
      "text": "Subject Name: DISTRIBUTED COMPUTING\nUnit No: 1 Unit Name: Introduction to\ndistributed System\nFaculty Name:\nMrs. . HarshaSaxena\nMrs.BhavanaAlte\nMs.DhanashriBhosale\nIndex\n\n\n\n\n\n\n\n\n\noffered by middleware system\n\n\n\n\nUnit No: 1 Unit name:Introduction to Distributed Systems\nLectureNo:4\nHardware concept,Software\nHardware concept,Software\nconcept\nHardware Concepts\nDistributed system consist of multiple CPUs and memory, there are several\ndifferent ways the hardware can be organized in terms of how they are\ninterconnected and how they communicate.\nMULTIPROCESSOR\nMULTICO\nMPUTER\n\n\nHardware Concepts Classification\n-\nMulti Processor and Multi Computer\nMulti Processor-Those have shared memory(Single Physical address space)\nMulti Computer-has its own private memory\n-\nBus and Switched\nBus-Single network, backplane, cable that connect all the machine.\nSwitch-There are individual wires from machine to machine with many different\nSwitch-There are individual wires from machine to machine with many different\nwiring pattern\n-\nHomogenous and Heterogeneous (Distinction for only Multicomputer)\nHomogenous-use same technology for interconnection, same access to private\nmemory( generally working on same problem)\nHeterogeneous-Different independent computer connected through different\nnetwork through switch\n\n\nA bus-based multiprocessor\n-\nDirect access to shared memory\n-\nConnected to a common bus\n-\nMemory should maintain coherent (Consistent)property\n-\nWith increase of CPU performance drop\n-\nTo increase performance, High-speed cache memory is used between CPU\nMultiprocessors (1)\n-\nTo increase performance, High-speed cache memory is used between CPU\nand bus.\n-\nIt store recent data\n-\nCan lead to incoherent memory and need replication mechanisms.\n\n\nMultiprocessors (2)\nA Switch-based multiprocessor\n- for more than 256 CPU\n- Memory is divided in modules\na) A crossbar switch-Each CPU and each\nmemory are connected.\n- Switch closed when one CPU access the\nmemory. Rest CPU waits for switch to open.\n\n\nmemory. Rest CPU waits for switch to open.\n- For n CPU and n memory,n^2 switches are\nneeded.(network is limited to less value of n)\nb) An omega switching network\n- In fig network contain 2X2 switch, each having\n2 input and 2 output\n- Disadvantage- There may be several switching\nstages between the CPU\nIn CPU-CPU communication, the volume of traffic is of lower magnitude than\nfor CPU-Memory traffic.\n-\nSystem Area Networks(SANs) ‚Äìthe nodes are mounted in a big rack and\nconnected through single, high-performance network.\n-\nBus-based Multicomputer(20-100 nodes)- Connected through shared\nmulti-access network such as Fast Ethernet.(message is broadcast )\n-\nSwitch-based Multicomputer- (message is routed)\na)\nGrid: 2-dimensional network\nHomogeneous Multicomputer Systems\na)\nGrid: 2-dimensional network\nb)\nHypercube: n-dimensional cube , each vertex is CPU\n\n\nAn overview of\n-DOS (Distributed Operating Systems)\n-NOS (Network Operating Systems)\n-Middleware\nSoftware Concepts\n\n\nComputer architectures consisting of interconnected, multiple processors are\nbasically of two types:\n-\nTightly coupled systems\nComputer Architecture\n-\nLoosely coupled systems\n\n\n-\nIn these systems, there is a single system wide primary memory (address\nspace) that is shared by all the processors.\n-\nIf any processor writes, for example, the value 100 to the memory location x,\nany other processor subsequently reading from location x will get the value\n100.\n-\nTherefore, in these systems, any communication between the processors\nusually takes place through the shared memory.\nTightly Coupled System\n\n\n\n-\nIn these systems, the processors do not share memory, and each processor\nhas its own local memory.\n-\nIf a processor writes the value 100 to the memory location x, this write\noperation will only change the contents of its local memory and will not affect\nthe contents of the memory.\n-\nIn these systems, all physical communication between the processors is\ndone by passing messages across the network that interconnects the\nLoosely coupled systems\ndone by passing messages across the network that interconnects the\nprocessors.\n\n\n\nSoftware Concepts\nSystem\nDescription\nMain Goal\nDistributed\nOperating\nSystem (DOS)\nTightly-coupled operating system for multi-\nprocessors and homogeneous\nmulticomputers\nHide and manage\nhardware\nresources\nNetwork\nOperating\nSystem (NOS)\nLoosely-coupled operating system for\nheterogeneous multicomputers (LAN and\nWAN)\nOffer local\nservices to remote\nclients\nMiddleware\nAdditional layer atop of NOS implementing\ngeneral-purpose services\nProvide\ndistribution\ntransparency\n\n\n1. Distributed Operating Systems (1)\n1.11\n-\nDOS functionality is same as traditional OS for uniprocessor system, except that\nthey handle multiple CPUs.\n-\nOS should have full control on how the hardware resource are used and shared\n\n\nUniprocessor Operating System\n1. Distributed Operating Systems (2)\n-\nAccess to Shared Memory\n-\nHigh performance\n-\nNo of CPUs are transparent to application\n\n\nMultiprocessor Operating System\nGeneral structure of a homogeneous multicomputers operating system\n1. Distributed Operating Systems (3)\n1.14\n\n\nMulticomputer Operating Systems\nAlternatives for blocking and buffering in message passing.\nS1: Block sender when buffer is full\nS2: Message being sent\nS3: Message is arrived\nS4: Message is sent to receiver\n1. Distributed Operating Systems (4)\n\n\nRelation between blocking, buffering, and reliable communications.\n1. Distributed Operating Systems (5)\nSynchronization point\nSend buffer\nReliable comm.\nguaranteed?\nBlock sender until buffer not full\nYes\nNot necessary\nBlock sender until buffer not full\nYes\nNot necessary\nBlock sender until message sent\nNo\nNot necessary\nBlock sender until message received\nNo\nNecessary\nBlock sender until message delivered\nNo\nNecessary\n\n\n-\nProgramming\na\nmulticomputer\nis\nmuch\nharder\nthan\nprogramming\nmultiprocessor.\n-\nFor communication to access shared data process in Multiprocessor used\nsemaphore and monitor for synchronization which is easier than message\n1. Distributed Operating Systems (6)\npassing.\n-\nBuffering, blocking and reliable communication make things worse.\n-\nTo avoid this researcher provide virtual shared memory to multicomputer.\n-\nWhere user access large virtual address space which lead to page-based\ndistributed shared memory(DSM)\n\n\na)\nPages (of size 4KB or 8KB) of address\nspace distributed among four machines\nb)\nCPU1 Reference instruction 0,2,5,9, the\nreferences are done locally\nc)\nSituation after CPU 1 references page\n10 is not present locally, a trap occurs\nthe OS fetch the page containing the\naddress and move it from CPU2 to\n1. Distributed Operating Systems (7)\naddress and move it from CPU2 to\nCPU1\nd)\nThis is normal paging concept, except\nthat remote RAM is used instead of local\ndisk.\ne)\nSituation if page 10 is read only and\nreplication is used to increase\nperformance.\n\n\nDistributed Shared Memory Systems\n-\nIssue with DSM is to decide how large page should be.\n-\nIf page size is large, total number of transfer will be reduce which increase\nperformance\n-\nA page contain two independent processes, OS need to consequently transfer the\npage between two processor Known as False sharing\n1. Distributed Operating Systems (8)\n1.18\n\n\n-\nHeterogeneous system\n-\nServices Provided by NOS\n-\nUser can remotely login to another machine\n-\nTransfer of file\n-\nGlobal file server accessible to all\n2. Network Operating System (1)\n1-19\n\n\n-\nTwo clients and a server in a network operating system.\n-\nFile server maintain hierarchical file system each with a root directory containing\nsubdirectories and files.\n2. Network Operating System (2)\n1-20\n\n\nDifferent clients may mount the servers in different places.\n2. Network Operating System (3)\n1.21\n\n\n-\nDOS provide full transparency\n-\nNOS lacks in maintaining transparency\n-\nDisadvantage of NOS\n-\nIts hard to login to every machine in network\n-\nCant use same password for every machine\n-\nProtection against malicious attack\nDifference between DOS and NOS\n-\nProtection against malicious attack\n-\nAdvantage of NOS\n-\nEasy to add or remove machine/Server\n-\nEasy to communicate existence of new machine to other\n-\nNeither DOS or NOS qualifies as Distributed system definition\n-\nWe need Openness and Scalability of NOS\n-\nAnd Transparency of DOS\n\n\n3. Middleware(1)\n\n\nIn an open middleware-based distributed system, the protocols used by each\nmiddleware layer should be the same, as well as the interfaces they offer to\napplications.\nMiddleware\n\n\nThere are many services offered by middleware system\n1.Naming service\n2.Persistence service\n3.Message service\n4.Querying service\nServices offered by Middleware System\n4.Querying service\n5.Concurrency service\n6.Security service\n\nMiddleware system\n\nComparison between Systems\nItem\nDistributed OS\nNetwork\nOS\nMiddleware-\nbased OS\nMultiproc.\nMulticomp.\nDegree of\ntransparency\nVery High\nHigh\nLow\nHigh\nSame OS on all\nnodes\nYes\nYes\nNo\nNo\nNumber of copies of\n\nN\nN\nN\nNumber of copies of\nOS\n\nN\nN\nN\nBasis for\ncommunication\nShared\nmemory\nMessages\nFiles\nModel\nspecific\nResource\nmanagement\nGlobal,\ncentral\nGlobal,\ndistributed\nPer node\nPer node\nScalability\nNo\nModerately\nYes\nVaries\nOpenness\nClosed\nClosed\nOpen\nOpen\n\n\nThank You\nThank You"
    },
    {
      "filename": "DC_Week3.pdf",
      "path": "data/materials\\Distributed Computing\\DC_Week3.pdf",
      "text": "Subject Name: DISTRIBUTED COMPUTING\nUnit No: 1 Unit Name: Introduction to\ndistributed System\nFaculty Name:\nMrs. . HarshaSaxena\nMrs.BhavanaAlte\nMs.DhanashriBhosale\nIndex\n\n\n\n\n\n\n\n\n\noffered by middleware system\n\n\n\n\nUnit No: 1 Unit name:Introduction to Distributed Systems\nLectureNo:5\nModels of\nMiddleware,Services of\nMiddleware,Services of\noffered by Middleware\nservices\nModels of Middleware\nModels of\nMiddleware\nRemote\nProcedure\ncall(RPC)\nMessage\noriented\nmiddleware(\nMOM)\nRemote\nmethod\ninvocation(R\nWeb\nServices\nDistributed\nFile System\n\n\nModels of\nMiddleware\ninvocation(R\nMI)\nCommon\nobject\nrequest\nbroker\narchitecture\n(CORBA)\nDistributed\ncomponent\nobject\nmodelling(D\nCOM)\nService\noriented\narchitecture\n(SOA)\nWeb\nServices\n1. Remote Procedure call\n-\nIn distributed computing, a remote procedure call (RPC) is when a computer\nprogram causes a procedure (subroutine) to execute in a different address\nspace (commonly on another computer on a shared network), which is\ncoded as if it were a normal (local) procedure call, without the programmer\nexplicitly coding the details for the remote interaction.\nModels of Middleware\nexplicitly coding the details for the remote interaction.\n\n\n2. Remote Method Invocation\n-\nRMI stands for Remote Method Invocation.\n-\nIt is a mechanism that allows an object residing in one system (JVM) to\naccess/invoke an object running on another JVM.\n-\nRMI is used to build distributed applications; it provides remote\nModels of Middleware\n-\nRMI is used to build distributed applications; it provides remote\ncommunication between Java programs.\n\n\n3. Common object request broker architecture (CORBA)\n-\nCommon Object Request Broker Architecture (CORBA) is a standard\nspecification\ndeveloped\nby\nthe\nObject\nManagement\nGroup\n(OMG)\nconsortium to support communication of objects on one machine to other\nobjects running on same or different machines in a distributed and\nheterogeneous environment\nModels of Middleware\nheterogeneous environment\n\n\n4. Distributed component object modeling (DCOM)\n-\nDesigned to allow software components residing on remote computers to\ninteract with one another\n-\nAs in CORBA, objects in DCOM are accessed via interfaces\nModels of Middleware\n-\nAs in CORBA, objects in DCOM are accessed via interfaces\n-\nUnlike CORBA, however, DCOM objects may have multiple interfaces\n-\nWhen a client requests a DCOM object from a server, the client must also\nrequest a specific interface of the object\n\n\nModels of Middleware\nCommunication\nservices\n- Procedure calls\nacross networks\n- Remote-object\nmethod\ninvocation\n- Message-\nInformation\nsystem services\n- Large-scale,\nsystem-wide\nnaming services\n- Advanced\ndirectory\nservices (search\nControl services\n- Distributed\ntransaction\nprocessing\n- Code migration\nSecurity services\n- Authentication\nand\nauthorization\nservices\n- Simple\nencryption\n\n\n- Message-\nqueuing\nsystems\n- Advanced\ncommunication\nstreams\n- Event\nnotification\nservice\nservices (search\nengines)\n- Location\nservices for\ntracking mobile\nobjects\n- Persistent\nstorage facilities\n- Data caching\nand replication\nencryption\nservices\n- Auditing\nservice\nUnit No: 1 Unit name:Introduction to Distributed Systems\nLectureNo:6\nClient-Server model\nClient-Server model\n-In the basic client-server model, processes in a distributed system are\ndivided\ninto two groups.\n---Client\n---Server\nGeneral Interaction between a client and a server\n-A server is a process implementing a specific service,\n-Example, a file system service or a database service\n-A client is a process that requests a service from a server by sending it\na request and subsequently waiting for the server's reply.\n\nGeneral interaction between a client and a server.\nClients and Server Model\n1.25\n\n\n-when a client requests a service, it simply packages a message for the server,\nidentifying the service it wants, along with the necessary input data.\n-The message is then sent to the server.\n-The latter, in turn, will always wait for an incoming request, subsequently\n-The latter, in turn, will always wait for an incoming request, subsequently\nprocess it, and package the results in a reply message that is then sent to the\nclient.\n\n\n-As many client-server applications are targeted toward supporting user access\nto databases, many people have advocated a distinction between the following\nthree levels:\n1. The user-interface level\nLayered architectural style\n-Contains all that is necessary to directly interface with the\nuser, such as display management.\n-Clients typically implement user interface level\n2. The processing level\n3. The data level\n\n\nThe processing level typically contains the applications\nThe data level manages the actual data that is being acted on.\n-\nDistributed systems are often complex pieces of software of which the components\nare by definition dispersed across multiple machines.\n-\nTo master their complexity, it is crucial that these systems are properly organized.\n-\nThere are different ways on how to view the organization of a distributed system, but\nan obvious one is to make a distinction between the logical organization of the\ncollection of software components and on the other hand the actual physical\nrealization.\nrealization.\n-\nThe organization of distributed systems is mostly about the software components that\nconstitute the system. These software architectures tell us how the various software\ncomponents are to be organized and how they should interact.\n-\ncommonly applied approaches toward organizing (distributed) computer systems is\nClient Server\n\n-\nMany client-server applications can be constructed from roughly three different\npieces:\na part that handles interaction with a user,\na part that operates on a database or file system,\na middle part that generally contains the core functionality of an application.\nThis middle part is logically placed at the processing level.\nIn contrast to user interfaces and databases, there are not many aspects common\nIn contrast to user interfaces and databases, there are not many aspects common\nto the processing level.\nTherefore, we shall give several examples to make this level clearer.\n\nExample: Consider an Internet search engine\n\nFigure: The simplified organization of an Internet search engine into three\ndifferent layers.\nAs a first example, consider an Internet search engine.\nIgnoring all the animated banners, images, and other fancy window dressing,\nthe user interface of a search engine is very simple: a user types in a string of\nkeywords and is subsequently presented with a list of titles of Web pages. The\nback end is formed by a huge database of Web pages that have been\nprefetched and indexed. The core of the search engine is a program that\ntransforms the user's string of keywords into one or more database queries. It\nExplanation\ntransforms the user's string of keywords into one or more database queries. It\nsubsequently ranks the results into a list, and transforms that list into a series of\nHTML pages. Within the client-server model, this information retrieval part is\ntypically placed at the processing level.\n\nModern Architecture\n\n\nAn example of a server acting as client.\nThe communication between client and server done in the following manner.\n1.Client process sends request to the server by calling method locally\n2.Server process receives remote method call from the client .It locate the\nmethod and passing parameters to calculate the results\n3.Server process sends request back to the client over the network.\n4.Client process receives result of called method through network.\nClients and Server Model\n4.Client process receives result of called method through network.\nThe component of client-server model are\n1. Client\n2. Server\n3. Network\n\n\nModern Architecture: Horizontal\n\n\nUnit No:2\nUnit Name:Communication\nLecture No:7\nLayered Protocols\nLayered Protocols\n-\nCommunication in distributed systems is always based on low-level message\npassing as offered by the underlying network.\n-\nThree widely-used models for communication: Remote Procedure Call (RPC),\nMessage-Oriented Middleware (MOM), and data streaming.\n-\nDue to the absence of shared memory, all communication in distributed\nLayered Protocols\n-\nDue to the absence of shared memory, all communication in distributed\nsystems is based on sending and receiving (low level) messages.\n\n\nLayered Protocols\nÔÇ∑\nWhen process A wants to communicate with process B\no\nit first builds a message in its own address space.\no\nThen it executes a system call that causes the operating system to send the message over the network to\nB.\no\nAlthough this basic idea sounds simple enough, in order to prevent chaos, A and B have to agree on the\nmeaning of the bits being sent.\no\nIf A sends a brilliant new novel written in French and encoded in IBM's EBCDIC character code, and B\nexpects the inventory of a supermarket written in English and encoded in ASCII, communication will be\n\n\nexpects the inventory of a supermarket written in English and encoded in ASCII, communication will be\nless than optimal.\no\nMany different agreements are needed.\n-\nHow many volts should be used to signal a 0-bit\n-\nhow many volts for a 1-bit?\n-\nHow does the receiver know which is the last bit of the message?\n-\nHow can it detect if a message has been damaged or lost, and what should it do if it finds out?\n-\nHow long are numbers, strings, and other data items, and how are they represented?\n-\nIn short, agreements are needed at a variety of levels, varying from the low-level details of bit\ntransmission to the high-level details of how information is to be expressed.\nLayered Protocols\n-\nTo resolve issues involved in communication, the International Standards Organization (ISO) developed a\nreference Open Systems Interconnection Reference Model (OSI model).\n-\nThe OSI model is designed to allow open systems to communicate. An open system is one that is prepared to\ncommunicate with any other open system by using standard rules that govern the format, contents, and\nmeaning of the messages sent and received. These rules are formalized in what are called protocols.\n-\nBefore exchanging data the sender and receiver first explicitly establish a connection, and possibly negotiate\n\n\nthe protocol they will use.\n-\nA distinction is made between two general types of protocols.\nCommunication\nConnection\noriented\nExample:\nTelephone\nConnectionless\nExample:\nMailbox\nLayered Protocols\n-\nIn the OSI model, communication is divided up into seven levels or layers\n-\nEach layer deals with one specific aspect of the communication. In this way, the problem can be divided up\ninto manageable pieces, each of which can be solved independent of the others.\n-\nEach layer provides an interface to the one above it. The interface consists of a set of operations that together\ndefine the service the layer is prepared to offer its users.\n\n\nA typical message as it appears on the network.\nLayered Protocols\n2-2\n\n\nOSI Layer\nLAYERS\nFUNCTION\nPROTOC\nOLS\nDEVICE\nApplicatio\nn Layer\nApplication\nprotocol data unit\nProvide user Interface\nHHTTP,FTP,\nSSH,DNS,S\nMTP\nGateway\nPresentati\non Layer\nPresentation\nprotocol data unit\nchecks syntax and semantics of the user data and perform\nencryption and decryption\nSSL,IMAP\nGateway\nSession\nLayer\nSession protocol\ndata unit\nmanage sessions between users and applications and provides\ndifferent communication mode like half duplex, full duplex or\nsimplex; it also manages the connection between two entries.\nProvide synchronization facility.\nRPC,Netbio\ns,Sockets\nand API‚Äôs\nGateway\nTransport\nLayer\nTransport\nprotocol data unit\nEnd to end connection reliability and flow control\nTCP,UDP,S\nCTP,SPX\nGateway\n\n\nNetwork\nLayer\nPacket\ninterconnecting\ndis-similar\nnetworks,\nusing\nrouters\nand\nselecting the shortest path between any two nodes using\nrouting algorithm. This layer use connectionless IP protocol for\nlogical addressing of machines and generate packets with the\ndata for transmission over the network.\nIP,IPSec,IC\nMP,IOMP\nRouter\nGateway\nData Link\nLayer\nFrame\nError detection and correction flow control on MAC and LLC\nlayers\nSLIP,PPP\nGateway\nSwitch\nbridge\nWAP\nPhysical\nLayer\nBit\nMedia Signal and Binary transmission\nWired or\nwireless\ntransmissi\non on\nprotocols\nGateway\nHUB,repe\nators\nDiscussion between a receiver and a sender in the data link layer.\nData Link Layer\n2-3\n\n\na)\nNormal operation of TCP.\nb)\nTransactional TCP.\nClient-Server TCP\n2-4\n\n\n-Middleware is an application which logically lives in application layer where session and\npresentation layers are replaced with application independent protocols.\n-Authentication protocols\n-Authorization protocols\n-Distributed commit protocols\nMiddleware protocols\n-Distributed locking protocols\n-Middleware communication protocols\nRemote Procedure Call (RPC)\nRemote Object Invocation\nRemote Method Invocation (RMI)\nMessage Oriented Communication\nStream Oriented Communication\n\n\nAn adapted reference model for networked communication.\nMiddleware Protocols\n2-5\nApplication\nindependent\nProtocol\n\n\nThank You\nThank You"
    },
    {
      "filename": "DC_Week4.pdf",
      "path": "data/materials\\Distributed Computing\\DC_Week4.pdf",
      "text": "Subject Name: Distributed Computing\nUnit No: 02Unit Name: Communication\nFaculty Name:\nMs. Harsha Saxena\nMs. Bhavana Alte\nMs. Puja Padiya\nUnit No:2\nUnit Name:Communication\nLecture No:8\nInterprocess communication\nInterprocess communication\n(IPC): MPI\nRemote Procedure Call (RPC)\n-\nProcesses executing concurrently in the operating system may be either\nindependent or cooperating processes.\n-\nReasons for providing an environment that allows process cooperation.\n1) Information Sharing\nSeveral users may be interested in the same piece of information.\n2) Computational Speed up\nINTERPROCESS COMMUNICATION\n2) Computational Speed up\nProcess can be divided into sub tasks to run faster, speed up can be\nachieved if the computer has multiple processing elements.\n3) Modularity\nDividing the system functions into separate processes or threads.\n4) Convenience\nEven an individual user may work on many tasks at the same time.\n\n\nINTERPROCESS COMMUNICATION\nCooperating processes require IPC\nmechanism that allow them to exchange\ndata and information. Communication\ncan take place either by Shared memory\nor Message passing Mechanisms.\nShared Memory:\n1) Processes can exchange information by\nreading and writing data to the shared\nregion.\n2) Faster than message passing as it can\n\n\n2) Faster than message passing as it can\nbe\ndone at memory speeds when within a\ncomputer.\n3) System calls are responsible only to\nestablish\nshared memory regions.\nMessage Passing:\nMechanism to allow processes to\ncommunicate and synchronize their\nactions without sharing the same\naddress space and is particularly useful\nin distributed environment.\nMessage Passing\n-\nA process is a program counter and address space.\n-\nMessage passing is used for communication among processes.\n-\nInter-process communication:\n-\nType:\nSynchronous / Asynchronous\n-\nMovement of data from one process‚Äôs address space to another‚Äôs\n\n\nSynchronous Vs. Asynchronous\nÔÇ¢\nA synchronous communication is not complete until the message has been\nreceived.\nÔÇ¢\nAn asynchronous communication completes as soon as the message is on the\nway.\n\n\nSynchronous Vs. Asynchronous\n\n\nMessage Passing Interface\n-\nMPI is a communication protocol for programming parallel computers.\n-\nBoth point-to-point and collective communication are supported.\n-\nMPI \"is a message-passing application programmer interface, together\nwith protocol and semantic specifications for how its features must\nbehave in any implementation.\n-\nMPI's goals are high performance, scalability, and portability.\n\n\n-\nMPI's goals are high performance, scalability, and portability.\n-\nMPI\nremains\nthe\ndominant\nmodel\nused\nin\nhigh-performance\ncomputing today\nConventional Procedure Call\na)\nParameter passing in a local procedure call: the stack before the call to\nread\nb)\nThe stack while the called procedure is active\nRemote Procedure Call\n\n\n-\nRPC is a protocol that one program can use to request a service from a program\nlocated in another computer on a network without having to understand the\nnetwork details.\n-\nA procedure call is also called as function call or subroutine call.\n-\nRPC uses the client server model\nRemote Procedure Call\n\n\n-\nIt includes mainly five elements :-\n-\nThe Client\n-\nThe Client stub (Stub: Piece of code used for converting the parameters)\n-\nThe RPC Runtime (RPC communication package)\n-\nThe Server stub\nRemote Procedure Call\n-\nThe Server\n\n\n-\nThe Client\nÔÅ±\nIt is used process which initiates a RPC\nÔÅ±\nThe client make a perfectly normal call that invokes a corresponding procedure in the client stub.\n-\nThe Client stub\nÔÅ±\nOn receipt of a request it packs a requirement into a message and ask to RPC Runtime to send\nÔÅ±\nOn receipt of a result it unpacks the result and passes it to client.\n-\nThe RPC Runtime\nÔÅ±\nIt handle transmission of message between Client and server\nRemote Procedure Call\nÔÅ±\nIt handle transmission of message between Client and server\n-\nThe Server stub\nÔÅ±\nIt unpack a call request and make a perfectly normal call to invoke the appropriate procedure in the\nserver\nÔÅ±\nOn receipt of a result a procedure execution it pack the result and asks to RPC Runtime to send\n-\nThe Server\nÔÅ±\nIt executes an appropriate procedure and returns the result from a server stub.\n\n\nRemote Procedure Call\nReturn\nCall\nCall\nReturn\nUnpack\nPack\nUnpack\nPack\nClient\nServer\nServer Stub\nClient Stub\n\nUnpack\nPack\nUnpack\nPack\nReceive\nSend\nReceive\nSend\nRPC Routine\nRPC Routine\nCall Packet\nResult Packet\n\nSteps involved in doing remote computation through RPC\nPassing Value Parameters (1)\n2-8\n\n\n1.\nClient procedure calls client stub in normal way\n2.\nClient stub builds message, calls local OS\n3.\nClient's OS sends message to remote OS\n4.\nRemote OS gives message to server stub\n5.\nServer stub unpacks parameters, calls server\nSteps of a Remote Procedure Call\n5.\nServer stub unpacks parameters, calls server\n6.\nServer does work, returns result to the stub\n7.\nServer stub packs it in message, calls local OS\n8.\nServer's OS sends message to client's OS\n9.\nClient's OS gives message to client stub\n10.\nStub unpacks result, returns to client\n\n\nUnit No:2\nUnit Name:Communication\nLecture No:9\nRemote Procedure Call (RPC)\n-Till the time scalar types are same it works fine but in a distributed\nenvironment multiple machine types are present. e.g. EBCDIC code\nand ASCII code\n-Representation diff due to little endian and big endian format. e.g. intel\nVarious formats\nformat and SPARC format.\n-Integers are reversed by the different byte ordering but strings are not\n\n\n-\nIn RPC, the parameters are passed in two ways\n-\nPass by value\n-\nPass by reference\n-\nIn Pass by value, the actual parameters and their data types are copied into\nParameter Passing in RPC\nthe stack and passed to the called procedure.\n-\nIn Pass by reference, the pointer to the data is passed instead of the value to\nthe called procedure at the server side.\n-\nIt is difficult to implement as the server needs to keep the track of the\npointer to the data at the client address space.\n\n\nExtended RPC models:- These system uses RPC for their communication\n-\nDoors\n-\nAsynchronous RPC\nDoors were developed by the sun microsystems for their Spring Operating system.\n-\nOriginal RPC assume remote communication.\n-\nIf client and server are on the same machine, then more efficient communication may\nbe used, based on IPC system call library.\n-\nSo called door is used for co-located client server process communication\nExtended RPC Model\n-\nSo called door is used for co-located client server process communication\n-\nIn this case, server is registered with a named door, so that it can be uniquely\nidentified by the operating system when named by the clients\n-\nIn Solaris, door is associated a file name and treated as such.\n-\nThey allow server processes to create the door and the client processes to call doors.\n-\nIt use IPC to call subroutine or procedure on the remote host.\n-\nIt use following functions for communication\n-\ndoor_create()- server call this function to create door\n-\ndoor_call()- client invoke different procedure on the server using this function\n-\nDoor_return()- server invoke this function to return result back to the called\nprocess.\n\n\nExtended RPC Model-Doors\n\nThe principle of using doors as IPC mechanism.\n\na)\nThe interconnection between client and server in a\ntraditional RPC\nb)\nThe interaction using asynchronous RPC\nExtended RPC Model-Asynchronous RPC (1)\n2-12\n\na)\nThe interconnection between client and server in a traditional RPC\nb)\nThe interaction using asynchronous RPC\n\nA client and server interacting through two asynchronous RPCs or deferred synchronous RPC\nAsynchronous RPC (2)\n2-13\n\n\n-\nDCE RPC is a middleware system between OS and application, developed by so called\nOpen Group.\n-\nServices:\n-\nDistributed file service\n-\nDirectory service\n-\nSecurity service\nExample of RPC: Distributed Computing Environment (DCE)\n-\nSecurity service\n-\nDistributed time service\n-\nGoal: to provide transparency in all above services, including binding, communication, data\ntype conversion, and language differences\n-\nClient and servers can run on different architecture and implemented using different\nprogramming languages\n-\nThe DCE RPC includes a number of components including languages, libraries, daemons,\nand utility programs\n\n\nThe steps in writing a client and a server in DCE RPC.\nWriting a Client and a Server\n1. UUIDGEN generates a unique user\nidentifier and an IDL file with that\nidentifier\n2. The identifier is 128-bit binary\nnumber, so that it can never be\n\nnumber, so that it can never be\nreused\n3. Interface Definition Language (IDL)\nfile is than allowed to be updated\ncompiled to produce the stubs and\nthe header file\n\nThe steps in writing a client and a server in DCE\nRPC.\n4. Server and client codes are then written and\ncompiled with corresponding stub and the header\nfile to be linked with the DCE RPC library, to\nproduce the binaries to be run\n5.\nDCE\ndaemon\nmaintains\na\ntable\nfor\nserver\nWriting a Client and a Server\nendpoints\n6.\nA directory server maintains the address of the\nregistered servers with their network addresses.\n7.\nClient get the network address of the machine\nrunning the server from the Directory Service and\nasks the Daemon for the endpoint of the server to\nsend a request‚Ä¶\n\n\nClient-to-server binding in DCE.\nBinding a Client to a Server\n\n\nThank You\nThank You"
    }
  ]
}