{
  "subject": "Minor (CE) Data Science-Computational Intelligence-1",
  "files": [
    {
      "filename": "CI Syllabus.pdf",
      "path": "data/materials\\Minor (CE) Data Science-Computational Intelligence-1\\CI Syllabus.pdf",
      "text": "Subject\nCode\nSubject\nName\nTheory\nHrs\nPractical\nHrs\nTutorial\nHrs\nTheory\nCredit\nPractical/Oral\nCredit\nTutorial\nCredits\nTotal\nCredits\nCEMDC701\nComputat\nional\nIntelligenc\ne\n\n-\n-\n\n-\n-\n\nSubject Code\nSubject\nName\nExamination Scheme\nTheory Marks\nTerm\nWork\nPractical\n& Oral\nOral\nTotal\nIn-Sem Evaluations\nEnd\nSem\nExam\nIA1\nIA2\nAVG\nMid\nSem\nExam\nCEMDC701\nComputat\nional\nIntelligenc\ne\n\n\n\n\n\n-\n-\n-\n\nCourse Objectives:\n1. To familiarize the students with basics of ANN, optimization and deep Learning.\n2. To understand the various steps followed in solving Prediction problems and to choose the right prediction\nmethod to develop a model.\n3. To learn the modern heuristic optimization techniques and apply them for problem solving.\n4. To learn MLP BP Neural Network and deep learning methods and to design and develop ANN\nClassification systems.\n5. To learn the underlying theory of building hybrid systems for better decision making.\nCourse Outcomes: At the end of the course learner will able to\n1. Understand the basics of various CI techniques and to find a suitable classifier based on the given problem.\n2. Differentiate various Prediction methods and choose the right prediction method to develop a model.\n3. Analyse and apply the various optimization techniques to solving complex problems which cannot be\nsolved using traditional methods.\n4. Differentiate the various supervised and unsupervised ANN techniques and to choose a suitable technique\nto design and develop classification and regression problems.\n5.\nTo select, configure and to use the CNN and RNN deep learning networks.\n6. To build a hybrid model for better accuracy in decision making.\nPrerequisites:\nArtificial Intelligence, Machine Learning, Soft Computing, Image Processing, Computer Vision\n\nSr.\nNo.\nModule\nDetailed Content\nHours\nCO\nMapping\n\nSoft Computing\nIntroduction\nDifferentiate Hard and Soft Computing,\nSoft\nComputing\nConstituents\nand\ncharacteristics, Foundation of Prediction,\n\nCO1\n\nPrediction\nData Preparation, Different Prediction\nMethods,\nMathematical\nMethods,\nDistance based Methods, Logic Methods,\n\nCO2\n\nOptimization\nAnnealing, Tabu Search, Evolutionary\nAlgorithms\n-\nGenetic\nOptimization,\nParticle Swarm Optimization, Ant Colony\nOptimization\n\nCO3\n\nNeural Networks\nSupervised ANN\nSingle layer Feed forward networks, Multi\nlayer feed forward net, learning algorithms,\nback-propagation\nUnsupervised ANN\nSelf Organizing Maps, Learning Vector\nQuantization\n\nCO4\n\nDeep Networks\nConvolution Neural Network\nConvolutional Neural Networks, LeNet,\nAlexNet, VGGNet\nRecurrent and Recursive Nets\nBidirectional RNNs, Bidirectional RNNs,\nThe Long Short-Term Memory and Other\n\nCO%\n\nHybrid Systems\nHybrid Systems for Prediction, Hybrid\nSystems for Optimization\n\nCO6\n\nText Books:\n1. Andries P. Engelbrecht, “Computational Intelligence An Introduction, Second Edition, Wiley Publication\n2. Samir Roy and Chakraborty, “Introduction to soft computing”, Pearson Edition.\n3. Zbigniew Michalewicz, Martin Schmidt, Matthew Michalewicz, Constantin Chiriac, \"Adaptive Business\nIntelligence\", Springer Publication 2006\n4. Ian Goodfellow and Yoshua Bengio and Aaron, Deep Learning, An MIT Press book.\n5. S.N.Sivanandam, S.N.Deepa \"Principles of Soft Computing\" Second Edition, Wiley Publication\nReference Books:\n1.\nJ.S.R.Jang \"Neuro-Fuzzy and Soft Computing\", PHI 2003.\n2.\nS. Rajasekaran and G.A. Vijaylakshmi Pai., “Neural Networks Fuzzy Logic, and Genetic\nAlgorithms”, Prentice Hall of India.\n3.\nSatish Kumar, \"Neural Networks A Classroom Approach\" Tata McGrawHill.\nEvaluation Scheme:\n Assessment consists of two Internal Assessments (IA1, IA2) out of which; one should be compulsory\nclass test (on minimum 02 Modules) and the other is a class test / assignment on case studies / course\nproject.\n Mid Semester Examination (MSE) will be based on 40-50% of the syllabus.\nEnd-Semester Examination:\n Question paper will comprise of full syllabus.\n In the question paper, weightage of marks will be proportional to the total number of lecture hours as\nmentioned in the syllabus\n\nSubject\nCode\nSubject\nName\nTheory\nHrs\nPractical\nHrs\nTutorial\nHrs\nTheory\nCredit\nPractical/Oral\nCredit\nTutorial\nCredits\nTotal\nCredits\nCEMDL701\nComputat\nional\nIntelligenc\ne Lab\n\n\n\nSubject Code\nSubject\nName\nExamination Scheme\nTheory Marks\nTerm\nWork\nPractical\n& Oral\nOral\nTotal\nIn-Sem Evaluations\nEnd\nSem\nExam\nIA1\nIA2\nAVG\nMid\nSem\nExam\nCEMDL701\nComputat\nional\nIntelligenc\ne Lab\n\n\n\nCourse Objectives:\n1. To familiarize the students with basics of modern Computational Intelligence techniques.\n2. Enable the student to analyse the problem, identify the soft computing techniques to address the\nproblem, analyse and design solutions to solve the problem.\n3. To learn the tools and techniques of Computational Intelligence and apply them to solve the problem\nCourse Outcomes: At the end of the course learner will able to\n1. Analyse a given problem, identify the soft computing techniques to address the problem.\n2. Perform pre-processing, design and Implement a Prediction model for the above application.\n3. Analyse the problem, identify the Optimization techniques to address the problem, design and\nimplement the optimization model.\n4. Analyse the problem, identify the suitable NN techniques to address the problem, design and implement\nthe NN model\n5. Analyse the problem, identify the suitable Deep Learning techniques to address the problem, design and\nimplement the model\n6. Analyse the problem, identify the suitable models to build a hybrid module that can perform better than\nthe individual models.\nPrerequisites:\nMachine Learning, Soft Computing, Image Processing, Computer Vision\n\nSuggested Experiments:\nSr.\nNo.\nModule\nDetailed Content\nHours\nCO\nMapping\n\n\nChoose an application, analyse the problem, identify the\nsoft computing techniques to address the problem\n\nLO1\n\n\nPerform pre-processing, design and Implement a\nPrediction model for the above application\n\nLO2\n\n\nChoose an application, analyse the problem, identify the\nOptimization techniques to address the problem\n\nLO2\n\n\nDesign and Implement an Optimization model for the\nabove application\n\nLO2\n\n\nChoose an application, analyse the problem, identify the\nNN technique to address the problem\n\nLO3\n\n\nDesign and Implement an NN model for the above\napplication\n\nLO4\n\n\nChoose an application, analyse the problem, identify the\nDeep learning technique to address the problem\n\nLO4\n\n\nDesign and Implement an Deep learning NN model for\nthe above application\n\nLO4\n\n\nChoose an application, analyse the problem, identify the\nneed for Hybrid technique to address the problem\n\nLO5\n\n\nDesign and Implement an Hybrid model for the above\napplication\n\nLO6\nText Books:\n1. Jake VanderPlas, Python Data Science Handbook: Essential Tools for Working with Data, O’reilly\nPublication\n2. Wes McKinney , Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython,\nO’reilly Publication\n3. Jason Test, PYTHON FOR DATA SCIENCE\n4. Jason Test, Python Programming: 3 BOOKS IN 1\nTerm Work:\nThe Term work Marks are based on the weekly experimental performance of the students, Oral\nperformance and regularity in the lab.\nStudents are expected to be prepared for the lab ahead of time by referring the manual and perform\nthe experiment under the guidance and discussion. Next week the experiment write-up to be\ncorrected along with oral examination.\nEnd Semester Examination:\nEnd of the semester, there will be Oral examination based on the laboratory work and the\ncorresponding theory syllabus."
    },
    {
      "filename": "CI_Lab Manual.pdf",
      "path": "data/materials\\Minor (CE) Data Science-Computational Intelligence-1\\CI_Lab Manual.pdf",
      "text": "ODD Semester\nDepartment of Computer Engineering\nLab Manual\nFinal Year Semester-VII\nSubject: Computational Intelligence Lab\nDepartment of Computer Engineering\ni\nInstitutional Vision, Mission and Quality Policy\nOur Vision\nTo foster and permeate higher and quality education with value added engineering, technology programs,\nproviding all facilities in terms of technology and platforms for all round development with societal awareness\nand nurture the youth with international competencies and exemplary level of employability even under highly\ncompetitive environment so that they are innovative adaptable and capable of handling problems faced by our\ncountry and world at large.\nOur Mission\nThe Institution is committed to mobilize the resources and equip itself with men and materials of excellence\nthereby ensuring that the Institution becomes pivotal center of service to Industry, academia, and society with\nthe latest technology. RAIT engages different platforms such as technology enhancing Student Technical\nSocieties, Cultural platforms, Sports excellence centers, Entrepreneurial Development Center and Societal\nInteraction Cell. To develop the college to become an autonomous Institution & deemed university at the\nearliest with facilities for advanced research and development programs on par with international standards. To\ninvite international and reputed national Institutions and Universities to collaborate with our institution on the\nissues of common interest of teaching and learning sophistication.\nOur Quality Policy\nOur Quality Policy\nIt is our earnest endeavour to produce high quality engineering professionals who are\ninnovative and inspiring, thought and action leaders, competent to solve problems faced\nby society, nation and world at large by striving towards very high standards in learning,\nteaching and training methodology.\nOur Motto: If it is not of quality, it is NOTRAIT!\nDepartment of Computer Engineering\nii\nDepartmental Vision, Mission\nVision\nTo impart higher and quality education in computer science with value added engineering and technology\nprograms to prepare technically sound, ethically strong engineers with social awareness. To extend the facilities,\nto meet the fast changing requirements and nurture the youths with international competencies and exemplary\nlevel of employability and research under highly competitive environments.\nMission\n\nTo mobilize the resources and equip the institution with men and materials of excellence to provide\nknowledge and develop technologies in the thrust areas of computer science and Engineering.\n\nTo provide the diverse platforms of sports, technical, co-curricular and extracurricular activities for the\noverall development of student with ethical attitude.\n\nTo prepare the students to sustain the impact of computer education for social needs encompassing\nindustry, educational institutions and public service.\n\nTo collaborate with IITs, reputed universities and industries for the technical and overall upliftment of\nstudents for continuing learning and entrepreneurship.\nDepartment of Computer Engineering\niii\nDepartmental Program Educational Objectives\n(PEOs)\n1. Learn and Integrate\nTo provide Computer Engineering students with a strong foundation in the mathematical, scientific and\nengineering fundamentals necessary to formulate, solve and analyze engineering problems and to\nprepare them for graduate studies.\n2. Think and Create\nTo develop an ability to analyze the requirements of the software and hardware, understand the\ntechnical specifications, create a model, design, implement and verify a computing system to meet\nspecified requirements while considering real-world constraints to solve real world problems.\n3. Broad Base\nTo provide broad education necessary to understand the science of computer engineering and the\nimpact of it in a global and social context.\n4. Techno-leader\nTo provide exposure to emerging cutting edge technologies, adequate training & opportunities to work\nas teams on multidisciplinary projects with effective communication skills and leadership qualities.\n5. Practice citizenship\nTo provide knowledge of professional and ethical responsibility and to contribute to society through\nactive engagement with professional societies, schools, civic organizations or other community\nactivities.\n6. Clarify Purpose and Perspective\nTo provide strong in-depth education through electives and to promote student awareness on the life-\nlong learning to adapt to innovation and change, and to be successful in their professional work or\ngraduate studies.\nDepartment of Computer Engineering\niv\nDepartmental Program Outcomes (POs)\nPO1: Engineering knowledge: Apply the knowledge of mathematics, science, engineering\nfundamentals, and an engineering specialization to the solution of complex engineering problems.\nPO2: Problem analysis: Identify, formulate, review research literature, and analyze complex\nengineering problems reaching substantiated conclusions using first principles of mathematics,\nnatural sciences, and engineering sciences..\nPO3: Design/development of solutions: Design solutions for complex engineering problems and\ndesign system components or processes that meet the specified needs with appropriate\nconsideration for the public health and safety, and the cultural, societal, and environmental\nconsiderations.\nPO4: Conduct investigations of complex problems: Use research-based knowledge and\nresearch methods including design of experiments, analysis and interpretation of data, and\nsynthesis of the information to provide valid conclusions.\nPO5: Modern tool usage: Create, select, and apply appropriate techniques, resources, and modern\nengineering and IT tools including prediction and modeling to complex engineering activities with\nan understanding of the limitations.\nPO6: The engineer and society: Apply reasoning informed by the contextual knowledge to assess\nsocietal, health, safety, legal and cultural issues and the consequent responsibilities relevant to the\nprofessional engineering practice.\nPO7: Environment and sustainability: Understand the impact of the professional engineering\nsolutions in societal and environmental contexts, and demonstrate the knowledge of, and need for\nsustainable development.\nPO8: Ethics: Apply ethical principles and commit to professional ethics and responsibilities and\nnorms of the engineering practice.\nPO9: Individual and team work: Function effectively as an individual, and as a member or\nleader in diverse teams, and in multidisciplinary settings.\nPO10: Communication: Communicate effectively on complex engineering activities with the\nengineering community and with society at large, such as, being able to comprehend and write\neffective reports and design documentation, make effective presentations, and give and receive\nclear instructions.\nPO11: Project management and finance: Demonstrate knowledge and understanding of the\nengineering and management principles and apply these to one’s own work, as a member and\nleader in a team, to manage projects and in multidisciplinary environments.\nPO12: Life-long learning: Recognize the need for, and have the preparation and ability to\nengage in independent and life-long learning in the broadest context of technological change.\nDepartment of Computer Engineering\nv\nProgram Specific Outcomes (PSOs)\nPSO1: To build competencies towards problem solving with an ability to understand, identify, analyze\nand design the problem, implement and validate the solution including both hardware and software.\nPSO2: To build appreciation and knowledge acquiring of current computer techniques with an ability\nto use skills and tools necessary for computing practice.\nPSO3: To be able to match the industry requirements in the area of computer science and engineering.\nTo equip skills to adopt and imbibe new technologies.\nDepartment of Computer Engineering\nvi\nIndex\nSr. No.\nContents\nPage No.\n1.\nList of Experiments\n\n2.\nCourse Objectives, Course Outcomes and\nExperiment Plan\n\n3.\nStudy and Evaluation Scheme\n\n4.\nExperiment No. 1\n\n5.\nExperiment No. 2\n\n6.\nExperiment No. 3\n\n7.\nExperiment No. 4\n\n8.\nExperiment No. 5\n\n9.\nExperiment No. 6\n\n10.\nExperiment No. 7\n\n11.\nExperiment No. 8\n\n12.\nExperiment No. 9\n\n13.\nExperiment No. 10\n\n\nExperiment No. 11\n\nDepartment of Computer Engineering\n\nList of Experiments\nSr. No.\nExperiments Name\n\nCase study on soft computing and its applications.\n\nPerform pre-processing, design and implement logistic regression model for\nsolar radiation prediction.\n\nAnalyze solar radiation prediction dataset to optimize features using Genetic\nalgorithm.\n\nAnalyze solar radiation prediction dataset to optimize features using Particle\nSwarm Optimization.\n\nDesign and implement feedforward backpropagation to classify solar radiation\npreiction using (a) single layer perceptron and (b) multi-layer perceptron.\n\nDesign and implement convolution neural network (CNN) to classify medical\nimages to detect raspiratory diseases.\n\nImplement transfer learning to classify medical images to detect respiratory\ndiseases.(VGGNet)\n\nDesign and implement PSO-MLP model for the solar radiation prediction.\n\nVLAB –Inception ResNet\n\nVLAB – Backpropagation OR SVM\n\nContent beyond Syllabus\nDepartment of Computer Engineering\n\nCourse Outcome & Experiment Plan\nCourse Outcomes:\nCO1\nAnalyse a given problem, identify the soft computing techniques to address the problem.\nCO2\nPerform pre-processing, design and Implement a Prediction model for the above application.\nCO3\nAnalyse the problem, identify the Optimization techniques to address the problem, design and\nimplement the optimization model.\nCO4\nAnalyse the problem, identify the suitable NN techniques to address the problem, design and\nimplement the NN model\nCO5\nAnalyse the problem, identify the suitable Deep Learning techniques to address the problem,\ndesign and implement the model\nCO6\nAnalyse the problem, identify the suitable models to build a hybrid module that can perform\nbetter than the individual models.\nDepartment of Computer Engineering\n\nExperiment Plan:\nModule\nNo.\nWeek\nNo.\nExperiments Name\nCourse\nOutcome\n\nW1,W2\nCase study on soft computing and its applications.\nCO1\n\nW3\nPerform pre-processing, design and implement logistic\nregression model for solar radiation prediction.\nCO2\n\nW4\nAnalyze solar radiation prediction dataset to optimize\nfeatures using Genetic algorithm.\nCO2\n\nW5\nAnalyze solar radiation prediction dataset to optimize\nfeatures using Particle Swarm Optimization.\nCO3\n\nW6\nDesign and implement feedforward backpropagation to\nclassify solar radiation preiction using (a) single layer\nperceptron and (b) multi-layer perceptron.\nCO3\n\nW7\nDesign and implement convolution neural network\n(CNN) to classify medical images to detect raspiratory\ndiseases.\nCO4\n\nW8\nImplement transfer learning to classify medical images to\ndetect respiratory diseases.(VGGNet)\nCO5\n\nW9\nDesign and implement PSO-MLP model for the solar\nradiation prediction.\nCO5\n\nW10\nVLAB –Inception ResNet\nCO6\n\nW11\nVLAB – Backpropagation OR SVM\nCO6\n\nW12\nContent beyond Syllabus\nDepartment of Computer Engineering\n\nMapping of Course outcomes with Program outcomes:\nSubject\nWeight\nCourse Outcomes\nContribution to Program outcomes (PO)\n\n\n\n\n\n\n\n\n\n\n\n\nCO1\nAnalyse a given problem, identify the soft\ncomputing techniques to address the\nproblem.\n\n\n\n\n\n\n\n\n\nCO2\nPerform pre-processing, design and\nImplement a Prediction model for the above\napplication.\n\n\n\n\n\n\nPRATICAL\n40%\nCO3\nCO3: Analyse the problem, identify the\nOptimization techniques to address the\nproblem, design and implement the\noptimization model.\n\n\n\n\n\n\nCO4\nCO4: Analyse the problem, identify the\nsuitable NN techniques to address the\nproblem, design and implement the NN\nmodel\n\n\n\n\n\n\nCO5\nCO5: Analyse the problem, identify the\nsuitable Deep Learning techniques to\naddress the problem, design and implement\nthe model\n\n\n\n\n\n\n\nCO6 CO6: Analyse the problem, identify the\nsuitable models to build a hybrid module\nthat can perform better than the individual\nmodels.\n\n\n\n\n\n\nDepartment of Computer Engineering\n\nMapping of Course outcomes with Program Specific outcomes:\nCourse Outcomes\nContribution to\nProgram Specific\noutcomes\n(PSO)\n\n\n\nCO1\nAnalyse a given problem, identify the soft computing techniques to\naddress the problem.\n\n\n\nCO2\nPerform pre-processing, design and Implement a Prediction model for\nthe above application.\n\n\n\nCO3 Analyse the problem, identify the Optimization techniques to address\nthe problem, design and implement the optimization model.\n\n\n\nCO4 Analyse the problem, identify the suitable NN techniques to address\nthe problem, design and implement the NN model\n\n\n\nCO5 Analyse the problem, identify the suitable Deep Learning techniques to\naddress the problem, design and implement the model\n\n\n\nCO6 Analyse the problem, identify the suitable models to build a hybrid\nmodule that can perform better than the individual models.\n\n\n\nDepartment of Computer Engineering\n\nStudy and Evaluation Scheme\nCourse\nCode\nCourse Name\nTeaching Scheme\nCredits Assigned\nCEMDL\n\nComputational\nIntelligence\nLab\nTheory Practical Tutorial Theory Practical Tutorial Total\n\n\n--\n\n\n--\n\nCourse Code\nCourse Name\nExamination Scheme\nCEMDL701\nComputational\nIntelligence\nLab\nTerm Work\nOral &\nPractical\nTotal\n\n\n\nTerm Work:\n1. The Term work Marks are based on the weekly experimental performance of the students,\nOral performance and regularity in the lab.\n2. Students are expected to be prepared for the lab ahead of time by referring the manual and\nperform the experiment under the guidance and discussion. Next week the experiment\nwrite-up to be corrected along with oral examination.\nPractical & Oral:\n1. Practical and Oral exam will be based on the entire syllabus of Computational Intelligence.\nDepartment of Computer Engineering\n\nComputational Intelligence\nExperiment No.: 1\nCASE STUDY ON SOFT\nCOMPUTING AND ITS\nAPPLICATIONS\nDepartment of Computer Engineering\n\nExperiment No. 1\n1. Aim: Case study on Soft Computing and Its Applications\n2. Objectives: From this experiment, the student will be able to\n Showcase how computational intelligence techniques are applied to\nsolve real-world problems in diverse domains such as healthcare,\nfinance, robotics, and environmental science.\n Assess and compare the performance of different computational\nmethods in specific scenarios, highlighting strengths and\nweaknesses.\n Analyse complex systems and behaviours, providing insights into\nhow computational intelligence can model and predict outcomes.\n Identify novel applications or improvements to existing algorithms,\nencouraging innovation in computational techniques.\n Explore how computational intelligence intersects with other fields\n(e.g., machine learning, artificial intelligence, data mining),\npromoting a broader understanding of the discipline.\n3. Outcomes: The learner will be able to\n Improve comprehension of how computational intelligence\nalgorithms work and their applications in solving real-world\nproblems.Applying fundamental engineering concepts appropriate to\nthe discipline.\n Evaluate strengths and weaknesses of various algorithms, helping\nresearchers and practitioners choose the most suitable methods for\nspecific applications.\n4. Software Required: Google Colab/Python/R\n5. Theory\nIntroduction :\nSoft computing is a collection of computational techniques designed to\nsolve complex problems that involve uncertainty, imprecision, and\napproximation. Unlike traditional computing methods that rely on rigid\nalgorithms and precise inputs, soft computing embraces a more flexible\napproach, allowing for the handling of real-world data that is often\nuncertain or vague.\nKey Components of Soft Computing:\n Fuzzy Logic:\nDepartment of Computer Engineering\n\n Fuzzy logic deals with reasoning that is approximate rather than\nfixed and exact. It allows for degrees of truth rather than the usual\ntrue/false binary. This is particularly useful in situations where\ninformation is uncertain or incomplete, enabling systems to make\nmore human-like decisions.\n Neural Networks:\n Inspired by the human brain, neural networks consist of\ninterconnected nodes (neurons) that process data. They are\nparticularly effective in recognizing patterns, learning from data, and\nmaking predictions. Neural networks are widely used in applications\nlike image and speech recognition.\n Genetic Algorithms:\n Genetic algorithms are optimization techniques based on the\nprinciples of natural selection and genetics. They are used to solve\noptimization problems by iteratively improving candidate solutions\nthrough processes such as selection, crossover, and mutation.\n Probabilistic Reasoning:\n This involves making inferences based on the likelihood of events.\nTechniques like Bayesian networks allow for the modeling of\nuncertainty and are widely used in areas such as machine learning\nand decision-making.\n● Applications of Soft Computing:\n1. Healthcare\nEnhancing Diagnostic Systems: Soft computing techniques, especially\nfuzzy logic and neural networks, are used to develop intelligent\ndiagnostic systems that can analyze patient data and identify diseases\nmore accurately. These systems can handle the uncertainty and\nvariability of medical symptoms, leading to better diagnoses.\n2. Finance:\nRisk Assessment: Financial institutions use soft computing techniques\nto assess the risk of investments and loans. Fuzzy logic models can\nevaluate various factors influencing risk, helping lenders make\ninformed decisions based on imprecise and uncertain data.\nFraud Detection: Neural networks and machine learning algorithms are\nemployed to identify fraudulent transactions. By analyzing patterns in\ntransaction data, these systems can detect anomalies that may indicate\nfraud, enabling quick responses to prevent financial loss.\nAlgorithmic Trading: Soft computing methods can optimize trading\nstrategies by analyzing vast amounts of market data. Genetic\nalgorithms, for instance, can evolve trading strategies based on\nhistorical data, allowing traders to adapt to changing market\nDepartment of Computer Engineering\n\nconditions.\n3. Engineering:\nControl Systems: Soft computing techniques, particularly fuzzy logic\nand neural networks, are used in control systems to manage\nprocesses that require a level of human-like reasoning. For example,\nin industrial automation, these systems can control machinery and\nprocesses under varying conditions.\nOptimization Problems: Soft computing is utilized to solve complex\noptimization problems in engineering, such as design optimization\nand resource allocation. Genetic algorithms can explore a vast\nsolution space efficiently, identifying optimal or near-optimal\nsolutions.\nFault Diagnosis: In engineering systems, soft computing can help\ndetect and diagnose faults. Fuzzy logic systems can analyze sensor\ndata and make decisions about system health, enabling early\ndetection of potential failures and minimizing downtime.\n4. Natural Language Processing (NLP):\nSentiment Analysis: Soft computing techniques, including neural\nnetworks and fuzzy logic, are employed to determine the sentiment\nbehind text, such as social media posts or customer reviews. These\nsystems can classify sentiments as positive, negative, or neutral,\nhelping businesses understand customer opinions.\nTranslation: Soft computing methods are used in machine translation\nsystems to improve the accuracy of translations between languages.\nNeural networks, particularly deep learning models, can learn\ncomplex language patterns and context, leading to more fluent\ntranslations.\nInformation Retrieval: Soft computing techniques enhance search\nengines and information retrieval systems by improving the\nrelevance of search results. By analyzing user queries and document\ncontent, these systems can provide more accurate and context-aware\nresults.\n● Advantages of soft computing:\n1. Flexibility\nHandling Imprecision and Uncertainty:\nSoft computing techniques are inherently designed to work with\nDepartment of Computer Engineering\n\nimprecise, uncertain, and vague data, which are common in real-world\napplications. For example, fuzzy logic allows for reasoning about\nconcepts that cannot be precisely defined, such as \"tall\" or \"hot.\" This\nflexibility makes soft computing particularly valuable in fields like\nhealthcare, finance, and engineering, where data can be noisy or\nambiguous.\nReal-World Application Suitability:\nBecause soft computing can process a wide range of data types and\nstructures, it is suitable for applications in dynamic environments. For\ninstance, in weather forecasting, data may be incomplete or subject to\nrapid changes, but soft computing methods can still generate useful\npredictions by accommodating uncertainties.\n2. Robustness\nPerformance with Noisy or Incomplete Data:\nSoft computing techniques are robust and can maintain performance\neven when the input data is noisy, missing, or incomplete. For\nexample, neural networks can be trained to recognize patterns despite\nthe presence of noise, making them effective in applications like\nspeech recognition or image classification, where background noise or\ndistortion may occur.\n3. Adaptability\nLearning from Data:\nSoft computing techniques, particularly machine learning and neural\nnetworks, have the ability to learn from data. They can improve their\nperformance as they are exposed to more information, allowing them\nto adapt to new patterns or changes in the environment. For example, a\nrecommendation system can refine its suggestions for users based on\ntheir interactions over time.\n6. Conclusion:\nSoft computing represents a significant advancement in computational\nintelligence, providing powerful tools for modeling and solving\ncomplex problems in various domains. Its ability to mimic human\nreasoning and decision-making processes makes it an essential\napproach in today’s data-driven world.\nDepartment of Computer Engineering\n\n7. Viva Questions:\n1. What is the primary objective of your case study?\n2. Can you summarize the main findings of your case study?\n3. Why did you choose this specific case for your study?\n4. What are the key challenges you encountered during your research?\n8. Additional Learning:\n8 References:\nBooks\n1. Russell, S., & Norvig, P. (2016). Artificial Intelligence: A Modern\nApproach. Pearson.\n2. Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.\nJournals and Articles\n3. Zadeh, L. A. (1965). Fuzzy Sets. Information and Control, 8(3), 338-\n353.\n4. Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization,\nand Machine Learning. Addison-Wesley.\nDepartment of Computer Engineering\n\nComputational Intelligence\nExperiment No.: 2\nPerform pre-processing, design and implement\nlogistic regression model for solar radiation\nprediction.\nDepartment of Computer Engineering\n\nExperiment No. 2\n1. Aim: Perform pre-processing, design, and implement a logistic regression model\nfor solar radiation prediction.\n2. Objectives: From this experiment, the student will be able to\n Understand the preprocessing of data for logistic regression.\n Understand use of logistic regression by implementing a model for real-time\ndatasets.\n3. Outcomes: The learner will be able to\n Understand, identify, analyze, and design the problem, implement and\nvalidate the solution for regression\n Applying fundamental engineering concepts appropriate to the discipline.\n Potential to formulate and solve engineering problems.\n4. Software Required: Google Colab/Python/R\n5. Theory:\nLogistic regression is a statistical machine learning algorithm that\nclassifies the data by considering outcome variables on extreme ends and\ntries to make a logarithmic line that distinguishes between them. Logistic\nregression is another powerful supervised ML algorithm used for binary\nclassification problems (when a target is categorical). The best way to\nthink about logistic regression is that it is a linear regression but for\nclassification problems. Logistic regression essentially uses a logistic\nfunction defined below to model a binary output variable. The primary\ndifference between linear regression and logistic regression is that logistic\nregression's range is bounded between 0 and 1.\nTypes of logistic regression\nBinary (eg. Tumor Malignant or Benign)\nMulti-linear functions failsClass (eg. Cats, dogs or Sheep's)\n● Logistic Regression\nDepartment of Computer Engineering\n\nLinear Regression VS Logistic Regression Graph| Image\nWe can call a Logistic Regression a Linear Regression model but the Logistic\nRegression uses a more complex cost function, this cost function can be defined\nas the ‘Sigmoid function’ or also known as the ‘logistic function’ instead of a\nlinear function. In the logistic function equation, x is the input variable.\n● Sigmoid Function\nIn order to map predicted values to probabilities, we use the Sigmoid function.\nThe function maps any real value into another value between 0 and 1. In machine\nlearning,\nwe\nuse\nsigmoid\nto\nmap\npredictions\nto\nprobabilities.\nSigmoid Function Graph\n6.\nResult:\nDepartment of Computer Engineering\n\n7.\nConclusion:\nThis experiment aims to predict solar radiation using a logistic regression model after\nperforming pre-processing techniques. By selecting relevant features, normalizing the\ndata, and designing the logistic regression model, we can obtain a reliable prediction\nmodel for solar radiation. The evaluation of the model's performance using appropriate\nmetrics will provide insights into the effectiveness of the logistic regression approach for\nsolar radiation prediction.\n8.\nAdditional Learning:\n9.\nViva Questions:\n What is Logistic Regression?\n What are different applications of Logistic Regression?\n10.\nReferences:\n1. Jake Vander Plas, Python Data Science Handbook: Essential Tools for Working with\nData, O’reilly Publication\n2. Wes McKinney , Python for Data Analysis: Data Wrangling with Pandas, NumPy, and\nIPython, O’reilly Publication\n3. Jason Test, PYTHON FOR DATASCIENCE\n4. Jason Test, Python Programming: 3 BOOKS IN1\nDepartment of Computer Engineering\n\nComputational Intelligence\nExperiment No.: 3\nGENETIC ALGORITHM\nDepartment of Computer Engineering\n\nExperiment 3\n1. Aim: Analyze solar radiation prediction dataset to optimize features using Genetic\nalgorithm.\n2.\nObjectives:\n● To familiarize with Mathematical foundations for Genetic algorithm, operator.\n● To study the Applications of Genetic Algorithms.\n3.\nOutcomes: The student will be able to,\n● Creating an understanding about the way the GA is used and the domain of\napplication.\n● To appreciate the use of various GA operators in solving different types of GA\nproblems.\n● Match the industry requirements in the domains of Programming and\nNetworking with the required management skills.\n4.\nSoftware Required: Python/MATLAB\n5.\nTheory:\nGenetic algorithm is a search technique used in computing to find true or\napproximate solutions to approximate solutions to optimization & search problems.\nGenetic algorithms are inspired by Darwin's theory about evolution. Solution to a\nproblem solved by genetic algorithms is evolved. Algorithms are started with a set\nof solutions (represented by chromosomes) called population. Solutions from one\npopulation are taken and used to form a new population. This is motivated by a\nhope that the new population will be better than the old one. Solutions which are\nselected to form new solutions (offspring) are selected according to their fitness -\nthe more suitable they are the more chances they have to reproduce. This is repeated\nuntil some condition (for example number of populations or improvement of the\nbest solution) is satisfied.\nTerminology for Genetic Algorithm\nDepartment of Computer Engineering\n\nPopulation contains a set of possible solutions for the stochastic search process to begin.\nGA will iterate over multiple generations till it finds an acceptable and optimized solution.\nFirst-generation is randomly generated.\nChromosome represents one candidate solution present in the generation or population.\nA chromosome is also referred to as a Genotype. A chromosome is composed of Genes\nthat contain the value for the optimal variables.\nPhenotype is the decoded parameter list for the genotype that is processed by the Genetic\nAlgorithm. Mapping is applied to the genotype to convert to a phenotype.\nThe Fitness function or the objective function evaluates the individual solution or\nphenotypes for every generation to identify the fittest members.\nOutline of the Basic Genetic Algorithm\n1. [Start] Generate random population of n chromosomes (suitable solutions for the\nproblem)\n2. [Fitness] Evaluate the fitness f(x) of each chromosome x in the population\n3. [New population] Create a new population by repeating following steps until the new\npopulation is complete\na. [Selection] Select two parent chromosomes from a population according to their\nfitness (the better fitness, the bigger chance to be selected)\nb. [Crossover] With a crossover probability cross over the parents to form a new\noffspring (children). If no crossover was performed, offspring is an exact copy of parents.\nc. [Mutation] With a mutation probability mutate new offspring at each locus\n(position in chromosome).\n4. [Accepting] Place new offspring in a new population 5. [Replace] Use new generated\npopulation for a further run of algorithm\n6. [Test] If the end condition is satisfied, stop, and return the best solution in current\npopulation 7. [Loop] Go to step 2\nFlowchart:\nDepartment of Computer Engineering\n\n6.\nResult:\n7.\nConclusion:\n The solar radiation prediction dataset is optimized using the genetic algorithms which\ninclude all the genetic algorithm operators. Genetic algorithm includes the selection,\ncrossover, mutation operators along with fitness function.\n8.\nAdditional Learning:\n9.\nViva Questions:\n What is a genetic algorithm, and how does it mimic natural evolution?\n Can you explain the basic components of a genetic algorithm?\n What are the key differences between genetic algorithms and traditional optimization\ntechniques?\n Describe the typical steps involved in a genetic algorithm.\n10.\nReferences:\nBooks\n1. Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization, and Machine\nLearning. Addison-Wesley.\n2. Mitchell, M. (1998). An Introduction to Genetic Algorithms. MIT Press.\n3. Fogel, D. B. (2006). Evolutionary Computation: A Unified Approach. IEEE Press.\n4. Koza, J. R. (1992). Genetic Programming: On the Programming of Computers by\nMeans of Natural Selection. MIT Press.\nJournal Articles\n5. Holland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of\nMichigan Press.\n6. Whitley, D. (1994). A Genetic Algorithm Tutorial. Statistics and Computing, 4(2),\n65-85.\n7. Deb, K. (2001). Multi-Objective Optimization Using Evolutionary Algorithms.\nWiley.\nDepartment of Computer Engineering\n\nExperiment No.: 4\nParticle Swarm Optimization\nDepartment of Computer Engineering\n\nExperiment 4\n1.\nAim: Analyze solar radiation prediction dataset to optimize features using Particle\nSwarm Optimization (PSO).\n2.\nObjectives:\n- To familiarize with Mathematical foundations for PSO algorithms.\n- To study the Applications PSO Algorithms.\n- Understanding the basic principles of PSO can offer insights for both novices\nand experts in the machine learning domain\n3.\nOutcomes:\nThe student will be able to,\n- Creating an understanding about the PSO algorithm is used and the domain\nof application.\n- To understand the use of various types of PSO problems.\n4.\nSoftware Required: Python/MATLAB\n5.\nTheory:\n- Particle Swarm Optimization (PSO) drawing inspiration from the collective\nintelligence of birds and fish, PSO is a powerful meta-heuristic algorithm that\nhas become a cornerstone strategy for tackling optimization problems.\n- Its underlying mechanism allows particles to dynamically adjust their\nvelocities based on personal and collective achievements, which results in a\nblend of individual effort and communal insight.\n- Particle Swarm Optimization is a remarkable computational technique that\nharnesses the collective intelligence of natural organisms to tackle\noptimization challenges.\n- PSO is a fantastic tool that can help to navigate complex functions and find\nglobal maxima with precision.\n- The basic concept of the PSO algorithm is to create a swarm of particles\nwhich move in the space around them searching for their goal or the place\nwhich best suits their needs given by a fitness function.\nThere are two main ideas behind its optimization properties:\n1. A single particle which can be seen as a potential solution to the problem can\ndetermine how good its current position is. It not only from its problem space\nDepartment of Computer Engineering\n\nexploration knowledge but also from the knowledge obtained and shared by\nthe other particles.\n2. A stochastic factor in each particle's velocity makes them move through\nunknown problem space regions. This property combined with a good initial\ndistribution of the swarm enable an extensive exploration of the problem\nspace and gives a very high chance of finding the best solutions efficiently.\nAdvantages and Applications:\nPSO is a flexible and easy-to-implement algorithm that doesn’t require hyper\nparameter tuning. These characteristics make it a versatile tool for numerous\noptimization problems, like neural network training, optimization of electric\npower distribution networks, structural optimization, and system identification\nin biomechanics.\nChallenges:\nDespite its benefits, PSO has some limitations, such as slow convergence during\nthe refined search stage, which can lead to weaker local search capability.\nKey Components:\nInertia Weight (w): Affects the balance between exploring the search space and\nexploiting known good areas. A higher weight promotes global exploration,\nwhile a lower weight favors local exploitation.\nCognitive (c1) and Social (c2) Coefficients: Determine the influence of a\nparticle’s personal best and the global best on its velocity. Higher values of\nc1 encourage individual learning, while higher values of c2 promote group\nlearning and exploration.\nSwarm and Neighborhood Size: Affects the diversity and convergence speed\nof the swarm. Larger swarms cover more search space but increase\ncomputational complexity. The neighborhood size dictates the extent of\ninformation sharing among particles, influencing the swarm’s convergence\nbehavior.\nDepartment of Computer Engineering\n\nAlgorithm\nDepartment of Computer Engineering\n\nParticle Swarm Optimization (PSO) algorithm implemented in Python:\nimport random\nimport numpy as np\n# Objective function\ndef objective_function(x):\nreturn x[0]**2 + x[1]**2 + 1\n# PSO parameters\nnum_dimensions = 2\nnum_particles = 30\nmax_iterations = 100\n# Initialize particles\nparticles = np.random.rand(num_particles, num_dimensions)\nvelocities = np.random.rand(num_particles, num_dimensions)\npersonal_best_positions = particles.copy()\npersonal_best_values = np.array([objective_function(x) for x in particles])\n# Initialize global best\nglobal_best_position = personal_best_positions[np.argmin(personal_best_values)]\nglobal_best_value = min(personal_best_values)\n# PSO loop\nfor iteration in range(max_iterations):\nfor i in range(num_particles):\n# Update velocities\nvelocities[i] = velocities[i] + \\\nrandom.random() * (personal_best_positions[i] - particles[i]) + \\\nrandom.random() * (global_best_position - particles[i])\n# Update particle positions\nparticles[i] += velocities[i]\n# Update personal bests\ncurrent_value = objective_function(particles[i])\nif current_value < personal_best_values[i]:\npersonal_best_positions[i] = particles[i]\npersonal_best_values[i] = current_value\n# Update global best\nif current_value < global_best_value:\nglobal_best_position = particles[i]\nglobal_best_value = current_value\nprint(f\"Best position: {global_best_position}\")\nprint(f\"Best value: {global_best_value}\")\nDepartment of Computer Engineering\n\n6.\nConclusion:\nThrough this inclusive exploration of Particle Swarm Optimization, we’ve\nexplored the foundational concepts and the vast applications of PSO,\nhighlighting both its strengths and the challenges it faces. Solving complex\noptimization problems across various domains.The solar radiation prediction\ndataset is optimized using the PSO algorithm.\n7.\nAdditional Learning:\n8.\nViva Questions:\n1.\nWhat is Particle Swarm Optimization (PSO)?\n2.\nWhat are the core principles behind Particle Swarm Optimization?\n3.\nWhat are the fundamental parameters that influence PSO?\n9.\nReferences:\n1. Jake VanderPlas, Python Data Science Handbook: Essential Tools for Working\nwith Data, O’reilly Publication\n2. Wes McKinney , Python for Data Analysis: Data Wrangling with Pandas,\nNumPy, and IPython, O’reilly Publication\n3. Jason Test, PYTHON FOR DATA SCIENCE\n4. Jason Test, Python Programming: 3 BOOKS IN 1\nDepartment of Computer Engineering\n\nComputational Intelligence\nExperiment No.: 5\nDesign and implement feedforward\nbackpropagation to classify solar radiation\nprediction using\n(a) single layer perceptron\nand\n(b) multi-layer perceptron.\nDepartment of Computer Engineering\n\nExperiment 5\n1.\nAim: Design and implement feedforward backpropagation to classify solar\nradiation prediction using (a) single layer perceptron and (b) multi-layer\nperceptron.\n2.\nObjectives:\n●\nTo familiarize with Mathematical foundations for ANN\n●\nTo study the Applications of Feed forward Back propagation .\n3.\nOutcomes: The student will be able to,\n●\nCreating an understanding about the way the ANN is used and the\ndomain of application.\n●\nTo appreciate the use of various ANN in solving different types of ANN\nproblems.\n●\nMatch the industry requirements in the domains of Programming and\nNetworking with the required management skills.\n4.\nSoftware Required: Python/MATLAB\n5.\nTheory:\nPerceptron is Machine Learning algorithm for supervised learning of various\nbinary classification tasks. Perceptron model is also treated as one of the best\nand simplest types of Artificial Neural networks. However, it is a supervised\nlearning algorithm of binary classifiers. Hence, we can consider it as a single-\nlayer neural network with four main parameters, i.e., input values ,weights\nand Bias, net sum, and an activation function.\nFig. Basic Components of Perceptron\nDepartment of Computer Engineering\n\nHow does Perceptron work?\nIn Machine Learning, Perceptron is considered as a single-layer neural\nnetwork that consists of four main parameters named input values (Input\nnodes), weights and Bias, net sum, and an activation function. The\nperceptron model begins with the multiplication of all input values and their\nweights, then adds these values together to create the weighted sum. Then\nthis weighted sum is applied to the activation function 'f' to obtain the\ndesired output. This activation function is also known as the step function\nand is represented by 'f'.\nTypes of Perceptron Models\nBased on the layers, Perceptron models are divided into two types. These\nare as follows:\n Single-layer Perceptron Model\n Multi-layer Perceptron model\n6.\nResult :\n7.\nConclusion:\nThis experiment aims to classify solar radiation using feedforward\nbackpropagation to classify solar radiation prediction using (a) single layer\nperceptron and (b) multi-layer perceptron and it is classified successfully.\n8.\nAdditional Learning:\nDepartment of Computer Engineering\n\n9.\nViva Questions\n1.\nWhat is the difference between a Single Layer Perceptron (SLP) and a\nMulti-Layer Perceptron (MLP)?\n2.\nCan you explain the working of a feedforward neural network?\n3.\nHow does backpropagation work? Can you walk me through the\nprocess?\n4.\nWhy do we use activation functions in neural networks, and which\nones are used in SLP and MLP?\n5.\nWhat are the advantages of using an MLP over an SLP for\nclassification tasks?\n10.\nReferences:\n1.\nNeural Networks and Deep Learning by Michael Nielsen: A great\nresource for understanding the theoretical aspects of neural networks.\n2.\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron\nCourville: A comprehensive guide to deep learning techniques,\nincluding backpropagation and MLPs.\n3.\nCoursera Neural Networks and Deep Learning Course: A detailed\nonline course on neural networks, including SLP and MLP.\n4.\nKhan Academy: Backpropagation Algorithm: Good for understanding\nthe basics of feedforward and backpropagation algorithms.\nDepartment of Computer Engineering\n\nExperiment No.: 6\nConvolution neural network (CNN)\nDesign and implement convolution neural\nnetwork (CNN) to classify medical images to\ndetect respiratory diseases\nDepartment of Computer Engineering\n\nExperiment 6\n1.\nAim: Design and implement convolution neural network (CNN) to classify\nmedical images to detect respiratory diseases\n2.\nObjectives:\n● To familiarize with Mathematical foundations for CNN, operator.\n● To study the Applications of CNN.\n3.\nOutcomes: The student will be able to,\n● Creating an understanding about the way the CNN is used and the domain of\napplication.\n● To appreciate the use of various GA operators in solving different types CNN.\n● Match the industry requirements in the domains of Programming and\nNetworking with the required management skills.\n4.\nSoftware Required: Python/MATLAB\n5.\nTheory:\nImage classification is the task of assigning a label or class to an input image.\nIt is a supervised learning problem, where a model is trained on a labeled\ndataset of images and their corresponding class labels, and then used to predict\nthe class label of new, unseen images.\nThere are many architectures for image classification, one of the most popular\nbeing convolutional neural networks (CNNs). CNNs are especially effective at\nimage classification because they are able to automatically learn the spatial\nhierarchies of features, such as edges, textures, and shapes, which are important\nfor recognizing objects in images.\nThis algorithm is inspired by the working of a part of the human brain which\nis the Visual Cortex. The visual Cortex is a part of the human brain which is\nresponsible for processing visual information from the outside world. It\nconsists of various layers, with each layer serving a specific function by\nextracting information from the image or visual. Ultimately, all the\ninformation gathered from each layer combines to interpret or classify the\nimage or visual.\nSimilarly, CNN utilizes various filters, with each filter extracting specific\ninformation from the image, such as edges and different shapes (vertical,\nhorizontal, round). These extracted features combine to help identify the\nimage.\nThe CNN model works in two steps: feature extraction and Classification\nFeature extraction is a phase where various filters and layers apply to the\nimages to extract information and features. Once this process is complete, the\nextracted data moves to the next phase, classification, where it is classified\nbased on the target variable of the problem.\nDepartment of Computer Engineering\n\nIn a neural network, the layers comprise interconnected nodes or neurons that\nprocess the input data and pass it through the network to produce an output:\nprocess is as shown in figure.\nCNN for Image Classification: How It Works\nCNNs consist of a series of interconnected layers that process the input data.\nThe first hidden layer of a CNN is usually a convolutional layer, which\napplies a set of filters to the input data to detect specific patterns. Each filter\ngenerates a feature map by sliding over the input data and performing\nelement-wise multiplication with the entries in the filter. These feature maps\nare then combined and passed through non-linear activation functions, such as\nthe ReLU function, which introduces non-linearities into the model and\nallows it to learn more complex patterns in the data.\nSubsequent layers in a CNN may include additional convolutional layers,\npooling layers, and fully-connected layers. Pooling layers reduce the size of\nthe feature maps. This helps reduce the overall number of parameters in the\nmodel and makes it more computationally efficient. Fully-connected layers\nare typically found after convolutional and pooling layers of a CNN. A fully-\nconnected layer connects all the neurons in a layer to all the neurons in the\nnext layer, allowing the model to learn possible non-linear combinations of\nthe features learned by the convolutional layers.\nThe final layer of a CNN is typically a softmax layer, which produces a\nprobability distribution across the possible class labels for the input data. The\nclass that has the highest probability is chosen as the prediction of the model.\nSteps for CNN:\nStep 1: Choose a Dataset\nStep 2: Prepare the Dataset for Training\nStep 3: Create Training Data and Assign Labels\nStep 4: Define and Train the CNN Model\nDepartment of Computer Engineering\n\nStep 5: Test the Model’s Accuracy\n6.\nConclusion:\nThis experiment aims to detect respiratory diseases using medical images using CNN\nand it is detected successfully.\n7.\nAdditional Learning:\n8.\nViva Questions:\n● What is CNN and how does it works?\n● What are different applications of CNN?\n9.\nReferences:\n1. Jake Vander Plas, Python Data Science Handbook: Essential Tools for Working\nwith Data, O’reilly Publication\n2. Wes McKinney , Python for Data Analysis: Data Wrangling with Pandas, NumPy,\nand IPython, O’reilly Publication\n3. Jason Test, PYTHON FOR DATASCIENCE\n4. Jason Test, Python Programming: 3 BOOKS IN1\nDepartment of Computer Engineering\n\nExperiment No.: 7\nTransfer learning\nImplement transfer learning to classify medical\nimages to detect respiratory diseases (VGGNet).\nDepartment of Computer Engineering\n\nExperiment -7\n1. Aim: Implement transfer learning to classify medical images to detect\nrespiratory diseases (VGGNet).\n2. Objectives:\n●\nTo familiarize with the concept of transfer learning.\n●\nTo study the applications of transfer learning to medical image\nclassification.\n3. Outcomes: The student will be,\n●\nable to use the concept of transfer learning.\n●\naware of networks used in the image classification domain.\n●\nable to be compatible with the industry requirements in the\ndomain of deep learning.\n4. Software Required: Python/MATLAB\n5. Theory:\nTransfer learning is a technique in machine learning where a model trained\non one task is used as the starting point for a model on a second task. This can\nbe useful when the second task is similar to the first task, or when there is\nlimited data available for the second task. By using the learned features from\nthe first task as a starting point, the model can learn more quickly and\neffectively on the second task. This can also help to prevent overfitting, as the\nmodel will have already learned general features that are likely to be useful\nin the second task.\nWhy transfer learning?\nMany deep neural networks trained on images have a curious phenomenon in\ncommon: in the early layers of the network, a deep learning model tries to\nlearn a low level of features, like detecting edges, colors, variations of\nintensities, etc. Such features appear not to be specific to a particular dataset\nor a task because no matter what type of image we are processing either for\ndetecting a lion or car. In both cases, we have to detect these low-level\nfeatures. All these features occur regardless of the exact cost function or\nimage dataset. Thus learning these features in one task of detecting lions can\nbe used in other tasks like detecting humans.\nThe following diagram shows comparison of traditional machine learning Vs\ntransfer learning:\nDepartment of Computer Engineering\n\nAdvantages of transfer learning:\n●\nSpeed up the training process.\n●\nBetter performance.\n●\nHandling small datasets.\nDisadvantages of transfer learning:\n●\nDomain mismatch.\n●\nOverfitting.\n●\nComplexity.\nImage classification with pre-trained models is a popular technique in deep\nlearning and computer vision that allows developers and researchers to leverage\npre-existing, large-scale neural networks trained on vast datasets to classify new\nimages.\nDepartment of Computer Engineering\n\n6. Algorithm:\nSteps to use Transfer Learning:\n1. Select a pre-trained model\n2. Choose layers to retrain\n3. Prepare the data\n4. Re-train the model\n5. Fine-tune the model\n6. Evaluate the model\nTransfer learning model VGGNet:\nIt is a typical deep Convolutional Neural Network (CNN) design with\nnumerous layers, and the abbreviation VGG stands for Visual Geometry\nGroup. The term “deep” describes the number of layers, with VGG-16 or\nVGG-19 having 16 or 19 convolutional layers, respectively.\nThe VGGNet, created as a deep neural network, outperforms benchmarks\non a variety of tasks and datasets outside of ImageNet. It also remains one\nof the most often used image recognition architectures today.\nVGG architecture\n● Input VGG neural network accepts a 224×224 pixel RGB image as\ninput. To keep the input image size consistent for the ImageNet\ncompetition, the authors clipped out the middle 224×224 patch in each\nimage.\n● Convolutional Layers have a 3×3 receptive field, which is the smallest\nsize achievable while still capturing left/right and up/down. There are\nadditional convolution filters that perform a linear change of the input\nbefore passing it through a ReLU unit. The stride is set at 1 pixel in order\nto preserve spatial resolution following convolution.\n● VGG’s hidden layers all use ReLU, an AlexNet invention that\nsignificantly reduces training time. Local Response Normalization (LRN)\nis not used by VGG since it increases memory usage and training time\nwithout improving accuracy.\n● VGG contains three completely connected layers, the first two of which\neach has 4096 channels and the third of which has 1000 channels, one for\neach class.\nDepartment of Computer Engineering\n\nFig.VGGNet Architecture\nAdvantages:\n●\nVGG uses very small receptive fields instead of massive fields like\nAlexNet. So it uses 3×3 with a stride of 1. The decision function is more\ndiscriminative now that there are three ReLU units instead of simply one.\nThere are also fewer parameters (27 times the number of channels vs. 49\ntimes the number of channels in AlexNet).\n●\nWithout modifying the receptive fields, VGG uses 1×1 convolutional\nlayers to make the decision function more non-linear.\n●\nThe VGG model can have a considerable number of weight layers due\nto the small size of the convolution filters; of course, more layers mean\nbetter performance. However, this isn’t an unusual trait.\n7. Conclusion: In conclusion, transfer learning is a powerful technique in deep\nlearning that allows users to leverage pre-trained models to accelerate the\ntraining process of a new model.\nIt can result in faster training, higher accuracy, and lower computational\nresources required.\nHowever, some potential drawbacks include limited flexibility, overfitting,\nunintended biases, and limited generalization. Overall, transfer learning can\nbe a valuable tool.\n8. Additional Learning:\nDepartment of Computer Engineering\n\n9. Viva Questions:\n What is transfer learning?\n What is VGGNet? What are the advantages of this model over CNN?\n10. References:\n1.\nJake Vander Plas, Python Data Science Handbook: Essential Tools for\nWorking with Data, O’reilly Publication\n2.\nWes McKinney , Python for Data Analysis: Data Wrangling with Pandas,\nNumPy, and IPython, O’reilly Publication\n3.\nJason Test, PYTHON FOR DATA SCIENCE\n4.\nJason Test, Python Programming: 3 BOOKS IN1\nDepartment of Computer Engineering\n\nComputational Intelligence\nExperiment No.: 8\nPSO-MLP\nDesign and implement PSO-MLP model for\nsolar radiation prediction\nDepartment of Computer Engineering\n\nExperiment 8\n1. Aim: Design and implement PSO-MLP model for solar radiation\nprediction.\n2. Objectives:\n To familiarize with the concept of PSO and MLP.\n To study the applications of PSO using MLP.\n3. Outcomes: The student will be,\n Able to use the concept of PSO.\n Aware of applications studied in previous modules.\n4. Software Required: Python/MATLAB\n5. Theory:\nA MLP (Multi -Layer Perceptron) is one of the most commonly used\nalgorithms of Artificial Neural Networks. It's training is done by adjusting the\nweights for each connection between the hidden layers of artificial neurons\nusing some optimization algorithm. This code is an example of how to use\nParticle Swarm Optimization (PSO) as a way to train an MLP.\nPSO is a global optimization algorithm that simulates the foraging behaviour\nof birds in groups. The position and velocity of each particle can be updated\nbased on the globally optimal solution and the current optimal solution; as a\nresult, all the particles move in the direction guided by the objective function.\nThen, the ﬁnal global optimal solution can be calculated. The PSO algorithm\nhas better global optimization capability and higher calculation performance\nthan other optimization algorithms (e.g., the genetic algorithm and the ant\ncolony algorithm).\nThe mini-batch gradient descent algorithm is mainly used to determine the\nappropriate connection weights between the neurons of MLP. However,\nseveral structural parameters, such as the learning rate, learning delay rate, and\nthe learning momentum, are needed for appropriate determination The learning\nrate indicates the change range of the weight values between the neurons for\neach training iteration, and the learning momentum is applied to ensure that the\nchange direction of the weight value is stable. In addition, the number of\nneurons in the hidden layer has an important eﬀect on the prediction\nperformance of MLP. Thus, it is necessary to select a proper number of neurons\nin the hidden layer.\nThe calculation processes of the PSO-MLP are shown in Figure 1. Firstly, the\ninitial parameters of PSO itself should be selected, including the number of\nparticles, maximum iterations, and so forth. Secondly, the MLP model with the\nmini-batch gradient descent algorithm should be trained and tested based on\nDepartment of Computer Engineering\n\nsamples. Then, the prediction accuracy index of the area under the receiver\noperating characteristic (ROC) curve (AUC) should be selected as the ﬁtness\nfunction of the MLP, and this ﬁtness function is also the objective function of\nPSO. Fourthly, by comparing the AUC value of each particle to the global and\nlocal best AUC values, the position and velocity of each particle should be\nupdated gradually. Finally, the update process continues until the end of the\nsetting maximum iteration of PSO is reached.\nFig.1: Flow chart of the PSO-MLP model\n6. Conclusion:\nThe PSO algorithm can effectively optimize the structure parameters of the\nMLP model with the mini-batch gradient descent algorithm compared with\nconventional MLP. The PSO-MLP model has the advantage of being more\nglobal and accurate prediction.\n7. Additional Learning:\n8. Viva Questions:\n\nWhat is the purpose of PSO-MLP algorithm?\n\nWrite weakness and advantages over genetic algorithm.\nDepartment of Computer Engineering\n\n9. References:\n1. Jake VanderPlas, Python Data Science Handbook: Essential Tools for\nWorking with Data, O’reilly Publication\n2. Wes McKinney , Python for Data Analysis: Data Wrangling with Pandas,\nNumPy, and IPython, O’reilly Publication\n3. Jason Test, PYTHON FOR DATA SCIENCE\n4. Jason Test, Python Programming: 3 BOOKS IN 1\nDepartment of Computer Engineering\n\nExperiment No.: 9\nVLAB- Inception ResNet\nDepartment of Computer Engineering\n\nExperiment No.: 10\nVLAB - Backpropagation OR SVM"
    },
    {
      "filename": "Module 2.pdf",
      "path": "data/materials\\Minor (CE) Data Science-Computational Intelligence-1\\Module 2.pdf",
      "text": "Computational Intelligence\n\n\nDecision Tree\nBasic of Decision Tree\n-\nDecision Tree is a Supervised learning technique that can be used for\nboth classification and Regression problems, but mostly it is preferred for\nsolving\nClassification\nproblems.\nIt\nis\na\ntree-structured\nclassifier,\nwhere internal nodes represent the features of a dataset, branches\nrepresent the decision rules and each leaf node represents the\noutcome.\n-\nIn a Decision tree, there are two nodes, which are the Decision\nNode and Leaf Node. Decision nodes are used to make any decision and\nhave multiple branches, whereas Leaf nodes are the output of those\ndecisions and do not contain any further branches.\n-\nThe decisions or the test are performed on the basis of features of the\ngiven dataset.\n-\n-\nIt is a graphical representation for\ngetting all the possible solutions to\na problem/decision based on given\nconditions.\n-\nIt is called a decision tree because,\nsimilar to a tree, it starts with the\nroot node, which expands on further\nbranches and constructs a tree-like\nstructure.\n-\nIn order to build a tree, we use\nthe CART algorithm, which stands\nfor Classification and Regression\nTree algorithm.\n-\nA decision\ntree\nsimply\nasks\na\nquestion, and based on the answer\n(Yes/No), it further split the tree\ninto subtrees.\nWhy use Decision Trees?\n- There are various algorithms in Machine learning, so\nchoosing the best algorithm for the given dataset and problem\nis the main point to remember while creating a machine\nlearning model. Below are the two reasons for using the\nDecision tree:\n- Decision Trees usually mimic human thinking ability while\nmaking a decision, so it is easy to understand.\n- The logic behind the decision tree can be easily understood\nbecause it shows a tree-like structure.\n\nDecision Tree Terminologies\n\n-Root Node: Root node is from where the decision tree starts.\nIt represents the entire dataset, which further gets divided into\ntwo or more homogeneous sets.\n-Leaf Node: Leaf nodes are the final output node, and the tree\ncannot be segregated further after getting a leaf node.\n-Splitting: Splitting is the process of dividing the decision\nnode/root node into sub-nodes according to the given\nconditions.\n-Branch/Sub Tree: A tree formed by splitting the tree.\n-Pruning: Pruning is the process of removing the unwanted\nbranches from the tree.\n-Parent/Child node: The root node of the tree is called the\nparent node, and other nodes are called the child nodes.\nHow does the Decision Tree algorithm Work?\n-\nIn a decision tree, for predicting the class of the given dataset, the\nalgorithm starts from the root node of the tree. This algorithm compares\nthe values of root attribute with the record (real dataset) attribute and,\nbased on the comparison, follows the branch and jumps to the next node.\n-\nFor the next node, the algorithm again compares the attribute value with\nthe other sub-nodes and move further. It continues the process until it\nreaches the leaf node of the tree.\n\nDecision Tree Algorithm\n-Step-1: Begin the tree with the root node, says S, which contains the complete dataset.\n-Step-2: Find the best attribute in the dataset using Attribute Selection Measure (ASM).\n-Step-3: Divide the S into subsets that contains possible values for the best attributes.\n-Step-4: Generate the decision tree node, which contains the best attribute.\n-Step-5: Recursively make new decision trees using the subsets of the dataset created in step -3. Continue\nthis process until a stage is reached where you cannot further classify the nodes and called the final node as a\nleaf node.\n\n-\nSuppose there is a candidate who has\na job offer and wants to decide\nwhether he should accept the offer or\nNot.\n-\nSo, to solve this problem, the decision\ntree starts with the root node (Salary\nattribute by ASM).\n-\nThe root node splits further into the\nnext decision node (distance from the\noffice) and one leaf node based on the\ncorresponding labels.\n-\nThe next decision node further gets\nsplit into one decision node (Cab\nfacility) and one leaf node.\n-\nFinally, the decision node splits into\ntwo leaf nodes (Accepted offers and\nDeclined offer)\nExample\n\nAttribute Selection Measures\n- While implementing a Decision tree, the main issue arises\nthat how to select the best attribute for the root node and for\nsub-nodes.\n- So, to solve such problems there is a technique which is\ncalled as Attribute selection measure or ASM.\n- By this measurement, we can easily select the best attribute\nfor the nodes of the tree. There are two popular techniques\nfor ASM, which are:\n- Information Gain\n- Gini Index\n\nInformation Gain:\n-\nInformation gain is the measurement of changes in entropy after the\nsegmentation of a dataset based on an attribute.\n-\nIt calculates how much information a feature provides us about a class.\n-\nAccording to the value of information gain, we split the node and build\nthe decision tree.\n-\nA decision tree algorithm always tries to maximize the value of\ninformation gain, and a node/attribute having the highest information gain\nis split first. It can be calculated using the below formula:\n-\nInformation Gain= Entropy(S)-[(Weighted Avg) *Entropy(each feature)\n\n-\nEntropy: Entropy is a metric to measure the impurity in a given attribute.\nIt specifies randomness in data. Entropy can be calculated as:\n-\nEntropy(s)= -P(yes)log2 P(yes)- P(no) log2 P(no)\n-\nWhere,\n-\nS= Total number of samples\n-\nP(yes)= probability of yes\n-\nP(no)= probability of no\n\n. Gini Index:\n-\nGini index is a measure of impurity or purity used while creating a\ndecision\ntree\nin\nthe\nCART(Classification\nand\nRegression\nTree)\nalgorithm.\n-\nAn attribute with the low Gini index should be preferred as compared to\nthe high Gini index.\n-\nIt only creates binary splits, and the CART algorithm uses the Gini index\nto create binary splits.\n-\nGini index can be calculated using the below formula:\nGini Index= 1- ∑jPj2\n\n\nDecision Tree calculation\nContents\nDecision tree examples\nClassification using the ID3 algorithm\nConstructing decision trees using Gini index\n- Constructing decision trees using Gini index\n-\nIt is a classification algorithm that follows a greedy approach by selecting a\nbest attribute that yields maximum Information Gain(IG) or minimum\nEntropy(H).\n-\nThe steps in ID3 algorithm are as follows:\nID3: Iterative Dichotomiser 3\n\nCalculate entropy for dataset.\nFor each attribute/feature.\n2.1 Calculate entropy for all its categorical values.\n2.2. Calculate information gain for the feature.\n1.Find the feature with maximum information gain.\n1.Repeat it until we get the desired tree.\nClassification using the ID3 algorithm\nConsider whether a dataset to determine whether to play Tennis or not:\n\nDay\nOutlook\nTemperature\nHumidity\nWind\nPlayTennis\nD1\nSunny\nHot\nHigh\nWeak\nNo\nD2\nSunny\nHot\nHigh\nStrong\nNo\nD3\nOvercast\nHot\nHigh\nWeak\nYes\nD4\nRain\nMild\nHigh\nWeak\nYes\nD5\nRain\nCool\nNormal\nWeak\nYes\nD6\nRain\nCool\nNormal\nStrong\nNo\nD7\nOvercast\nCool\nNormal\nStrong\nYes\nD8\nSunny\nMild\nHigh\nWeak\nNo\nD9\nSunny\nCool\nNormal\nWeak\nYes\nD10\nRain\nMild\nNormal\nWeak\nYes\nD11\nSunny\nMild\nNormal\nStrong\nYes\nD12\nOvercast\nMild\nHigh\nStrong\nYes\nD13\nOvercast\nHot\nNormal\nWeak\nYes\nD14\nRain\nMild\nHigh\nStrong\nNo\nClassification using the ID3 algorithm\nHere Four independent variables (Outlook, Temperature, Hu-\nmidity, and Wind) to determine the dependent variable (whether\nto play Tennis or not)\nFirst step, find the parent node for decision tree using:\nFind the entropy of the class variable\nE(S) = −[(9/14)log(9/14) + (5/14)log(5/14)] = 0.94\nFor outlook obtain the table:\nPlay\n\n\nYes\nNo\nTotal\nSunny\n\n\n\nOvercast\n\n\n\nRainy\n\n\n\n\nClassification using the ID3 algorithm\nHere Four independent variables (Outlook, Temperature, Hu-\nmidity, and Wind) to determine the dependent variable (whether\nto play Tennis or not)\nFirst step, find the parent node for decision tree using:\nFind the entropy of the class variable\nE(S) = −[(9/14)log(9/14) + (5/14)log(5/14)] = 0.94\nFor outlook obtain the table:\nPlay\n\n\nYes\nNo\nTotal\nSunny\n\n\n\nOvercast\n\n\n\nRainy\n\n\n\n\nClassification using the ID3 algorithm\nHere Four independent variables (Outlook, Temperature, Hu-\nmidity, and Wind) to determine the dependent variable (whether\nto play Tennis or not)\nFirst step, find the parent node for decision tree using:\nFind the entropy of the class variable\nE(S) = −[(9/14)log(9/14) + (5/14)log(5/14)] = 0.94\nFor outlook obtain the table:\nPlay\n\n\nYes\nNo\nTotal\nSunny\n\n\n\nOvercast\n\n\n\nRainy\n\n\n\n\nFirst Attribute - Outlook\n\nCategorical values - sunny, overcast and rain\nH(Outlook=sunny) = -(2/5)*log(2/5)-(3/5)*log(3/5) =0.971\nH(Outlook=rain) = -(3/5)*log(3/5)-(2/5)*log(2/5) =0.971\nH(Outlook=overcast) = -(4/4)*log(4/4)-0 = 0\nAverage Entropy Information for Outlook -\nI(Outlook) = p(sunny) * H(Outlook=sunny) + p(rain) *\nH(Outlook=rain) + p(overcast) * H(Outlook=overcast)\n= (5/14)*0.971 + (5/14)*0.971 + (4/14)*0\n= 0.693\nInformation Gain = H(S) - I(Outlook)\n= 0.94 - 0.693\n= 0.247\nSecond Attribute - Temperature\n-\nCategorical values - hot, mild, cool\n-\nH(Temperature=hot) = -(2/4)*log(2/4)-(2/4)*log(2/4) = 1\n-\nH(Temperature=cool) = -(3/4)*log(3/4)-(1/4)*log(1/4) = 0.811\n-\nH(Temperature=mild) = -(4/6)*log(4/6)-(2/6)*log(2/6) = 0.9179\n-\nAverage Entropy Information for Temperature -\n-\nI(Temperature) = p(hot)*H(Temperature=hot) +\np(mild)*H(Temperature=mild) + p(cool)*H(Temperature=cool)\n-\n= (4/14)*1 + (6/14)*0.9179 + (4/14)*0.811\n-\n= 0.9108\n-\nInformation Gain = H(S) - I(Temperature)\n-\n= 0.94 - 0.9108\n-\n= 0.0292\n\nThird Attribute - Humidity\n-\nCategorical values - high, normal\n-\nH(Humidity=high) = -(3/7)*log(3/7)-(4/7)*log(4/7) = 0.983\n-\nH(Humidity=normal) = -(6/7)*log(6/7)-(1/7)*log(1/7) = 0.591\n-\nAverage Entropy Information for Humidity -\n-\nI(Humidity) = p(high)*H(Humidity=high) +\np(normal)*H(Humidity=normal)\n-\n= (7/14)*0.983 + (7/14)*0.591\n-\n= 0.787\n-\nInformation Gain = H(S) - I(Humidity)\n-\n= 0.94 - 0.787\n-\n= 0.153\n\nFourth Attribute - Wind\n-\nCategorical values - weak, strong\n-\nH(Wind=weak) = -(6/8)*log(6/8)-(2/8)*log(2/8) = 0.811\n-\nH(Wind=strong) = -(3/6)*log(3/6)-(3/6)*log(3/6) = 1\n-\nAverage Entropy Information for Wind -\n-\nI(Wind) = p(weak)*H(Wind=weak) + p(strong)*H(Wind=strong)\n-\n= (8/14)*0.811 + (6/14)*1\n-\n= 0.892\n-\nInformation Gain = H(S) - I(Wind)\n-\n= 0.94 - 0.892\n-\n= 0.048\n\n-\nHere, the attribute with maximum information gain is Outlook.\nSo, the decision tree built so far –\n\nDecision tree after first step\nThe partially learned decision tree resulting from the first step of\nID3 (Root node selection)\n\nDecision tree after first step\nThe next step is to find the next node in our decision tree. Nowwe will\nfind one under sunny. We have to determine which of the following\nTemperature, Humidity or Wind has higher information gain\nFind the entropy of the class variable “Sunny”\nE(sunny) = −[(3/5)log(3/5) + (2/5)log(2/5)] = 0.971\n\nDay\nTemperature\nHumidity\nWind\nPlayTennis\nD1\nHot\nHigh\nWeak\nNo\nD2\nHot\nHigh\nStrong\nNo\nD8\nMild\nHigh\nWeak\nNo\nD9\nCool\nNormal\nWeak\nYes\nD11\nMild\nNormal\nStrong\nYes\n-\nHere, when Outlook == overcast, it is of pure class(Yes).\nNow, we have to repeat same procedure for the data with rows\nconsist of Outlook value as Sunny and then for Outlook value as\nRain.\n-\nComplete entropy of Sunny is -\n-\nH(S) = - p(yes) * log2(p(yes)) - p(no) * log2(p(no))\n-\n= - (2/5) * log2(2/5) - (3/5) * log2(3/5)\n-\n= 0.971\n\nFirst Attribute - Temperature\nCategorical values - hot, mild, cool\nH(Sunny, Temperature=hot) = -0-(2/2)*log(2/2) =\n\nH(Sunny, Temperature=cool) = -(1)*log(1)- 0 = 0\nH(Sunny, Temperature=mild) = -(1/2)*log(1/2)-\n(1/2)*log(1/2) = 1\nAverage Entropy Information for Temperature -\nI(Sunny, Temperature) = p(Sunny, hot)*H(Sunny,\nTemperature=hot) + p(Sunny, mild)*H(Sunny,\nTemperature=mild) + p(Sunny, cool)*H(Sunny,\nTemperature=cool)\n= (2/5)*0 + (1/5)*0 + (2/5)*1\n= 0.4\nInformation Gain = H(Sunny) - I(Sunny,\nTemperature)\n= 0.971 - 0.4\n= 0.571\nSecond Attribute - Humidity\nCategorical values - high, normal\nH(Sunny, Humidity=high) = - 0 - (3/3)*log(3/3) =\n\nH(Sunny, Humidity=normal) = -(2/2)*log(2/2)-0\n= 0\nAverage Entropy Information for Humidity -\nI(Sunny, Humidity) = p(Sunny, high)*H(Sunny,\nHumidity=high) + p(Sunny, normal)*H(Sunny,\nHumidity=normal)\n= (3/5)*0 + (2/5)*0\n= 0\nInformation Gain = H(Sunny) - I(Sunny,\nHumidity)\n= 0.971 - 0\n= 0.971\n\nCategorical values - weak, strong\nH(Sunny, Wind=weak) = -(1/3)*log(1/3)-(2/3)*log(2/3) = 0.918\nH(Sunny, Wind=strong) = -(1/2)*log(1/2)-(1/2)*log(1/2) = 1\nAverage Entropy Information for Wind -\nI(Sunny, Wind) = p(Sunny, weak)*H(Sunny, Wind=weak) + p(Sunny,\nstrong)*H(Sunny, Wind=strong)\n= (3/5)*0.918 + (2/5)*1\n= 0.9508\nInformation Gain = H(Sunny) - I(Sunny, Wind)\n= 0.971 - 0.9508\n= 0.0202\nThird Attribute - Wind\n\nHere, the attribute with maximum information gain is Humidity. So, the\ndecision tree built so far\n\nDecision tree after second step\nThe partially learned decision tree resulting from the second step of\nID3\n\nDecision tree after second step\nThe next step is to find the next node in our decision tree. Now we\nwill find one under rain. Wehave to determine which of the following\nTemperature, Humidity or Wind has higher information gain\nFind the entropy of the class variable “Rain”\nE(sunny) = −[(3/5)log(3/5) + (2/5)log(2/5)] = 0.971\n\nDay\nTemperature\nHumidity\nWind\nPlayTennis\nD5\nMild\nHigh\nWeak\nYes\nD6\nCool\nNormal\nWeak\nYes\nD6\nCool\nNormal\nStrong\nNo\nD10\nMild\nNormal\nWeak\nYes\nD14\nMild\nHigh\nStrong\nNo\nHere, when Outlook = Sunny and Humidity = High, it is a pure class of category\n\"no\". And When Outlook = Sunny and Humidity = Normal, it is again a pure\nclass of category \"yes\". Therefore, we don't need to do further calculations.\nNow, finding the best attribute for splitting the data with Outlook=Sunny values{\nDataset rows = [4, 5, 6, 10, 14]}.\nComplete entropy of Rain is -\nH(S) = - p(yes) * log2(p(yes)) - p(no) * log2(p(no))\n= - (3/5) * log(3/5) - (2/5) * log(2/5)\n= 0.971\n\nFirst Attribute - Temperature\nCategorical values - mild, cool\nH(Rain, Temperature=cool) = -(1/2)*log(1/2)-\n(1/2)*log(1/2) = 1\nH(Rain, Temperature=mild) = -(2/3)*log(2/3)-\n(1/3)*log(1/3) = 0.918\nAverage Entropy Information for\nTemperature -\nI(Rain, Temperature) = p(Rain, mild)*H(Rain,\nTemperature=mild) + p(Rain, cool)*H(Rain,\nTemperature=cool)\n= (2/5)*1 + (3/5)*0.918\n= 0.9508\nInformation Gain = H(Rain) - I(Rain,\nTemperature)\n= 0.971 - 0.9508\n= 0.0202\nSecond Attribute - Wind\nCategorical values - weak, strong\nH(Wind=weak) = -(3/3)*log(3/3)-0 = 0\nH(Wind=strong) = 0-(2/2)*log(2/2) = 0\nAverage Entropy Information for Wind -\nI(Wind) = p(Rain, weak)*H(Rain,\nWind=weak) + p(Rain, strong)*H(Rain,\nWind=strong)\n= (3/5)*0 + (2/5)*0\n= 0\nInformation Gain = H(Rain) - I(Rain,\nWind)\n= 0.971 - 0\n= 0.971\n\nDecision tree after third (last) step\nThe final learned decision tree resulting from the third step of ID3\n\n\nHere, the attribute with maximum information gain is Wind. So, the\ndecision tree built so far -\n\nClassification using Gini index\nFour independent variables (Outlook, Temperature, Humidity,\nWind) to determine the dependent variable (play Tennis or not)\nFirst step, find the parent node for decision tree using:\nCalculate the Gini index of the class variable\nc\nGini(S) = 1 −\np\n\\\nj=1\n\nj\n= 1 −\nI f\n9 \\ 2\nf\n5 \\ 21\n\n\n+\n= 1 −[0.4132 + 0.1275]\n= 0.4592\nGini(S) = 0.4592\n\n\nClassification using Gini index\nFour independent variables (Outlook, Temperature, Humidity,\nWind) to determine the dependent variable (play Tennis or not)\nFirst step, find the parent node for decision tree using:\nCalculate the Gini index of the class variable\nc\nGini(S) = 1 −\np\n\\\nj=1\n\nj\n= 1 −\nI f\n9 \\ 2\nf\n5 \\ 21\n\n\n+\n= 1 −[0.4132 + 0.1275]\n= 0.4592\nGini(S) = 0.4592\n\n\nClassification using Gini index\nFour independent variables (Outlook, Temperature, Humidity,\nWind) to determine the dependent variable (play Tennis or not)\nFirst step, find the parent node for decision tree using:\nCalculate the Gini index of the class variable\nc\nGini(S) = 1 −\np\n\\\nj=1\n\nj\n= 1 −\nI f\n9 \\ 2\nf\n5 \\ 21\n\n\n+\n= 1 −[0.4132 + 0.1275]\n= 0.4592\nGini(S) = 0.4592\n\n\nFor outlook obtain the table:\nPlay\nGini(S,O) =\nGini(2,3) +\n\n\n\n14Gini(4,0) + 5\n14Gini(3,2)\n=\n1 −( )2 −(\n+\n1 −( )2 −(\n5 1\n\n\n\n\n\n\n\n)2\n4 1\n\n\n\n\n4)2\n+\n1 −( )2 −(\n5 1\n\n\n\n)2\n\nYes\nNo\nTotal\nSunny\n\n\n\nOvercast\n\n\n\nRainy\n\n\n\n\n=\n[1 −0.16 −0.36]+\n[1 −1 −0]\n\n\n\n\n\n+\n[1 −0.36 −0.16]\n\n= 0.3571 × 0.488 + 0 + 0.3571 × 0.48\n= 0.171 + 0.171\n= 0.342\nGiniGain(S,O) = 0.459 −0.342\nGiniGain(S,O) = 0.117\n\nFor temperature obtain the table:\nPlay\nGini(S,T) =\nGini(2,2) +\nGini(4,2) +\n\n\n\n\n\n14Gini(3,1)\n\n\n=\n1 −( )2 −(\n+\n1 −( )2 −(\n4 1\n\n\n\n\n\n)2\n6 1\n\n\n\n\n6)2\n+\n1 −( )2 −(\n4 1\n\n\n\n)2\n\nYes\nNo\nTotal\nHot\n\n\n\nMild\n\n\n\nCool\n\n\n\n\n=\n[1 −0.25 −0.25]+\n[1 −0.4444 −0.1111]\n\n\n\n\n+\n[1 −0.5625 −0.0625]\n\n\n= 0.2857 × 0.5 + 0.4285 × 0.4445 + 0.2857 × 0.375\n= 0.1428 + 0.190 + 0.1071\n= 0.4399\nGiniGain(S,T) = 0.459 −0.4399\nGiniGain(S,T) = 0.019\n\nFor humidity obtain the table:\nPlay\nGini(S,H) =\nGini(3,4) +\n\n\n\n14Gini(6,1)\n=\n1 −( )2 −(\n+\n1 −( )2 −(\n7 1\n\n\n\n\n\n\n\n)2\n7 1\n\n\n7)2\n= 0.5 [1 −0.1836 −0.3265]+ 0.5 [1 −0.7346 −0.020]\n\nYes\nNo\nTotal\nHigh\n\n\n\nNormal\n\n\n\n\n= 0.5 × 0.4899 + 0.5 × 0.2454\n= 0.2449 + 0.1227\n= 0.3676\nGiniGain(S,H) = 0.459 −0.3676\nGiniGain(S,H) = 0.0914\n\nFor wind obtain the table:\nPlay\nYes\nNo\nTotal\nWeak\n\nStrong\n\n\n\n\n\n\nGini(S,W) =\nGini(6,2) +\nGini(3,3)\n\n\n\n\n=\n1 −( )2 −(\n+\n1 −( )2 −(\n8 1\n\n\n\n\n\n\n\n)2\n6 1\n\n\n6)2\n= 0.5714 [1 −0.5625 −0.0625]+ 0.4285 [1 −0.25 −0.25]\n\n= 0.5714 × 0.375 + 0.4285 × 0.5\n= 0.2142 + 0.2142\n= 0.4285\nGiniGain(S,W) = 0.459 −0.4285\nGiniGain(S,W) = 0.0305\n\nRoot node selection for decision tree\nGini gain for outlook = 0.117 Gini\ngain for temperature = 0.019 Gini\ngain for humidity = 0.0914 Gini\ngain for wind = 0.0305\nNow select the feature having the highest Gaini gain\nRoot node selection\nOutlook feature having the highest Gini gain hence it forms\nthe root node of our decision tree\n\n\nRoot node selection for decision tree\nGini gain for outlook = 0.117 Gini\ngain for temperature = 0.019 Gini\ngain for humidity = 0.0914 Gini\ngain for wind = 0.0305\nNow select the feature having the highest Gaini gain\nRoot node selection\nOutlook feature having the highest Gini gain hence it forms\nthe root node of our decision tree\n\n\nRoot node selection for decision tree\nGini gain for outlook = 0.117 Gini\ngain for temperature = 0.019 Gini\ngain for humidity = 0.0914 Gini\ngain for wind = 0.0305\nNow select the feature having the highest Gaini gain\nRoot node selection\nOutlook feature having the highest Gini gain hence it forms\nthe root node of our decision tree\n\n\nRoot node selection for decision tree\nGini gain for outlook = 0.117 Gini\ngain for temperature = 0.019 Gini\ngain for humidity = 0.0914 Gini\ngain for wind = 0.0305\nNow select the feature having the highest Gaini gain\nRoot node selection\nOutlook feature having the highest Gini gain hence it forms\nthe root node of our decision tree\n\n\nDecision tree after first step\nThe next step is to find the next node in our decision tree. Nowwe will\nfind one under sunny. We have to determine which of the following\nTemperature, Humidity or Wind has higher Gini gain\nCalculate Gini index of the class variable\n\nDay\nTemperature\nHumidity\nWind\nPlayTennis\nD1\nHot\nHigh\nWeak\nNo\nD2\nHot\nHigh\nStrong\nNo\nD8\nMild\nHigh\nWeak\nNo\nD9\nCool\nNormal\nWeak\nYes\nD11\nMild\nNormal\nStrong\nYes\nReferences\nText and reference books\n1. Peter\nHarrington,\n“Machine\nLearning\nin\nAction”,\nDreamTech Press, 1st Edition, 2012\n2. Nathaniel E. Helwig, “Multivariate Linear Regression”,\nUniversity of Minnesota\n3. Tom M. Mitchell, “Machine Learning”, McGraw Hill, 1st\nEdition, 1997\n4. Stephen Marsland, “Machine Learning - An Algorithmic\nPerspective”, CRC Press, 2nd Edition, 2015\n\nOnline resources\nOnline reading resources\nhttps://www.kdnuggets.com/2020/01/\ndecision- t r e e - algorithm-explained.html\nh t t p s: //o nli ne. st at. p su. ed u/ s tat507/lesson/10/10.3\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/\nhttps://www.studocu.com/en-us/document/james-madison-uni v e r s i t y /\nepidemiology/ch-4-lecture- notes/20446178\nOnline video resources\nDecision Tree Classifier for Beginners\nMachine Learning Fundamentals: Sensitivity and Specificity\nSensitivity, Specificity, Balanced Accuracy\n\n\n\nKNN\nK-Nearest Neighbor(KNN) Algorithm\n-\nK-Nearest Neighbour is one of the simplest Machine Learning\nalgorithms based on Supervised Learning technique.\n-\nK-NN\nalgorithm\nassumes\nthe\nsimilarity\nbetween\nthe\nnew\ncase/data and available cases and put the new case into the\ncategory that is most similar to the available categories.\n-\nK-NN algorithm stores all the available data and classifies a new\ndata point based on the similarity. This means when new data\nappears then it can be easily classified into a well suite category\nby using K- NN algorithm.\n-\nK-NN algorithm can be used for Regression as well as for\nClassification\nbut\nmostly\nit\nis\nused\nfor\nthe\nClassification\nproblems.\n\n-\nExample:\nSuppose,\nwe\nhave\nan\nimage of a creature that looks similar\nto cat and dog, but we want to know\neither it is a cat or dog.\n-\nSo for this identification, we can use\nthe KNN algorithm, as it works on a\nsimilarity measure.\n-\nOur KNN model will find the similar\nfeatures of the new data set to the cats\nand dogs images and based on the\nmost similar features it will put it in\neither cat or dog category.\nexample\n\nHow does K-NN work?\n-Step-1: Select the number K of the neighbors\n-Step-2: Calculate the Euclidean distance of K number of neighbors\n-Step-3: Take the K nearest neighbors as per the calculated Euclidean\ndistance.\n-Step-4: Among these k neighbors, count the number of the data points\nin each category.\n-Step-5: Assign the new data points to that category for which the number\nof the neighbor is maximum.\n-Step-6: Our model is ready.\n\n-\nSuppose we have a new data point and we need to put it in the required\ncategory. Consider the below image:\n\n-\nFirstly, we will choose the number of\nneighbors, so we will choose the k=5.\n-\nNext, we will calculate the Euclidean\ndistance between the data points. The\nEuclidean distance is the distance between\ntwo points, which we have already studied\nin geometry. It can be calculated as:\n\n-\nBy calculating the Euclidean distance we\ngot the nearest neighbors, as three nearest\nneighbors in category A and two nearest\nneighbors in category B. Consider the\nbelow image:\n-\nAs we can see the 3 nearest neighbors are\nfrom category A, hence this new data point\nmust belong to category A.\n\nBelow are some points to remember while selecting the\nvalue of K in the K-NN algorithm:\n-There is no particular way to determine the best value for\n\"K\", so we need to try some values to find the best out of\nthem. The most preferred value for K is 5.\n-A very low value for K such as K=1 or K=2, can be noisy\nand lead to the effects of outliers in the model.\n-Large values for K are good, but it may find some\ndifficulties.\nHow to select the value of K in the K-NN Algorithm?\n\nAdvantages of KNN Algorithm:\n-It is simple to implement.\n-It is robust to the noisy training data\n-It can be more effective if the training data is large.\nDisadvantages of KNN Algorithm:\n-Always needs to determine the value of K which may be\ncomplex some time.\n-The computation cost is high because of calculating the distance\nbetween the data points for all the training samples.\n\nAssume K=3,\nTest Example BMI=43.6, Age=40, Sugar=?\nKNN Solved Example to predict Sugar of Diabetic Patient given BMI and\nAge\nApply K nearest neighbor classifier to predict the diabetic patient with the given\nfeatures BMI, Age. If the training examples are,\n\nBMI\nAge\nSugar\n33.6\n\n\n26.6\n\nO\n23.4\n\nO\n43.1\n\nO\n35.3\n\n\n35.9\n\n\n36.7\n\n\n25.7\n\nO\n23.3\n\nO\n\n\n\nThe given training dataset has 10 instances with two features BMI (Body Mass\nIndex) and Age. Sugar is the target label. The target label has two possibilities\n0 and 1. 0 means the diabetic patient has no sugar and 1 means the diabetic\npatient has sugar\nGiven the dataset and new test instance, we need to find the distance from the\nnew test instance to every training example. Here we use the euclidean\ndistance formula to find the distance.\nEuclidean distance formula\n\n\nBMI\nAge\nSugar\nFormula\nDistance\n33.6\n\n\n√((43.6-33.6)^2+(40-50)^2 )\n14.14\n26.6\n\nO\n√((43.6-26.6)^2+(40-30)^2 )\n19.72\n23.4\n\nO\n√((43.6-23.4)^2+(40-40)^2 )\n20.20\n43.1\n\nO\n√((43.6-43.1)^2+(40-67)^2 )\n27.00\n35.3\n\n\n√((43.6-35.3)^2+(40-23)^2 )\n18.92\n35.9\n\n\n√((43.6-35.9)^2+(40-67)^2 )\n28.08\n36.7\n\n\n√((43.6-36.7)^2+(40-45)^2 )\n8.52\n25.7\n\nO\n√((43.6-25.7)^2+(40-46)^2 )\n18.88\n23.3\n\nO\n√((43.6-23.3)^2+(40-29)^2 )\n23.09\n\n\n\n√((43.6-31)^2+(40-56)^2 )\n20.37\nBMI\nAge\nSugar\nDistance\nRank\n33.6\n\n\n14.14\n\n26.6\n\nO\n19.72\n23.4\n\nO\n20.20\n43.1\n\nO\n27.00\n35.3\n\n\n18.92\n35.9\n\n\n28.08\n36.7\n\n\n8.52\n\n25.7\n\nO\n18.88\n\n23.3\n\nO\n23.09\n\n\n\n20.37\nOnce you calculate the distance, the next step is to find the nearest neighbors\nbased on the value of k. In this case, the value of k is 3. Hence we need to find\n3 nearest neighbors.\n\nNow, we need to apply the majority voting technique to decide the resulting\nlabel from the new example\n. Here the 1st and 2nd nearest neighbors have target label 1 and the 3rd\nnearest neighbor has target label 0.\nTarget label 1 has the majority. Hence the new example is classified as 1, That\nis the diabetic patient has Sugar.\nTest Example BMI=43.6, Age=40, Sugar=1\n\nThank You"
    },
    {
      "filename": "QB.pdf",
      "path": "data/materials\\Minor (CE) Data Science-Computational Intelligence-1\\QB.pdf",
      "text": "Note: Numerical mentioned are for just to give idea, it doesn’t mean same numerical only will\nbe considered.\nCO-1\n1. Explain soft and hard computing with suitable examples.\n2. Differentiate soft and hard computing with suitable examples.\n3. Elaborate Characteristics of Soft computing?\n4. Explain soft computing using example.\n5. Explain hybrid computing using example.\nCO-2\n1. What Is Predictive Modelling? Explain with example\n2. Describe common predictive modelling.\n3. Calculate linear regression coefficients for following data.\nX\n0.2\n0.4\n0.8\n\n1.2\n1.6\n1.8\n\ny\n-1.56\n-1.12\n-0.24\n0.2\n0.64\n1.52\n1.96\n2.4\n4. Fill the missing value with the mean for 𝑥1 and median for 𝑥2 then normalize using min-max\nnormalization (in the range [-2,2]).\n5. Normalize the given features using z-score and min-max normalization (in range [-2, 2]).\n6. Consider the following dataset and replace the missing values wherever required:\nSr\nNo.\nDay\nTemperature Windspeed Event\n\n1/1/2017\n32.0\n6.0\nRain\n\n1/4/2017\n--\n9.0\nSunny\n\n1/5/2017\n28.0\n5.0\nSnow\n\n1/6/2017\n29.0\n7.0\n--\n\n1/7/2017\n32.0\n--\nRain\n\n1/8/2017\n--\n6.0\nSunny\n\n1/9/2017\n37.0\n--\n--\n\n1/10/2017\n34.0\n8.0\nCloudy\n\n1/11/2017\n40.0\n12.0\nSunny\n𝑥1\n\n\n\n?\n-1.2\n-1.8\n1.5\n\n𝑥2\n-1\n-1.2\n2.4\n\n\n?\n-1.8\n\n𝑥1\n0.2\n0.4\n0.8\n\n1.2\n1.6\n1.8\n\n𝑥2\n-1\n-1.2\n2.4\n\n\n1.5\n-1.8\n\n7.A clinical trail gave the following data about the BMI and Cholesterol level of 10 patients.\nPredict the likely value of Cholesterol level using linear regression for a patient who has BMI of\n27.\nBMI\n\n\n\n\n\nCholestrol 140\n\n\n\n\n8. Using a logistic regression find out probability for given object that is ready to whether\na tumour is cancerous, when size is 3.46mm. Logistic function is\nx\ne\ny\n−\n+\n−\n= 4\n(Sigmoid function)\nTumour\nsize\n3.78\n2.44\n2.09\n0.14\n1.72\n1.65\n4.92\n4.37\n4.96\nstatus\n\n\n\n\n\n\n\n\n\n9. Classify the unknown data using KNN algorithm (3 neighbours)\n𝑥1\n-2 -2 -2 -1\n\n\n\n\n3 -0.5 0\n\n\n𝑥2\n\n\n\n\n\n\n\n\n4 0.8 2\n\n\nClass\n\n\n\n\n\n\n\n\n\n\n\n?\n?\n10. What is decision tree explain advantages of it?\n11. Describe in detail Decision tree algorithm\n12. Consider whether a dataset to determine whether to play Tennis or not Using ID3\nalgorithm find out parent node for decision tree from given dataset.\n13. Evaluate Gini index for temperature parameter to build decision tree using following data\nDay\n\n\n\n\n\n\n\n\n\n\nOutlook\nS\nS\nO\nR\nR\nR\nO\nS\nS\nR\nTemp.\nH\nH\nH\nM\nC\nC\nC\nM\nC\nM\nHumidity G\nG\nG\nG\nN\nN\nN\nG\nN\nN\nWind\nW\nS\nW\nS\nW\nS\nW\nW\nW\nW\nPlay\nNo No Yes No Yes No Yes No No Yes\nS: Sunny, O: Overcast, R: Rain, H: Hot, M: Mild, C: Cold, G: High, N: Normal, W: Weak,\nS: Strong;\n14. Evaluate working principle and algorithm of KNN.\n15. Classify the unknown data using KNN algorithm (3 neighbours\nBRIGHTNESS\nSATURATION\nCLASS\n\n\nRed\n\n\nBlue\n\n\nBlue\n\n\n?\n\n\n?\n16. Consider following dataset, In order to create a decision tree that accurately predicts the\nsuspension system's performance, which attribute is the most important amongst Road\nCondition and Suspension Type?\nRoad\nCondition\nW\nD\nW\nD\nW\nD\nW\nD\nW\nD\nSuspension\nType\nI\nDe\nI\nDe\nI\nDe\nI\nDe\nI\nDe\nSpeed (mph)\n\n\n\n\n\n\n\n\n\n\nTemperature\n(°F)\n\n\n\n\n\n\n\n\n\n\nPerformance\nG\nG\nP\nG\nP\nG\nP\nP\nG\nP\nW-Wet, D-Dry, I- Independent, De-Dependent, G- Good, P-Poor"
    }
  ]
}