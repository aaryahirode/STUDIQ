{
  "subject": "Soft Computing",
  "files": [
    {
      "filename": "Auto associative network numerical.pdf",
      "path": "data/materials\\Soft Computing\\Auto associative network numerical.pdf",
      "text": "8. Train the autoassociative network tor input vecte\n|-1 1I 1] and also test the nerwork for the\nsame input vector. 1 Test the autoassociative\nwork with one missing, one mistake, two missing\nand two\nmistake entries in test vector,\nSolution: The input vector is x =|-1TI\nThe weight vector is\nw-JHpp) =\n\n-1\n-1-1\nL-1\n\n-1\n\n\n1 1\n\n\n-\n\n\n|4x1\n4x4\nTesting the network with same input vector. The test\ninput is (-1 1 1 1]. The weight obtained above\n4.10 Solved Problems\nis used as the initial weight here.\nComputing the\ninput, we get\n=x W= [-1 1 1 1]\n= [-4 4 4 4]\nJ,=fo) =\n=[-1 1 1 1]\n-1\nVimj =X W= [0 1 1 1]\n=[-3 3 3 3]\n-1\nHence the correct response is obtained.\nVinj =x: W\n=-1 1 0 1]\n\nApplying activations over the net input to calculate\nthe output, we have\nTesting the network with one missing entry\n=[-3 3 3 3]\n1 if\nyinj >0\nif\n-1-1 -1\n-1\n\n\n\n. Test input x = [0 1 1 1]. Computing the\ninput, we get\n\n\n-1\n\n-1\n\nVinj =x- W\n-1\n\n\n1 -1-1\n-1\n\n\n\nTest input x = -1 1l 0 1]. Computing\ninput, we obtain\nlesting the network with one mistake entry\n\n\n\n\n\nApplying the activations, we get y; = [-1 1\n\n1] which is the correct response.\n\n\n-1 -1\n\n\n\n\n\n\n\nApplying the activations, we get y; = -1 1\n1 1] which is the correcCt response.\nnet\nlest input x = [-1 -1 1 1]. Computing net\ninput, we get\n= |-1 -1 1 1]\n= [-2 2 2 2]\n=2 2 2 2]\nYinj =x W= [1 1 1 1]\n-1\n\" Test input x = [0 0 1\ninput, we get\n-1\nYny =X W = [0 0 1 1]\n=-2 2 2 2]\n-1\nVinj =x. W\n1 -1 -1 -1|\nApplying the activations, we get y; = [-1 1\n1 1] which is the correct response.\n=[-1 00 1]\n\" Test input x = [1\n1 1 1]. Computing net\ninput, we get\n=-2 2 2 2]\n-1\nTesting the network with two missing entry\n-1\n-1\n\n-1\n\n-1\nApplying the activations, we get y; = [-1 1\n1 1] which is the correct response.\n\n-1\n\n-1\n\n-1\n\n\n\n\n\n11. Computing\n\n-1 -1 -1\n\n\n1 1\n1 1\n\n\nApplying the activations, we get y; =\n1 1\n1 1] which is the correct response.\n\" Test input x = [-1 0 0 1]. Computing net\nInput, we obtain\n1 -1-1\n-1 -1 -1\n\n\n\n\n\n\n\n\n\n\nnet\n\n\n\nApplying the activations, we get y; =\n-1 1\n1 1] which is the correct response.\n\nTesting the netvork with two mistaken entry\nTest input x-1-1-1 1]. Computing net\ninput, we obtain\n-|0 0 0 0]\nApplying the activations over the net input, we get\ny; = |0 00 0] which is the incorrect response.\nThus, the network with two mistakes is not recog\nnized.\n9. Check the autoassociative network for input\nvector [1 1 -1]. Form the weight vector with\nno self-connection. Test whether the net is able to\nrecognize with one missing entry.\n|-1\n\nSolution: Input vector x = [1\n1 -1]. The weight\nvector is\nW=(psp) =\n\n1 -1\nThe weight vector with no self-connection (make\nthe diagonal elements in the weight vector zero) is\ngiven by\n\n\nTesting the network with one missing entry\n= |1 2. -1|\n\" Test input x = [1 0-1]\n\n\n-1\nApplying the activations, we gety; = |1 1 -),\nhence a correct response is obtained.\n\nS\n\" Test input x = [1 1 0]\nJoy =x W= [1\n1 0]\n-[1 1 -2]\n\n\n1 -1\n\n0-1\nWOris\n-1-1 0\nApplying the activations, we ger y; = [1 1 -\nhence a correct response is obtained.\nectos"
    },
    {
      "filename": "BE_DYPU_ Syllabus_CE-Soft Computing-CECDLO7052.pdf",
      "path": "data/materials\\Soft Computing\\BE_DYPU_ Syllabus_CE-Soft Computing-CECDLO7052.pdf",
      "text": "Theory\nHrs\nPractical\nHrs\nTutorial\nHrs\nTheory\nCredit\nPractical /\nOral\nCredit\nTutorial\nCredits\nTotal\nCredits\nCECDLO7052\nSoft\nComputing\n\n-\n-\n\n-\n-\n\nSubject Code\nSubject\nName\nExamination Scheme\nTheory Marks\nTerm\nWork\nPractical Oral Total\nIn-Sem Evaluations\nEnd\nSem\nExam\nIA1\nIA2\nAvg\nMS\nE\nCECDLO7052 Soft\nComputing\n\n\n\n\n\n-\n-\n--\n\nCourse Objectives:\n1. To learn the basic concepts of soft computing\n2. To understand the concepts of data analysis solutions\n3. To understand the concepts and techniques for designing intelligent systems\nCourse Outcomes: Students will be able to\n1. Understand the concepts of fundamentals of soft computing.\n2. Understand the concepts of neural network.\n3. Understand the concepts of Fuzzy Systems.\n4. Apply for the solution of multi-level optimization.\n5. Create hybrid systems.\n6. Evaluate and understand Backpropagation Networks.\nPrerequisites:\n1. Analysis of Algorithms\nSr.\nNo.\nModule\nDetailed Content\nHours\nCO\nMapping\n\nFundamentals\nof soft\nComputing\nIntroduction\nof\nSoft\nComputing,\nSoft\nComputing vs. Hard Computing, requirement\nof soft computing, Major areas of Soft\nComputing,\nVarious\nTypes\nof\nSoft\nComputing Techniques, Applications of Soft\nComputing.\n\nCO1\n\nArtificial\nNeural\nNetworks\nWhat is Neural Network, Learning rules and\nvarious activation functions, Single layer\nPerceptrons , Back Propagation networks,\n\nCO2\nArchitecture of Backpropagation(BP)\nNetworks,\nBackpropagation\nLearning,\nVariation of Standard Back propagation\nNeural Network, Introduction to Associative\nMemory, Adaptive Resonance theory and\nSelf Organizing Map, Recent Applications.\n\nFuzzy Logic\nIntroduction, Fuzzy sets and Fuzzy reasoning,\nBasic functions on fuzzy sets, relations, rule\nbased models and linguistic variables, fuzzy\ncontrols,\nFuzzy\nClassifications,\nFuzzy\ndecision making, applications of fuzzy logic\n\nCO3\n\nGenetic\nAlgorithms\nIntroduction, Genetic Algorithm, Fitness\nComputations,\nGenetic\nAlgorithm\nOperations,\nEvolutionary\nProgramming,\nClassifier Systems, Genetic Programming\nParse Trees, Variants of GA, Applications.\n\nCO4\n\nHybrid\nSystems:\nSequential Hybrid Systems, Auxiliary Hybrid\nSystems, Embedded Hybrid Systems, Neuro-\nFuzzy Hybrid Systems,\nNeuro-Genetic Hybrid Systems, Fuzzy-\nGenetic Hybrid Systems.\n\nCO5\n\nBackpropagati\non Networks\nGA based Weight Determination, K - factor\ndetermination in Columns.\nFuzzy Backpropagation Networks:\nLR type Fuzzy numbers, Fuzzy Neuron,\nFuzzy BP Architecture, Learning in Fuzzy\nBP, Application of Fuzzy BP\nNetworks.\n\nCO6\nText Books:\n1. Samir Roy and Udit Chakraborty, â€œIntroduction to Soft Computing, Pearson 1st Edition.\n2. Simon S. Haykin, Neural Networks, Prentice Hall, 2nd edition.\n3. Zimmermann, â€œFuzzy Set Theory and its Applicationâ€, 3rd Edition.\nReference Books:\n1. Xin-She Yang, â€œRecent Advances in Swarm Intelligence and Evolutionary Computation, Springer\nInternational Publishing, Switzerland, 2015.\n2. Kalyanmoy Deb, Multi-Objective Optimization using Evolutionary Algorithms, John Wiley & Sons,\n2001.\n3. James Kennedy and Russel E Eberheart, Swarm Intelligence, The Morgan Kaufmann Series in\nEvolutionary Computation, 2001.\n4. Neural Networks, Fuzzy Logic and Genetic Algorithms: Synthesis & Applications,\n5. S.Rajasekaran, G. A. Vijayalakshami, PHI.\n6. Genetic Algorithms: Search and Optimization, E. Goldberg.\n7. Neuro-Fuzzy Systems, Chin Teng Lin, C. S. George Lee, PHI.\n8. Build_Neural_Network_With_MS_Excel_sample by Joe choong.\nIn-Semester Assessment: Assessment consists of two tests out of which; one should be compulsory class test\n(on minimum 02 Modules) and the other is either a class test or assignment on live problems or course project.\nThere will be a mid semester Examination on 40-50% of the syllabus.\nEnd-Semester Examination:\n1. Question paper will comprise of total six question.\n2. All question carry equal marks\n3. Questions will be mixed in nature (for example supposed Q.2 has part (a) from module 3 then part (b)\nwill be from any module other than module 3)\n4. Only Four question need to be solved.\nIn question paper weightage of each module will be proportional to number of respective lecture hours\nas mentioned in the syllabus."
    },
    {
      "filename": "CE_BE_SC_Week 3_2024-25.pdf",
      "path": "data/materials\\Soft Computing\\CE_BE_SC_Week 3_2024-25.pdf",
      "text": "Soft Computing\nUnit 2: Artificial Neural Network\nFaculty Name : Ms. Archana Khodke\n\nBackpropagation Network\nBackpropagation Network\nLec-2: Introduction to AI\n\nBackpropagation net is an important class of artificial neural nets\nwhich is used in a wide range of applications.\nTh early enthusiasm over neural networks received a severe\nblow when Minsky and Papert (1969) demonstrated that\nperceptrons are unable to implement an elementary function\nlike a 2-input XOR.\nResearch community lost interest in the subject and no further\ndevelopment took place for several years. Discovery of\nmultilayered perceptron (also referred to as multilayered\nnetworks) independently by several researchers eventually\nrestored interest in this field.\nBackpropagation Network\n\nThe limitation of a single layer perceptron is overcome by\nmultilayer neural nets. It is proved that a multilayer\nfeedforward net can be made to learn any continuous function\nto any extent of desired accuracy.\nThe learning method called the generalized delta rule, or back-\npropagation (of errors), is employed to train the multilayer\nfeedforward networks. It is essentially gradient descent method\nto minimize the total squared error of the output computed by\nthe net. The learning is supervised.\nMULTI-LAYER FEEDFORWARD NETWORK\n\nThe single layer net has very limited capability. It is unable\nto learn such a simple function as the 2-input XOR.\nMultilayer perceptron has the capacity to overcome this\nlimitation. A multilayer network with one or more hidden\nlayers can learn any continuous mapping to an arbitrary\naccuracy.\nMoreover, it is proved that one hidden layer is suffi cient\nfor a multilayer perceptron to implement any\ncontinuous function. However, in some cases, more than\none hidden layer may be advantageous.\nMULTI-LAYER FEEDFORWARD NETWORK\n\nArchitecture\n\nTh e processing elements of a multi-layer feedforward\nneural net are arranged in a number of layer. The layers\nintermediate between the input and the output layers are\ncalled the hidden layers. The connecting paths are\nunidirectional. They are directed from the input layer to the\noutput layer. Signals flow from the input to the output\nthrough the hidden layers and not in the reverse direction.\nThe name feedforward is due to the unidirectional flow of\nsignal from the input to the output layer. During learning,\nthe net adjusts its interconnection weights on the basis of\nthe errors in computation.\nArchitecture\n\nCalculation of errors starts at the output layer and the\nerrors are propagated backward, i.e., from the output\nlayer towards the input layer. Because of this backward\npropagation of errors during the learning phase these nets\nare called backpropagation nets. The structure of an m-p-\nn multilayer net with one hidden layer . It can be easily\ngeneralized to nets having move than hidden layers. The\nbiases to the hidden units and the output units are\nprovided by the units X0 and H 0 respectively, each of\nwhich is permanently fed with the signal 1. The biases to\nthe hidden units H1, â€¦, Hp as v01, v02, â€¦, v0p.\nSimilarly, those to the output units Y1, â€¦, Yn are given by\nw01, â€¦, w0n.\nMULTI-LAYER FEEDFORWARD NETWORK\n\nx is input training pattern of length m.\nx = [ x1, â€¦ , xi, â€¦, xm ]\ny_out Output pattern produced by the activations of the\noutput units Y1, â€¦., Yn.\ny_out = [y_out1, â€¦, y_outk, â€¦, y_outn ]\nt Target output pattern for input pattern x.\nt = [ t1, â€¦, tk, â€¦, tn]\nh_out Pattern produced by the activations of the hidden units\nH1, â€¦, Hp. h_out = [ h_out1, â€¦, h_outj, â€¦, h_outp ]\nXi The ith input unit. The signal to input unit Xi is symbolized as\nxi. The activation of Xi is also xi, as it simply broadcasts the input\nsignal xi, without any processing, to the units of the hidden\nlayer.\nMULTI-LAYER FEEDFORWARD NETWORK\n\nHj The jth hidden unit, j = 1, .., p.\nThe total input signal to Hj is given by\nWhere vij is the interconnection weight between the\ninput unit Xi and the hidden unit Hj,\nvoj is the bias on the hidden unit Hj.\nh_outj The activation of the jth hidden unit Hj.\nh_outj = fh ( h_inj ), where fh is the activation function for\nthe hidden units.\nMULTI-LAYER FEEDFORWARD NETWORK\n\nYk The kth output unit, k = 1, â€¦, n. Th e total input signal to\nYk is given by\nWhere wjk is the interconnection weight between the\nhidden unit Hj and the output unit Yk\nwork is the bias on the output unit Yk.\ny_outk\nThe activation of the output unit Yk.\ny_outk = fo ( y_ink ), where fo is the activation function for\nthe output units.\nMULTI-LAYER FEEDFORWARD NETWORK\n\nÎ´(wk) A component of error correction weight adjustment\nfor wjk, j = 0, â€¦, p, that is due to an error at output Yk.\nMoreover, Î´k is propagated to the hidden units to further\ncalculate the error terms at the hidden units.\nÎ´(vj) A component of error correction weight adjustment for\nvij, i = 0, â€¦, m. Î´(vj) results from the backpropagation of error\ninformation from the output units to the hidden unit Hj.\na Learning rate\nBackpropagation Algorithm\n\nBackpropagation is a supervised learning method where the\nnet repeatedly adjusts its interconnection weights on the basis\nof the error, or deviation from the target output, in response\nto the training patterns.\nAs usual, learning takes place through a number of epochs.\nDuring each epoch the training patterns are applied at the\ninput layer and the signals flow from the input to the output\nthrough the hidden layers.\nCalculation of errors starts at the output layer and the errors\nare propagated backward, i.e., from the output layer towards\nthe input layer\nBackpropagation Algorithm\n\nBackpropagation Algorithm\n\nBackpropagation Algorithm\n\nBackpropagation Algorithm\n\nBackpropagation Algorithm\n\nBackpropagation Algorithm\n\nThank You"
    },
    {
      "filename": "CE_BE_SC_Week 5_2024-25.pdf",
      "path": "data/materials\\Soft Computing\\CE_BE_SC_Week 5_2024-25.pdf",
      "text": "Soft Computing\nUnit 3: Fuzzy Logic\nFaculty Name : Ms. Archana Khodke\n\nIntroduction of Fuzzy Logic\nWHAT IS FUZZY LOGIC?\nLec-2: Introduction to AI\n\nâ¢Definition of fuzzy\nï‚—\nFuzzy â€“ â€œnot clear, distinctâ€\nï‚—Definition of fuzzy logic\nï‚—Fuzzy logic is an approach to computing\nbased on \"degrees of truth\" rather than the\nusual \"true or false\" (1 or 0) Boolean logic on\nwhich the modern computer is based.\nTRADITIONAL REPRESENTATION OF LOGIC\n\nbool speed;\nget the speed\nif ( speed == 0) {\n// speed is slow\n}\nelse {\n// speed is fast\n}\nFast\nSlow\nSpeed = 0\nSpeed = 1\nREPRESENTATION OF FUZZY LOGIC\n\nFor every problem must\nrepresent in terms of fuzzy sets.\nWhat are fuzzy sets?\nSlowest\nFastest\nSlow\nFast\n[ 0.0 â€“ 0.25 ]\n[ 0.25 â€“ 0.50 ]\n[ 0.50 â€“ 0.75 ]\n[ 0.75 â€“ 1.00 ]\nREPRESENTATION OF FUZZY LOGIC\n\nfloat speed;\nget the speed\nif ((speed >= 0.0)&&(speed < 0.25)) {\n// speed is slowest\n}\nelse if ((speed >= 0.25)&&(speed < 0.5))\n{\n// speed is slow\n}\nelse if ((speed >= 0.5)&&(speed < 0.75))\n{\n// speed is fast\n}\nelse // speed >= 0.75 && speed < 1.0\n{\n// speed is fastest\n}\nORIGINS OF FUZZY LOGIC\n\nâ¢Traces back to Ancient Greece\nâ¢\nLotfi Asker Zadeh ( 1965 )\n-\nFirst to publish ideas of fuzzy logic.\nâ¢\nProfessor Toshire Terano ( 1972 )\n-\nOrganized the world's first working group on fuzzy systems.\nâ¢F.L. Smidth & Co. ( 1980 )\n-\nFirst to market fuzzy expert systems.\nFUZZY LOGIC IN CONTROL SYSTEMS\n\nâ¢Fuzzy Logic provides a more efficient and\nresourceful way to solve Control Systems.\nâ¢\nSome Examples\nâ–ªTemperature Controller\nâ–ªAnti â€“ Lock Break System ( ABS )\nTEMPERATURE CONTROLLER\n\nï‚¢The problem\nï‚—Change the speed of a heater fan, based on the room\ntemperature and humidity.\nï‚¢\nA temperature control system has four settings\nï‚—Cold, Cool, Warm, and Hot\nï‚¢\nHumidity can be defined by:\nï‚—Low, Medium, and High\nï‚¢\nUsing this we can define the fuzzy set.\nTEMPERATURE CONTROLLER\n\n\nFuzziness/Vagueness/Inexactness\nThe expressions as rich , very, expensive, old, weak etc. are\nextremely convenient in practical communication.\nThey are characteristically inexact in the sense that there are no\nwell-defined demarcations between rich and poor, very and little,\nexpensive and cheap, old and young, or weak and strong.\nA person can be rich as well as poor simultaneously, of course to\ndifferent extents. This vagueness is desirable ,otherwise everyday\nconversation would have been impossible.\nHowever, the classical set theory is not equipped to handle such\nvagueness as it does not allow an element to be a partial\nmember, or a partially non-member, of a set simultaneously.\nTherefore classical set theory is inadequate to model our intuitive\nnotion of a set in general.\nFuzzy set theory\n\nFuzzy set theory takes into consideration the natural vagueness\nthat we the human beings deal with in our practical, daily life.\nAs an example, let us consider the data related to a family as\ndescribed below in Table\nSet Membership\n\nIt is customary to refer to a classical set as crisp in order\nto differentiate it from a fuzzy set.\nThe crisp set of the family members U = {Grand-pa,\nGrand-ma, Dad, Mom, Sister, Brother, Aunt} may be\ntreated as the reference set, or the universe of discourse.\nMembership Function\nGiven an element x and a set S, the membership of x with\nrespect to S, denoted as m S (x), is defined as\nSet Membership\n\nLet us consider the set M of male family members and set F of\nfemale family members with reference to the family presented\nin Table 2.1.\nWe see that m M (Dad) = 1, and m M (Mom) = 0. Similarly, m F\n(Dad) = 0, and m F (Mom) = 1.\nMembership values of the other family members in M and F can\nbe ascertained in similar manner.\nNow, consider A to be the set of senior persons in the family.\nSeniority is a familiar and frequently used attribute to a\nperson. But is there any clear and unambiguous way to\ndecide whether a person should be categorized as senior or\nnot?\nSet Membership\n\nWe may agree without any hesitation that Grand-pa, being\n72 years old, is a senior person, so that Grand-pa ïƒA.\nOn the other hand the brother and the sister are both too\nyoung to be categorized as senior persons.\nTherefore, we may readily accept that Daughter ïƒA and Son\nïƒA. What about Mom, or Dad?\nThey are not as aged as Grand-pa but neither as young as the\ndaughter or the son.\nMoreover, Grand-ma is almost a senior person, being at 63\nyears, but she might be categorized as a middle-aged person\nas well.\nMembership Function\n\nQuite often it is convenient to express the membership of\nvarious elements with respect to a fuzzy set with the help of a\nfunction, referred to as the membership function.\nFor any x ïƒU, we may determine the membership value of x in\nA with the help of the following\nmembership function.\nMembership Function\n\nFor Grand-ma we have mA (Grand-ma) = (63 â€“ 30) / 40 =\n33 / 40 = 0.825. Hence Grand-ma is a senior person to a\nlarge extent, but not as fully as Grand-pa.\nMembership Function\n\nTherefore, the fuzzy set A of senior\npersons can be expressed as a set of ordered pairs A\n= {(Grand-pa, 1.0), (Grand-ma, 0.825), (Dad, 0.275),\n(Mom, 0.2), (Aunty, 0.55)}.\nFuzzy Sets\n\nA fuzzy set F on a given universe of discourse U is defined as a\ncollection of ordered pairs (x, mF (x)) where x ïƒU, and for all\nx ïƒU, 0.0 ï‚£mF (x) ï‚£1.0\nF = {(x, mF (x)) ï¼x ïƒU, 0.0 ï‚£mF (x) ï‚£1.0}\nClassical sets, often referred to as crisp sets to distinguish them\nfrom fuzzy sets, are special cases of fuzzy sets where the\nmembership values are restricted to either 0 or 1.\nEach pair (x, mF (x)) of the fuzzy set F is known as a singleton.\nNotation (Fuzzy sets) Apart from enumerating the singletons as\ndescribed above, fuzzy sets are frequently expressed as the\nunion of all singletons where a singleton is denoted as mF (x) / x.\nUsing this notation\nFuzzy Sets\n\nHere the summation sign â…€is to be interpreted\nas union over all singletons, and not arithmetic\nsum.\nFor continuous sets, the summation notation is\nreplaced by the integral sign , as shown below.\nThank You"
    },
    {
      "filename": "CE_BE_SC_Week 6_2024-25.pdf",
      "path": "data/materials\\Soft Computing\\CE_BE_SC_Week 6_2024-25.pdf",
      "text": "Soft Computing\nUnit 3: Fuzzy Logic\nFaculty Name : Ms. Archana Khodke\n\nCRISP SETS: A REVIEW\nSet and Set Notations\nLec-2: Introduction to AI\n\nA set is a collection of distinct elements, or members, without repetition\nand without ordering.\nTh e set atomic particles = {electron, proton, neutron}. Since a set is defi\nned by its members without\nregard to ordering or repetitions, the set of atomic particles cited above\nis identical to the set {proton, neutron, electron}, or the set {neutron,\nproton, neutron, electron, proton, proton}\nThere are two ways to describe a set. The most obvious one is by\nenumerating the members of the set.\nHowever, occasionally it is convenient to describe a set by citing a\nproperty common to all the members of the set. Both of these kinds of\nset notations are exemplified below.\nExample\nSet and Set Notations\nLec-2: Introduction to AI\n\nTh e sets A, B, C, D given below are described by enumeration of its\nmembers.\nA = {Jack, Jill, hill, pail, water}\nB = {+, âˆ’, Ã—, Ã·}\nC = {Socrates, toothbrush, loneliness, 235}\nD = {Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday}\nTh e set D of all days of a week can be equivalently expressed by citing\nthe property shared by its member. Thus,\nD = {x | x is a day of the week}\nA few other sets described in similar way are\nE = {x | x is an integer, and -1 ï‚£x ï‚£+1}}= {âˆ’1, 0, +1}\nF = {x | x is a prime number}\nG = {x | x is a polygon}\nH = {x | x is an element having 8 electrons in its outermost shell}\nThe Null Set and the Universal Set\n\nThere are two specially interesting sets, viz. the null (empty) set\nand the universal set. These are usually represented by the symbols ï¦and U,\nrespectively. Th e null set is the set without any member, so that | ï¦| = 0.\nThe universal set, on the other hand, is the set of all possible elements\nin a certain context.\nLet X be the set of all natural numbers that are divisible by 4 but not divisible\nby 2. As there is no such integer which is divisible by 4 but not divisible by 2,\nX = ï¦.\nSimilarly, the set of positively charged electrons is also the null set.\nMoreover, we may define a set S = {x | x is blue-eyed} in the context of the\nuniversal set of all human beings. Then S is the set of all blue-eyed persons\nand U is the set of all human beings. However, if U is the set of all living\ncreatures, then S is the set of all blue-eyed creatures including, but not\nlimited to, all blue-eyed human beings.\nSubset\n\nAn element x is said to belong to a set S, expressed as x ïƒS, if x is\nincluded in S. Otherwise x does not belong to S, written as x ïƒS. For\nexample, for the set F = {x | x is a prime number}, 3 ïƒF, as 3 is a prime\nnumber, but 4 ïƒF.\nA set T is said to be a subset of set S, written as T ïƒS, if every element\nof T is in S, i.e., For all (ï€¢) x if x ïƒT, then x ïƒS. Equivalently, S is\nsuperset of T, symbolized as S ïƒŠT, if and only if T is a subset of S.\nT is a proper subset of S, denoted as TïƒŒS, if T ïƒS and T ï‚¹S.\nHence, if T ïƒŒS, then there is at least one member x such that x ïƒS but\nx ïƒT.\nFor an arbitrary set S the following properties are obvious from\nthe definitions cited above.\n(i) ï¦ïƒS, (ii) S ïƒU (iii) S ïƒS\nEquality of Sets\n\nEquality of Sets\nTwo sets S and T are equal if every element of S is in T and vice versa.\nIn other words, S = T, if and only if, S ïƒT and T ïƒS.\nPower Set\nGiven a set S, the power set of S, denoted as P (S), or 2ğ‘ , is the set of all\nsubsets of S.\nExample 2.5\nMoreover, the chain rule applies to set inclusion operation, i.e., for any\nthree sets A, B, C, if A ïƒB and B ïƒC, then A ïƒC.\nOperations on Sets\n\nThere are three basic operations on sets, union (ïƒˆ), intersection (ïƒ‡), and\ncomplementation ( ï‚¢).\nSet Union\nGiven two sets P and Q, the union of P and Q, denoted as P ïƒˆQ, is the\nset of all elements either in P, or in Q, or in both P and Q.\nP ïƒˆQ = {x | x ïƒP or x ïƒQ}\nSet Intersection\nGiven two sets P and Q, the intersection of P and Q, denoted as\nP ïƒ‡Q, is the set of all elements both in P, and Q.\nP ïƒ‡Q = {x | x ïƒP and x ïƒQ}\nOperations on Sets\n\nComplement of a Set\nThe complement of P, denoted as P ï‚¢, P , ï¿¢P, or ï¾P, is the set\nof all elements (of U) outside P.\nP ï‚¢= {x | x ïƒP}\nThere are various notations for complementation, as indicated in the\ndefinition\nDifference between Sets\nThe difference of set P from Q, denoted as P â€“ Q, is the set\nof all elements in P but not in Q.\nP â€“ Q = {x | x ïƒP and x ïƒQ}\nIt is easy to prove that P â€“ Q = P ïƒ‡Q ï‚¢\nVenn Diagrams\n\nIt is convenient to represent a set theoretic expression visually with the\nhelp of a diagram called Venn diagram. Usually, a Venn diagram consists of a\nrectangle presenting the universal set U with other sets presented with the\nhelp of circles / ovals inside the rectangle.\nVenn Diagrams\n\nCartesian Product\nLet P and Q be two sets. The Cartesian product of P and Q, denoted as P Ã— Q,\nis the set of all ordered pairs (x, y) such that x ïƒP and y ïƒQ.\nP Ã— Q = {(x, y) | x ïƒP and y ïƒQ}\nProperties of Sets\n\nProperties of Sets\n\nFeatures of Fuzzy Sets\n\nFuzzy sets are often characterized with certain features, e.g.,\nnormality, height, support, core, cardinality etc. These features reveal\nthe nature and structure of a fuzzy set.\nFeatures of Fuzzy Sets\n\nFeatures of Fuzzy Sets\n\nFuzzy Membership\n\nFuzzy Membership\n\nThank You"
    },
    {
      "filename": "CE_BE_SC_Week 7_2024-25.pdf",
      "path": "data/materials\\Soft Computing\\CE_BE_SC_Week 7_2024-25.pdf",
      "text": "Soft Computing\nUnit 3: Fuzzy Logic\nFaculty Name : Ms. Archana Khodke\n\nFUZZY MEMBERSHIP\nFUNCTIONS\nFUZZY MEMBERSHIP FUNCTIONS\nLec-2: Introduction to AI\n\nTheoretically any function m F : U â†’ [ 0, 1 ] may act as the membership\nfunction for a fuzzy set F. The nature of the membership function\ndepends on the context of the application. For example, what is hot\nwith respect to human body temperature is certainly very cool for a\nblast furnace.\nFuzzy sets are usually described with simple membership\nfunctions.\nA few parameterized functions that are quite frequently\nused for this purpose are given below along with their graphical\nrepresentations.\nFUZZY MEMBERSHIP FUNCTIONS\nLec-2: Introduction to AI\n\n(a) Triangular function\nPerhaps the most frequently used membership function is the triangular\nfunction.\nFUZZY MEMBERSHIP FUNCTIONS\nLec-2: Introduction to AI\n\n(b) Trapezoidal function\nIts shape is similar to that of a triangular function except that instead of a\nsharp peak, it has a flat peak.\nFUZZY MEMBERSHIP FUNCTIONS\nLec-2: Introduction to AI\n\n(c) Gaussian function\nAnother widely used membership function is the Gaussian function.\nFUZZY MEMBERSHIP FUNCTIONS\nLec-2: Introduction to AI\n\n(d) S-function\nThe step function can be approximated by the S-function as closely\nas required by adjusting the parameters a and b.\nThe point m = (a + b) / 2 is known as the crossover\npoint of the S-function.\nTransformations\n\nThere are a few simple but useful transformations often applied to fuzzy\nmembership functions. Normalization, dilation, concentration, contrast\nintensification and fuzzification are widely used.\nThese transformations are briefly discussed in this subsection. The\neffects of these transformations on Gaussian functions are shown\ngraphically.\n(a) Normalization:\nThe normalization operation\nconverts a subnormal fuzzy set F to\na normal fuzzy set. It is obtained by\ndividing each membership value by\nthe height of the fuzzy set.\nTransformations\n\n(b) Dilation: This operation â€˜flattensâ€™ the membership function so that it\nattains relatively higher values and consequently the overall shape of the\nmembership function gets dilated. In other words, the resultant function\nis less pointed around the higher membership values.\nTransformations\n\n(c) Concentration: Concentration has the reverse effect of dilation. Here\nthe function attains relatively lower values and consequently it gets more\npointed around the higher membership values.\nTransformations\n\n(d) Contrast Intensification: Contrast intensification is achieved by\nreducing the membership values that are less than 0.5 and elevating those\nwith values higher that 0.5\nTransformations\n\n(e) Fuzzification: Fuzzification produces the reverse effect of contrast\nintensification. Here the membership values that are less than 0.5 are\nelevated while those with a value more than 0.5 are reduced.\nLinguistic Variables\n\nThe transformations presented above are useful while dealing with the so\ncalled linguistic variables.\nConventional variables usually have numeric or alphanumeric string\nvalues. In contrast, a linguistic variable may have one of a number of\nallowable words, or phrases, as its value. Each of these legitimate\nlinguistic values corresponds to a fuzzy set.\nFor instance, consider the variable Height. As a conventional variable it may\nhave any real number as its value. But as a linguistic variable itâ€™s value is\nallowed to be one of a few predefined words, say {short, average, tall}.\nWe may define a fuzzy set for each of the words short, average, and tall.\nNow, consider the attribute very tall. Here the word very may be viewed as a\ntransformation applied to the fuzzy meaning of tall. One can reasonably\nimplement very with the help of the concentration operation. The reason is\nconcentration reduces the magnitude of the membership value.\nTransformation on Fuzzy Membership functions\n\nTransformation on Fuzzy Membership functions\n\nOPERATIONS ON FUZZY SETS\n\nOPERATIONS ON FUZZY SETS\n\nOPERATIONS ON FUZZY SETS\n\nOPERATIONS ON FUZZY SETS\n\nOPERATIONS ON FUZZY SETS\n\nOPERATIONS ON FUZZY SETS\n\nOPERATIONS ON FUZZY SETS\n\nProperties of fuzzy set operations\n\nFuzzy Relations\n\nCartesian product\nLet A and B be two sets. Then the Cartesian product of A and B, denoted by\nA Ã— B, is the set of all ordered pairs (a, b) such that a ïƒA, and b ïƒB.\nA Ã— B = {(a, b) ï¼a ïƒA, and b ïƒB}\nSince (a, b) ï‚¹(b, a) we have in general A Ã— B ) ï‚¹(B Ã— A). Hence the operation\nof Cartesian product is\nnot commutative.\nFuzzy Relations\n\nGiven two crisp sets A and B, a crisp relation R between A and B is a\nsubset of A Ã— B.\nA crisp relation between sets A and B is conveniently expressed with the\nhelp of a relation matrix T. The rows and the columns of the relation\nmatrix T correspond to the members of A and B respectively. The\nentries of T are defined as\nFuzzy Relations\n\nFuzzy Relations\n\nFuzzy Relations\n\nThank You"
    },
    {
      "filename": "CE_BE_SC_Week 8_2024-25.pdf",
      "path": "data/materials\\Soft Computing\\CE_BE_SC_Week 8_2024-25.pdf",
      "text": "Soft Computing\nUnit 3: Fuzzy Logic\nFaculty Name : Ms. Archana Khodke\n\nFUZZY LOGIC BASICS\nFUZZY LOGIC BASICS\nLec-2: Introduction to AI\n\nThese are two-valued\nlogic because they are\nbased on the law of\nexcluded middle\naccording to which a\nstatement can be either\ntrue or false, and\nnothing else. Fuzzy logic\nextends crisp logic by\naccepting the possibility\nof the infinite shades\ntruths between absolute\nfalsehood and absolute\ntruth.\nFuzzy Propositions\nLec-2: Introduction to AI\n\nFuzzy logic operations\nLec-2: Introduction to AI\n\nFuzzy Controllers\nLec-2: Introduction to AI\n\nSensor or any other inputs are fed to the controller through the input values\nand mapped to membership values of appropriate fuzzy sets. This is the\nfuzzification step.\nThe processing stage carries some manipulations on the basis of the fired rules\nto obtain fuzzy sets relating to the consequent parts of the fired rules. Finally,\nthe outcome of the processing stage on each rule are combined together to\narrive at a crisp value for each control parameter. This is carried out during the\noutput stage and is termed as defuzzification.\nBlock Diagram of Fuzzy Controllers\nLec-2: Introduction to AI\n\nFuzzy Air Conditioner Controller\nLec-2: Introduction to AI\n\nFuzzy Air Conditioner Controller\nLec-2: Introduction to AI\n\nLet us consider a highly simplified version of an air-conditioner (AC). Its\nfunction is to keep track of the room temperature and regulate the\ntemperature of the air flown into the room. Th e purpose is to maintain\nthe room temperature at a predefined value.\nFor the sake of simplicity we assume that the AC does not regulate the flow\nof air into the room, but only the temperature of the air to be flown.\nLet T0 be the desired room temperature. The air conditioner has a\nthermometer to measure the current room temperature T.\nThe difference ï„T = T âˆ’T0 is the input to the controller. When ï„T ï€¾0, the\nroom is hotter than desired temperature and the AC has to blow cool air\ninto the room so that the room-temperature comes down to T0. If, on the\nother hand, ï„T ï€¼0, the room needs to be warmed up and so, the AC is to\nblow hot air into the room. In order to achieve the required temperature of\nthe air to be blown into the room, a â€˜dialâ€™ is turned at the appropriate\nposition within the range [âˆ’1, +1].\nFuzzy Air Conditioner Controller\nLec-2: Introduction to AI\n\nA positive value of the dial means hot air will be blown, and a negative\nvalue means cold air will be blown. The degree of hotness, or coldness, is\ndetermined by the magnitude of the dial position. No air is blown when\nthe dial is at 0. The input to the fuzzy controller is ï„T = T âˆ’ T0, and the\noutput is D, i.e., the position to which the AC dial is to be turned. Both ï„T\nand D are crisp values, but the mapping of ï„T to D takes place with the\nhelp of fuzzy logic.\n(a) Fuzzy sets: Occasionally, for fuzzy control systems, it is convenient to\ncategorize the strength of the input and the output parameters with the\nhelp of certain fuzzy sets referred to as Large Negative(LN), Medium\nNegative (MN), Small Negative (SN), Zero (ZE), Small Positive (SP), Medium\nPositive (MP), Large Positive (LP) etc. Th ese are indicative of the magnitude\nof the respective parameters in the context of the given application. For the\nsystem under consideration, the fuzzy sets defined on the input\nparameter ï„T and D are LN, MN, ZE, MP, and LP.\nFuzzy Air Conditioner Controller\nLec-2: Introduction to AI\n\nFor example, membership of ï„T to Medium Positive (MP) is zero for ï„T ï‚£0,\nand ï„T ï‚³ï€¶. Th e said membership increases uniformly as ï„T increases from\n0 to 3, becomes 1 at 3, and then uniformly diminishes to 0 as ï„T approaches\n6 from 3.\nFuzzy Air Conditioner Controller\nLec-2: Introduction to AI\n\n(b) Fuzzy rule base: Th e system under consideration has a simple rule base\nconsisting of five fuzzy rules. These are listed below.\nFuzzy Air Conditioner Controller\nLec-2: Introduction to AI\n\nTh e input to the system is ï„T, which is first fuzzified with the help of the\nfuzzy sets membership functions LN, MN, ZE, MP, LP for ï„T. Depending on\nthe result of this fuzzification, some of the rules among R1, â€¦, R5 are fired.\nAs a result of this fi ring of rules, certain fuzzy sets are obtained out of the\nspecification of LN, MN, ZE,MP, LP for D. These are combined and defuzzified\nto obtain the crisp value of D as the output.\nFuzzy Air Conditioner Controller\nLec-2: Introduction to AI\n\nFuzzy Classification\nLec-2: Introduction to AI\n\nFuzzy Classification\nLec-2: Introduction to AI\n\nFuzzy Classification\nLec-2: Introduction to AI\n\nFuzzy Decision Making\nLec-2: Introduction to AI\n\nThank You"
    },
    {
      "filename": "CE_BE_SC_Week1_2024-25.pdf",
      "path": "data/materials\\Soft Computing\\CE_BE_SC_Week1_2024-25.pdf",
      "text": "Soft Computing\nUnit 1: Fundamentals of soft Computing\nFaculty Name : Ms. Archana Ghate\n\nDissemination of\nInstitute & department\nvision-mission, PEO, POs, PSO,\nCOs & POs mapping\nInstitute vision\nTo foster and permeate higher and quality education with\nvalue added engineering, technology programs, providing all\nfacilities in terms of technology and platforms for all round\ndevelopment with social awareness and nurture the youth\nwith international competencies and exemplary level of\nemployability even under highly competitive environment so\nthat\nthey\nare\ninnovative,\nadaptable\nand\ncapable\nof\nhandling problems faced by our country and world at large.\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nSource: dypatil.edu/engineering/vision-mission-goal.php\nInstitute vision\nRAITâ€™s firm belief in a new form of engineering education that lays equal stress on\nacademics\nand\nleadership\nbuilding\nextracurricular\nskills\nhas\nbeen\na\nmajor\ncontribution to the success of RAIT as one of the most reputed institutions of higher\nlearning. The challenges faced by our country and the world in the 21st century needs\na whole new range of thoughts and action leaders, which a conventional educational\nsystem in engineering disciplines are ill equipped to produce. Our reputation in\nproviding good engineering education with additional life skills ensures that high\ngrade and highly motivated students join us. Our laboratories and practical\nsessions reflect the latest that is being followed in the industry. The project\nworks and summer internships make our students adept at handling the real-life\nproblems and be industry ready. Our students are well placed in the industry and\ntheir performance make reputed companies visit us with renewed demands and vigor.\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nSource: dypatil.edu/engineering/vision-mission-goal.php\nInstitute mission\nThe Institution is committed to mobilize the resources and equip itself with men\nand materials of excellence, there by ensuring that the Institution becomes a\npivotal center of service to Industry, Academy, and society with the latest\ntechnology. RAIT engages different platforms such as technology enhancing Student\nTechnical\nSocieties,\nCultural\nplatforms,\nSports\nexcellence\ncenters,\nEntrepreneurial Development Centers and a Societal Interaction Cell. To\ndevelop the college to become an autonomous institution & deemed university at\nthe earliest, we provide facilities for advanced research and development programs\non par with international standards. We also seek to invite international and\nreputed national Institutions and Universities to collaborate with our institution on\nthe issues of common interest of teaching and learning sophistication.\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nSource: dypatil.edu/engineering/vision-mission-goal.php\nInstitute mission\nRAITâ€™s Mission is to produce engineering and technology professionals\nwho are innovative and inspiring thought leaders, adept at solving\nproblems faced by our nation and world by providing quality education.\nThe Institute is working closely with all stake holders like industry,\nAcademy to foster knowledge generation, acquisition, dissemination using\nthe best available resources to address the great challenges being faced by our\ncountry and World. RAIT is fully dedicated to provide its students skills that\nmake them leaders and solution providers and are industry ready when\nthey graduate from the Institution.\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nSource: dypatil.edu/engineering/vision-mission-goal.php\nDepartment of Computer Engineering vision\nTo impart higher and quality education in computer science with value\nadded engineering and technology programs to prepare technically sound,\nethically strong engineers with social awareness. To extend the facilities, to\nmeet\nthe\nfast-changing\nrequirements\nand\nnurture\nthe\nyouths\nwith\ninternational competencies and exemplary level of employability and research\nunder highly competitive environments.\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nSource: dypatil.edu/engineering/vision-of-the-dept.php\nDepartment of Computer Engineering mission\n- To mobilize the resources and equip the institution with men and materials\nof excellence to provide knowledge and develop technologies in the thrust\nareas of computer science and Engineering.\n- To provide the diverse platforms of sports, technical, cocurricular and\nextracurricular activities for the overall development of student with ethical\nattitude.\n- To prepare the students to sustain the impact of computer education for\nsocial needs encompassing industry, educational institutions & public service.\n- To collaborate with IITs, reputed universities and industries for the\ntechnical and\noverall upliftment\nof\nstudents for continuing learning\nand\nentrepreneurship.\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nSource: dypatil.edu/engineering/mission-of-the-department.php\nProgram Outcomes (POs)\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nPO1- Engineering knowledge: Apply the knowledge of mathematics, science,\nengineering fundamentals, and an engineering specialization to the solution of\ncomplex engineering problems.\nPO2- Problem analysis: Identify, formulate, review research literature, and\nanalyze complex 30 engineering problems reaching substantiated conclusions\nusing first principles of mathematics, natural sciences, and engineering sciences.\nPO3-\nDesign/development\nof\nsolutions:\nDesign\nsolutions\nfor\ncomplex\nengineering problems and design system components or processes that meet\nthe specified needs with appropriate consideration for the public health and\nsafety, and the cultural, societal, and environmental considerations.\nPO4- Conduct investigations of complex problems: Use research-based\nknowledge and research methods including design of experiments, analysis and\ninterpretation of data, and synthesis of the information to provide valid\nconclusions.\nSource: dypatil.edu/engineering/pdf/po-co-anyalsis.pdf\nPOs\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nPO5-Modern tool usage: Create, select, and apply appropriate techniques,\nresources, and modern engineering and IT tools including prediction and\nmodelling to complex engineering activities with an understanding of the\nlimitations.\nPO6- The engineer and society: Apply reasoning informed by the contextual\nknowledge to assess societal, health, safety, legal and cultural issues and the\nconsequent responsibilities relevant to the professional engineering practice.\nPO7-Environment\nand\nsustainability:\nUnderstand\nthe\nimpact\nof\nthe\nprofessional engineering solutions in societal and environmental contexts, and\ndemonstrate the knowledge of, and need for sustainable development.\nPO8-Ethics: Apply ethical principles and commit to professional ethics and\nresponsibilities and norms of the engineering practice.\nSource: dypatil.edu/engineering/pdf/po-co-anyalsis.pdf\nPOs\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nPO9-Individual and team work: Function effectively as an individual, and as a\nmember or leader in diverse teams, and in multidisciplinary settings.\nPO10-Communication:\nCommunicate\neffectively\non\ncomplex\nengineering\nactivities with the engineering community and with society at large, such as,\nbeing able to comprehend and write effective reports and design documentation,\nmake effective presentations, and give and receive clear instructions.\nPO11-Project\nmanagement\nand\nfinance:\nDemonstrate\nknowledge\nand\nunderstanding of the engineering and management principles and apply these to\noneâ€™s own work, as a member and leader in a team, to manage projects and in\nmultidisciplinary environments.\nPO12-Life-long learning: Recognize the need for, and have the preparation and\nability to engage in independent and life-long learning in the broadest context of\ntechnological change.\nSource: dypatil.edu/engineering/pdf/po-co-anyalsis.pdf\nCourse Outcomes (Cos)\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\nCO1: Understand the concepts of fundamentals of soft computing.\nCO2: Understand the concepts of neural network.\nCO3: Understand the concepts of Fuzzy Systems.\nCO4: Apply for the solution of multi-level optimization.\nCO5: Create hybrid systems.\nCO6: Evaluate and understand Backpropagation Networks.\nSource: dypatil.edu/engineering/pdf/po-co-anyalsis.pdf\nText books\nLec-1: Institute & department vision-mission, PEO, POs, PSO,\nCOs & POs mapping\n\n-\nSamir Roy and Udit Chakraborty, â€œIntroduction to Soft Computing, Pearson 1st\nEdition.\n-\nSimon S. Haykin, Neural Networks, Prentice Hall, 2nd edition.\n-\nZimmermann, â€œFuzzy Set Theory and its Applicationâ€, 3rd Edition.\nLec-2: Introduction to AI\n\nComputing\nComputing\nInput\nor\nCause\nOutput\nor Effect\nLec-2: Introduction to AI\n\nComputational Paradigms\nLec-2: Introduction to AI\n\nSoft Computing\n- It refers to a group of computational\ntechniques that are based on artificial\nintelligence (AI) and natural selection.\nIt provides cost-effective solutions to\nthe complex real-life problems for\nwhich hard computing solution does\nnot exist.\nLec-2: Introduction to AI\n\nSoft Computing\nLec-2: Introduction to AI\n\nExample of Soft Computing\nLec-2: Introduction to AI\n\nExample of Soft Computing\nHard Computing\n-\nHard computing uses traditional mathematical\nmethods to solve problems, such as algorithms\nand mathematical models. It is based on\ndeterministic and precise calculations and is ideal\nfor solving problems that have well-defined\nmathematical solutions.\n-\nApplications of hard computing include mobile\nrobot coordination and forecasting combinational\nproblems. Hard computing generally requires\nprewritten programs and thus acts on a fixed set of\ninstructions.\n-\nSolving numerical problems for example finding\nroot of polynomials or finding integration or\nderivation we usually follow some mathematical\nmodels.\nLec-2: Introduction to AI\n\nHard Computing\nLec-2: Introduction to AI\n\nSoft Computing Vs Hard Computing\nLec-2: Introduction to AI\n\nSoft Computing Vs Hard Computing\nLec-2: Introduction to AI\n\nSoft Computing Vs Hard Computing\nLec-2: Introduction to AI\n\n\nIntroduction of Soft\nComputing\nCharacteristics of Soft Computing\n\nLec-3: Introduction to ML\nNeed for Soft Computing\n\nLec-3: Introduction to ML\nNeed for Soft Computing\n\nAdvantages of Soft Computing\n\nLec-3: Introduction to ML\nDisadvantages of Soft Computing\n\nLec-3: Introduction to ML\nLec-2: Introduction to AI\n\nSoft Computing\n-\nSoft computing is a family of techniques with\ncapacity to solve a class of problems for which other\nconventional techniques are found to be inadequate.\n-\nThe principal components of soft computing, as on\ntoday, includes\n-\nFuzzy systems (fuzzy set theory, fuzzy logic, fuzzy\ninference systems etc.)\n-\nRough set theory\n-\nArtificial neural networks\n-\nProbabilistic reasoning\n-\nEvolutionary search strategies (including genetic\nalgorithms, simulated annealing, ant colony\noptimization, swarm optimization etc.)\nLec-2: Introduction to AI\n\nSoft Computing Techniques\nLec-2: Introduction to AI\n\nSynergy in Soft Computing Techniques\nLec-2: Introduction to AI\n\nSynergy in Soft Computing Techniques\n-\nFor instance, artificial neural networks generally lack\ncertain characteristics which are present in\nfuzzy logic.\n-\nOn the other hand, fuzzy systems cannot learn,\nadapt, or support parallelism though these are\nclearly present in neural nets.\n-\nThis observation prompted researchers to develop\nneuro-fuzzy systems.\n-\nThat are highly successful hybrid systems. The\ncomplementary role of fuzzy logic and neuro-\ncomputing.\n-\nVarious hybrid soft computing systems, e.g., neuro-\nfuzzy systems,fuzzy neural networks, genetic fuzzy\nsystems, fuzzy-evolutionary algorithms, genetic-\nneural networks etc. have been developed in past\nyears and are being developed.\n\nIntroduction to\nNeural Network (NN) &\nDeep Learning (DL)\nHistory of AI\n-\n1836: Charles Babbage and Augusta Ada Byron invented\nthe first design for a programmable machine.\n-\n1940s: John von Neumann conceived the architecture for\nthe stored-program computer.\n-\n1943: First mathematical model of a neural network, by\nWarren McCulloch and Walter Pitts (\"A Logical Calculus of\nIdeas Immanent in Nervous Activityâ€œ).\n-\n1950: Turing Test by Alan Turing (computer's ability to fool\ninterrogators into believing its responses to their questions\nwere made by a human).\n\nLec-2: Introduction to AI\nHistory of AI\n-\n1956: John McCarthy coined term\nâ€œArtificial Intelligence\" at a\nconference at Dartmouth College, in\nHanover, New Hampshire\n-\n1974â€“80: First AI winter.\n-\n1980s: Revived due to funding from\nthe British government.\n-\n1987â€“1993: Second AI winter.\n-\n1997: IBM's Deep Blue defeated\nRussian grandmaster Garry\nKasparov.\n\nFather of AI\nJohn McCarthy\n(1927-2011)\nLec-2: Introduction to AI\nHistory of AI\n\n[Source: https://www.javatpoint.com]\nLec-2: Introduction to AI\nArtificial vs Natural Intelligence\n- Permanent\n- Precise\n- Consistent\n- Multitasking\n- Fast response\n- Ease of duplication\n- Non creative\n- Difficult to adapt\n- Highly objective\n- Working on self-awareness\n- AI\n- NI\n- Perishable\n- Misjudgement\n- Inconsistent\n- Limited goals\n- Slower than machines\n- Difficult to duplicate\n- Creative\n- Easy to adapt new examples\n- Highly subjective\n- Self-aware\n\nLec-2: Introduction to AI\nAI vs ML vs DL\n\n[source: www.edureka.co]\nLec-2: Introduction to AI\nWhat is Neuron?\nLec-4: Introduction to NN & DL\n\nSource: en.wikipedia.org/wiki/Biological_neuron_model\n- Also known as a spiking neuron models\n- Action potentials or spikes: generate sharp electrical\npotentials across their cell membrane (approx. 1 ms).\n- Transmitted along the axon and synapses from the\nsending neuron to many other neurons,\n- A major information processing unit of the nervous\nsystem.\nWhat is Neuron?\nLec-4: Introduction to NN & DL\n\nSource: en.wikipedia.org/wiki/Biological_neuron_model\nBiological neuron models\nWhat is Artificial Neural Network\n(ANN)?\nLec-4: Introduction to NN & DL\n\nBiological NN\nArtificial NN\nDendrites\nInputs\nCell nucleus\nNodes\nSynapse\nWeights\nAxon\nOutput\nHistory of ANN\n- 1943, Warren McCulloch & Walter Pitts: Created a computational model for\nneural networks.\n- 1949, D. O. Hebb: Created a learning hypothesis based on the mechanism of\nneural plasticity, known as Hebbian learning.\n- 1954, Farley & Wesley A. Clark: Simulate a Hebbian network using\ncomputational machines (\"calculatorsâ€œ).\n- 1958, Frank Rosenblatt: Invented the perceptron, first artificial neural network.\n- 1960, Kelley & 1961 Bryson: Derived the basics of continuous\nbackpropagation using principles of dynamic programming.\n- 1965, Ivakhnenko and Lapa: Published first functional multi-layered networks,\nas the Group Method of Data Handling.\n- 1969, Minsky and Papert: Discovered that basic perceptrons were incapable of\nprocessing the EXOR circuit & that computers lacked sufficient power to\nprocess useful neural networks.\nLec-4: Introduction to NN & DL\n\nANN architecture\nLec-4: Introduction to NN & DL\n\nSource: T. Jain et al. â€œApplication of polynomial chaos theory as an accurate\nand computationally efficient proxy model for heterogeneous steamâ€assisted\ngravity drainage reservoirs,â€ Energy Sci. & Engg., 5, 2017.\nThank You"
    },
    {
      "filename": "CE_BE_SC_Week2_2024-25.pdf",
      "path": "data/materials\\Soft Computing\\CE_BE_SC_Week2_2024-25.pdf",
      "text": "Soft Computing\nUnit 2: Artificial Neural Network\nFaculty Name : Ms. Archana Khodke\n\nPerceptron\nLec-2: Introduction to AI\n\nArtificial Neuron\nBiological Neuron\nLec-2: Introduction to AI\n\nBiological Neuron Vs Artificial Neuron\nLec-2: Introduction to AI\n\nArtificial Neuron\nLec-2: Introduction to AI\n\nLec-2: Introduction to AI\n\nArtificial Neuron\nLec-2: Introduction to AI\n\nArtificial Neuron\nLec-2: Introduction to AI\n\nArtificial Neuron\nLec-2: Introduction to AI\n\nArtificial Neuron\nLec-2: Introduction to AI\n\nArtificial Neuron\nMcCulloch-Pitts Neural Model\nLec-2: Introduction to AI\n\nMcCulloch-Pitts Neural Model\n\nLec-2: Introduction to AI\n\nAND Operation\nLec-2: Introduction to AI\n\nOR Operation\nLec-2: Introduction to AI\n\nAND NOT LOGIC\nLec-2: Introduction to AI\n\nXOR LOGIC\nMcCulloch-Pitts Neural Model\nLec-2: Introduction to AI\n\nMcCulloch-Pitts Neural Model\nLec-2: Introduction to AI\n\nMcCulloch-Pitts Neural Model\nLec-2: Introduction to AI\n\nMcCulloch-Pitts Neural Model\nLec-2: Introduction to AI\n\nPerceptron\nLec-2: Introduction to AI\n\nPerceptron\nLec-2: Introduction to AI\n\nPerceptron\nLec-2: Introduction to AI\n\nPerceptron\nLec-2: Introduction to AI\n\nPerceptron Learning\nLec-2: Introduction to AI\n\nPerceptron Function\nLec-2: Introduction to AI\n\nPerceptron Function\nLec-2: Introduction to AI\n\nActivation Function\nLec-2: Introduction to AI\n\n1.Identity Function\n2.Step /Threshold Function\n3. ReLU(Rectified Linear Unit) Function\n4.Sigmoid Function\n5.Hyperbolic Tangent Function\nIdentity Function\nLec-2: Introduction to AI\n\nIdentity Function\nStep Function\nAnother frequently used activation function is the step function.\nTh e basic step function produces a 1 or 0 depending on\nwhether the net input is greater than 0 or otherwise. This is the\nonly activation function we have used so far in this text.\nMathematically the step function is defined as follows.\nFig. 6.31(a) shows the shape of the basic step function\ngraphically. Occasionally, instead of 0 a non-zero\nthreshold value q is used. Th is is known as the threshold\nfunction and is defined as\nStep/Threshold Function\nStep/Threshold Function\nStep/Threshold Function\nOccasionally, it is more convenient to work with bipolar data,\nâˆ’1 and +1, than the binary data. If a signal of value 0 is sent\nthrough a weighted path, the information contained in the\ninterconnection weight is lost as it is multiplied by 0.\nTo overcome this problem, the binary input is converted to\nbipolar form and then a suitable bipolar activation function is\nemployed. Accordingly, binary step functions have their bipolar\nversions. The output of a bipolar step function is âˆ’1 or +1.\nReLU and Softplus\nLec-2: Introduction to AI\n\nReLU and Softplus\nLec-2: Introduction to AI\n\nSigmoid Activation Function\nLec-2: Introduction to AI\n\nAs the step function is not continuous it is not differentiable.\nSome ANN training algorithm requires that the activation\nfunction be continuous and differentiable. Th e step function\nis not suitable for such cases. Sigmoid functions have the nice\nproperty that they can approximate the step function to the\ndesired extent without losing its differentiability.\nBinary sigmoid, also referred to as the logistic sigmoid,\nis defined by Equation,\nSigmoid Activation Function\nLec-2: Introduction to AI\n\nTh e parameter s is known as the steepness parameter.\nThe transition from 0 to 1 could be made as steep as desired by\nincreasing the value of s to appropriate extent.\nDepending on the requirement, the binary sigmoid function can\nbe scaled to any range of values appropriate for a given\napplication. The most widely used range is from âˆ’1 to +1, and\nthe corresponding sigmoid function is referred to as the bipolar\nsigmoid function.\nSigmoid Activation Function\nLec-2: Introduction to AI\n\nHyperbolic or tanh Function\nLec-2: Introduction to AI\n\nAnother bipolar activation function that is widely employed in\nANN applications is the hyperbolic tangent function. The\nfunction, as well as its first derivative, are expressed by\nEquations respectively.\nThe hyperbolic tangent function is\nclosely related to the bipolar\nsigmoid function. When the input\ndata is binary and not continuously\nvalued in the range from 0 to 1,\nthey are generally converted to\nbipolar form and then a bipolar\nsigmoid or hyperbolic tangent\nactivation function is applied on\nthem by the processing unit.\nHyperbolic or tanh Function\nLec-2: Introduction to AI\n\nAND Perceptron\nLec-2: Introduction to AI\n\nOR Perceptron\n\nXOR Decision Boundary\n\nAugmented Vectors\n\nAn artificial neuron is characterized by its weight\nvector v, threshold Î¸ and activation function. During\nlearning, both the weights and the threshold are\nadapted. To simplify learning equations, the input\nvector is augmented to include an additional input\nunit, zI+1, referred to as the bias unit.\nThe value of zI+1 is always -1, and the weight vI+1\nserves as the value of the threshold. The net input\nsignal to the AN (assuming SUs) is then calculated\nas\nAugmented Vectors\n\nThe value of zI+1 is always -1,\nand the weight vI+1 serves as\nthe value of the threshold.\nwhere Î¸ = zI+1vI+1 = âˆ’vI+1.\nIn the case of the step function, an\ninput vector yields an output of 1\n\nAn input vector yields an output of 0\nGradient Descent Learning Rule\n\nWhile gradient descent (GD) is not the first training rule\nfor ANs, it is possibly the approach that is used most to\ntrain neurons .GD requires the definition of an error\nfunction to measure the neuronâ€™s error in approximating\nthe target.\nThe sum of squared errors is usually used.\nwhere tp and op are respectively the\ntarget and actual output for the\np-th pattern, and PT is the total\nnumber of input-target vector pairs\n(patterns) in the training set.\nGradient Descent Learning Rule\n\nThe aim of GD is to find the weight values that minimize\nE. This is achieved by calculating the gradient of E in\nweight space, and to move the weight vector along the\nnegative gradient\nweights are updated using\nWith\nWhere\nGradient Descent Learning Rule\n\nand Î· is the learning rate (i.e. the size of the steps taken in\nthe negative direction of the gradient). The calculation of\nthe partial derivative of f with respect to netp (the net input\nfor pattern p) presents a problem for all discontinuous\nactivation functions, such as the step and ramp functions;\nzi,p is the i-th input signal corresponding to pattern p.\nThe Widrow-Hoff learning rule presents a solution for the step\nand ramp functions, while the generalized delta learning rule\nassumes continuous functions that are at least once\ndifferentiable.\nWidrow-Hoff Learning Rule\n\nassume that f = netp ,\nWeights are then updated using\nThe Widrow-Hoff learning rule, also referred to as the least-means-\nsquare (LMS) algorithm , was one of the first algorithms used to train\nlayered neural networks with multiple adaptive linear neurons. This\nnetwork was commonly referred to as the Madaline .\nGeneralized Delta Learning Rule\n\nThe generalized delta learning rule is a generalization of\nthe Widrow-Hoff learning rule that assumes differentiable\nactivation functions. Assume that the sigmoid function\nis used. Then,\nFor the error-correction learning rule it\nis assumed that binary-valued\nactivation functions are used, for\nexample, the step function. Weights\nare only adjusted when the neuron\nresponds in error. That is, only when\nThank You"
    },
    {
      "filename": "Fuzzy Logic Numerical.pdf",
      "path": "data/materials\\Soft Computing\\Fuzzy Logic Numerical.pdf",
      "text": "D CorsidoL two given tozy ses\nA\nPertnn\nComplmsnt\n\nlbuing\nChion\nUnion, Tnteecon, diaence ond\n\noVer hzzy\nsels A and B.\ns\ntoa the\nghven frzy sel\nure have the\nAVB = max\n+\nAUB =\nCmglensnt\nANB=o2+ al + 2\n=eto2\n+o\n\nDAance\nA\\B = AAB =\nA=\nB=\nsels.\n\nFnd\naycbie sunm\nSum and\nbounded diAeence t the given frzz4\nO28\nalydie proluc, boundesA\nSum\n\n\n\nO O6\n+ O99\n\nt\n\n\nO02\nBounded Sum\nAOA\n)\n) Bsunded feence\n\n-max\n\n+Oio8\nQ6\n\n\n\nQ)\nTho. elormonts in two ses A and B ane given\nA= {2143 and B fajbef\nFnd the vaiaus\ncadegjam\nRiodu cs f thase two\nSon>\nhe vai ow Caresi an Rroduds f these twa\ngien ses\nAXA\nBxA - Sa,e), (be), Coz), (a,s), Cor), (G9\nae\nBXB =8\nQ.> Conidoe the follwing tws fuz24 sds:\nfuzzy sels.\nCadesie\nO9\nO7\nthase giren\nSâ€™ Tke fuzzy\nCalesian puduct perfumed ovee\nover fuzz4 sels A and B esulls 'm fuz4 onelaha R\ngren by R= AXB. Homce\nThus\n-min (o.3, 0.<) - o.3\n= nn ( o3, 09) = o3\nWR Coee, y2) = n CUA o),l8 cy2)\n= Un Co 7,0-9) = O7\nfollous\n= in i, o9) = o9\nthe\nCartesian Rradut betweon fzzy sek A\ncbtained\nTwo tuzzy nelebons\no'6 o3\nane gven by\nObtai n fuzey aelation T as a cornposi bon betucen\nSAnâ€™ The composbion bebveen two giren tuzzy\nnoleions s pofamed in two ways ag\nMax-min conposibon\nMax-podu ct compsibn\nMak min compastin\nT= Ros = e,o:6 OS 03\n*2o8 O4 07\n\nThe caleulations fon dtaining T ane as falkos\nZ3\n=max\nmin (o61), mno3, 08)\n= max (o6, 0:3)\nJlr (oi,2i)= o:6\nUT (ai9 Z2)=max\n=maxfinC o6,os), min (03,04\n=max CoSy013)\nJr Gaij2a) = 0:5\nUs (o, z3) = mxCsnin Co-6, o 35), min Co3, o\n= max (o13,03) =o3\nUs Coz z)- mox Cin Coi2,1),min (o9,0 8)\n= max (o2,os)= 0&\nT (3c2y z2) = max Cnin (o2,0 i5) , in (o9,o1\nrax (o-2, 0-)= o4\n=\n= mar Coi2, O?) =02\nNax- podact Con posihen\nT- RoS\nCaloula\nfar T ane\no tollocws;\n- max(0-6y Gr25) =06\n= max( 03, 0-12)= 03\nUT Cotigz3)=\nnax CCosxo3), Coo Xo))\n= max (0-18, o 2)= 0.2)\nu Cxezi= mno.2X), (o3 K0.8)\n= Jax Co 2yo72) = 0.72\nUr (ote22) = hax Co2x os\nCo9 xo))\n=nax 6.l y 0.36) - o 36\nUr Cxyz) = naxlco2x03), (o-3 xo2]\n= max (o.o6) 0-\n) =o63\nThe Fuzz4\nelatn Tby max\nCompis bin\n\n\nT dtyo. o3 02])"
    },
    {
      "filename": "Heteroassociative memory network numerical.pdf",
      "path": "data/materials\\Soft Computing\\Heteroassociative memory network numerical.pdf",
      "text": "1. Train a heteroassociative memory network using\nHebb rule to store input row vector s\n(s1, S2, 533 S4) to the output row vector t= (t,b).\nThe vector pairs are given in Table 1.\nTable 1\nInput targets\n1st\n2nd\n3rd\n4th\nS1\n\n\n\n\nS2\nS3\n\n\nS4\n\n\n\n\ntÃ‹\n\n\nt\n\n\nSolution: The network for the given problem is as\nshown in Figure 1. The training algorithm based on\nHebb rule is used to determine the weights.\nXA\nW21\nW11\nW22\nW31\nW12\nW32\nW\u00041\nWA2\nFigure1\nNeural net.\n\nFor lst input vector\nStep 0: Initialize the weights, the initial weights\nare taken as zero.\nStep l: For first pair (1,0, 1, 0):(1, 0)\nStep 2: Set the activations of input units:\nA=1, N)=0,\nNy\n1,\nx\n\nStep 3: Set the activations of output unit:\n))=1, y) =0\nStep 4: Update the weights,\nwy(new) = wy(old) + xJy\nen (new)= wm(old) + ) =0+1x 1=1\nw21 (new)=w2(old) +x) =0+0x 1 =0\ntU31 (new)\nW31 (old) + x3) =0+1xl= 1\nW41 (new)= wa (old) + x) =0+0 x l=0\nw2(new)= w2(old) +x2 =0+1x0=0\nw22 (new)=w2(old) + x2 =0+0x 0=0\nwg2 (new) = w32(old) + x3)2 = 0 +l x 0 =0\nwg2(new)= wpold) + x4y2 =0+0 x 0 = 0|\nFor 2nd input vector:\nThe input-output vector pair is (1, 0, 0, 1):(1, 0)\nx1=1,\nx) =0, x3 = 0, x4 = 1,\ny1 =1,\n2 =0\nwi (new)= wi(old) + x11 =1+1x l=2\nw\u0004| (new)= w4 (old) + x4y1 =0+1x l=1\nSince xp =X} =)2 =0, the other weights remains\nthe samne.\nThe final weights afer second input vector is pre\nsented are\nW] = 2,\nw21 = 0,\nw31 = l, w\u00041 =1\nW12\n= 0, w2 = 0, w32 = 0,\nw42 =0\nFor 3rd input vector:e\nThe input-output vector pair i is (1, I,0,0:0,1)\nxÃŠ, ) =1, xy =0, x4 = 0,\nTraining, using Hebb rule, evolves the\nfinal\nweighN\nas follows:\nSince y\n0, the weights of y1 are going to\nComputing the weights ofy unit, we obrain\nwp(new)\nwpold)\nare\nwy2 (new) = wp(old) +xy 0+1xl=\nwy) (new) = Wy(old) +xsyz\nwp(new) = Wp(old) +.\n=0+0 x 1 =\nThe final weights after presenting third inpur vece\nNetet\nUU = 2,\nwzi =0, W31 =1, ws =1\nU1) = 1, w2 = 1, w32 = 0, wsy\n\nFor 4th input vector:\nThe input-output vector pair is (0,0, 1, ):(0.\nx = 0, x) = 0, x3 =l, x4 =I, yi =0,=t\nThe weights are given by\nThe final weights obtained for the input vector pair\nThus, the weight matrix in matrix form is\nis used as initial weight here:\nw32 (new)= w2(old) + xy =0+1x1=l\nw42(new)= w2(old) + x4y2 = 0+1x|=l\nSince, x1=x) =y1 =0, the other weights remai\nthe same. The final weights after presenting the founs\nInput vector are\nW11\n=2, W21 =0, w31 = 1,w\u0004ÃŠ =1\nW12 =1, W22 = 1,w32 = 1, uwp =1\nW=\nW21 W22\nW31\nW32\nLw41\nW42\n2. Train the heteroassociative memory nerwork usil;\nOuter products rule to store input row veCtOE=\n(S1,52, 93,4) to the output row vectors\nUse the vector pairs as given in Table 2.\nA10 Solved Problems\nTable 2\nInput and targets\ngnd\n3rd\nqth\n\n)\ndp) =s'1) 1)\n\n\n\n\n\n\np) ap) =s'2) K2)\n\n4x1\n\n\n\np=l\nap) =(3) K3)\nP\n4x1\nS2\n\n\n\nW=)\nnp)\n\nS3\n\n\nSolution: Use outer products rule to determine the\nweight matrix:\n\n\n(0011), = (0 1). For p = 4,\nFor lst pair: The input and output vectors are s =\n(1010), t= (1 0). For p = 1,\n\n\n\n'p)ap) = s(4) 44)\n0 0\nt1\n\n1 0\n\nFor 2nd pair: The input and output vectors are s =\n(100 1), = (10). For p = 2,\n\n0 0\n\n\n\n1 07\n0 1\n0 0\n|\n\n\nFor 3rd pair:. The input and output vectors are s F\n(1100), r= (0 1). For p = 3,\n\nLo o42\n\nFor 4th pair: The input and output\nvectors are Â»=\nW\nThe final weight matrix is the summation of all the\nindividual weight matrices obtained for each pair.\np=l\n2nd\n3rd\n\n4th\n\n\n1 0\n0 0\n=)1) +T(2)42) +(3)3) +(4)44)\n|2 1\n\nTable 3\n1 1\n1 1\nInput and targets\n+\n0 0\n\nS1\n\n\n\n3. Train a heteroassociative memory nerwork to store\nthe input vectors s = (S1, S2, 53, 54) to the output\nvectors t = (1, ). The vector pairs are given in\nTable 3. Also test the performance of the network\nusing its training input as testing input.\nS2\nP\n\np=l\n\n\n0 0\n\n0 0\n\n0 1\n|0\n\n$3\n\n\n\n|0 0\n\n\n\nS4\n\n\n\n\nSolution: The network architecture for the given\ninput-target vector pair is shown in Figure 2. Train\ning the network means the determination of weights\nof the network. Here outer products rule is used to\ndetermine the weigh.\n1 10\nThe weight matrix Wusing outer products rule is\ngiven by\n\nFor p = 1 to 4,\nW\n\n\nMehod I\n\n00 +\nTesting the Network\n\n\nThis is the final weight of thc marrix.\nWI1\n\nW32\nWe\nWa2\n+\nFigure 2 Network architecture.\n0 0\n\nY2\nThe testing algorithm for a heteroassocjative mem\nory network is used to test the performance of the\nnet. The weight obtained from training algorithm is\nthe initial weight in testing algorithm.\nFor Ist testing input\nStep 0: Initialize the weights:\nStep 2:\nW\nStep 3:\nStep 1: Performs Steps 2-4 for\neach testing\ni=1\nW31\ninput-output vectot.\ntw41\nn\ni=1\ninpur pattern.\nAssociative Memory Netwos\nSet the activations, x =[1000L\nCompute the net input, n = 4, m =2\nFor i = 1 to 4 and j= 1 to 2:\nYin2 =\nxw2\ni=l\nW32\n1WA2\n=1x0 +0x0+0x1+0x2=\ninput, we obtain\n\n\nFor 2nd testing inpuE\n=X1 W12 +x2w22 +3wn2tx4W\n=lx2+0x1+0x0+0x0=2\nStep 4: Applying activation over the net input te\ncalculate the output.\nThe e output is [0, 1] which is correcE espoase foc tins\nn=fyint) =f0) =0\n2=fin2)=f(2) =1\nSet che activation x=|1 L100) Compucing\nche nct\n0+0+0+0=0\nYin2\nW12 tx2w22 +x3w32\n=2+1+0+0=3\nCompute the output by applying a activations over net\ninput,\ny1 =fYin) =f0) = 0\ny2 =fYin2) =f(3) =1\nThe output is [0, 1] which is correct response for\nsecond input pattern.\nFor 3rd testing input\nCthe activation x = [0 00 1). Computing net\ninput, we obtain\n=0+0 +0+2=2\nVin2 =X] W12 t12W22 t x3W32 t xwa\n=0+0 +0+0=0\nCalculate output of the network,\ny1 =fyin1) =f(2) =1\ny2 =fYin2) =f(0) =0\nFor 4th testing input\nSet the activation x = [0 0 11]. Calculating the net\nInput, we obtain\nVinl = X1 W11 t\nx W21 t x3 W31 + x4w41\n= 0+0 +1+2=3\nJin2 =*1 W12 t x2w2 + x3w32 + x4w42\n=0+0+0+0 =0\nCalculate the output of the network,\nMethod II\ny1 =fyin) =f(3) =1\ny2 =fyin2) = f(0) =0\ntourth testing input pattern.\ntest performance of network. The initial weights for\nthe network are\nW=\nFor Ist testing input\nThe binary activations are used, i.e.,\nThe output is (1 0] which is correct response for third\nApplying activations over the net input, we get\ntesting input pattern.\n0 1\nSet the activation x = [1 0 0 01. The net input is\ngiven by Vin = xW(in vector form):\nlyinl Yin2) = [100 0]1x4\n= [0 2]\n1 0\n|2 o\nFor 2nd testing input\nwe get\n|1 if x0\n]0 if x< 0\nSince net input is the dot product of the input row\nVector with the column of weight mnatrix, hence a\nmethod using matrix multiplication can be used to\ninput.\n(yinl Yin2] =[l100]\n\n= [0+0+0+0 2+0+0+0]\n= [0 3]\n0 21\n1 0\n|2 0J4x2\nyn y2] = [O 1]\nThe correct response is obtained for first testing input\npattern.\nSet the activation x = [1 1 0 0]. The net input is\nobtained by\n\n\nfo 27\n1 0\n\nThe Output is 1 0] which is correct response for\nApply activations over the net input to obtain output.\n= [0 + 0 +0+0 2 +1+0+ 0]\n(yinl Yin2] = |0 1]\nThe correct response is obtained for second testing\n\nFor 3rd testing input\nSet the activation x = [0 0 0 1]. The net input is\nobtained by\nyinl yin2] = [000 1]\nO 2\n\n1 0\n2 0\n= [0 +0+0+2 0 +0 +0 + 0]\n= [2 0]\nApplying activations to calculate the output, we get\nyn y2] = [1 0]\nThus, correct response is obtained for third testing\ninput.\ny'inl Jan2) = [0 0 1 1]\nFor 4th testing input\nSet the activation x = [0\n0 1 1]. The net input is\ncalculated as\nvector.\n0 2\n\n\n|2 0\n= [0 +0+1+2 0+0+0+0]\n= 3 0]\nThe output is obtained by applying activations over\nthe net input:\nIn y2]=[1 0]\nThe correct response is obtained for fourth test\ning input. Thus, training and testing of a hetero\nassociative network is done here.\n4. For Problem 3, test a heteroassociative network\nwith a similar test vector and unsimilar test\nSolution: The heteroassociative network has to be\ntested with similar and unsimilar test vector.\nWith similar test vector: From Problem 3, the sec\nond input vector is x= [1100] with target y = [0 1].\nTo test the network with a similar vector, making a\nchange in one component of the input vector, we get\nx= (0 1 0 0]\nThe weight matrix is\nW=\nyint yin2] = [0 10 0]\nAssociative\nMemory Neetwots\n0 2\nThe net input is calculated for the similar vector,\n\nThe weight matrix is\n\n2 0\nW=\n0 21\nyinl Yin2] = [0 11 0]\n\n\n= [0 + 0 +0+0 0+1+0+01\n= [0 1]\nThe output is obtained by applying activations oe\nthe net input\n1 0\n|2 o\ny1 y2l= [0 1]\nThe correct response same as the target is found.\nhence the vector similar to the input vector is\nrecognized by the network.\nWith unsimilar input vector The second inpur\nvector is x= [11 0 0] with target y= [0 1]. To\ntest the network with unsimilar vectors by making a\nchange in two components of the input vector, we ger\nol x= [0 1 1 0]\n[0 2\n\n\nThe net input is calculated for unsimilar vector,\n\n\n\n[2 o\n= [0 +0 +1+0 0+1+0+0)\n=[! 1]\nThe output is obtained by applying activations over\nthe net input\nn y2l= [1\n1]"
    },
    {
      "filename": "Practice questions for module 1 and 2.pdf",
      "path": "data/materials\\Soft Computing\\Practice questions for module 1 and 2.pdf",
      "text": "1. Explain advantages and disadvantages of Soft Computing.\n2. Compare hard and soft computing.\n3. Explain the need of Soft Computing.\n4. Explain different Soft Computing techniques.\n5. What is the activation function? Illustrate different types of activation functions.\n6. Explain Adaline Linear Network with architecture.\n7. Explain different learning rules for Artificial Neural Network.\n8. Illustrate multilayer feed forward network.\n9. Numerical on feed forward network using different activation functions.\n10. Illustrate McCulloch-Pitts Neural Model with example.\n11. Train Auto-associative memory network for input vector [1 1 -1 1],then test the\nnetwork for same input vector and also test for one missing and one mistake\nentries in test network.\n12. Train the hetero-associative memory network using Hebb rule to store input row\nvector S=(S1,S2,S3,S4) to the output row vector t=(t1,t2). The vector pairs are\ngiven below,\nI/P ,t\nS1\nS2\nS3\nS4\nt1\nt2\nI\n\n\n\n\n\n\nII"
    },
    {
      "filename": "Question bank  for module 1,2 and 3.pdf",
      "path": "data/materials\\Soft Computing\\Question bank  for module 1,2 and 3.pdf",
      "text": "1. Explain advantages and disadvantages of Soft Computing.\n2. Compare hard and soft computing.\n3. Explain the need of Soft Computing.\n4. Explain different Soft Computing techniques.\n5. What is the activation function? Illustrate different types of activation functions.\n6. Explain Adaline Linear Network with architecture.\n7. Explain different learning rules for Artificial Neural Network.\n8. Illustrate multilayer feed forward network.\n9. Numerical on feed forward network using different activation functions.\n10. Illustrate McCulloch-Pitts Neural Model with example.\n11. Train Auto-associative memory network for input vector [1 1 -1 1],then test the\nnetwork for same input vector and also test for one missing and one mistake\nentries in test network.\n12. Train the hetero-associative memory network using Hebb rule to store input row\nvector S=(S1,S2,S3,S4) to the output row vector t=(t1,t2). The vector pairs are\ngiven below,\nI/P ,t\nS1\nS2\nS3\nS4\nt1\nt2\nI\n\n\n\n\n\n\nII\n\n\n\n\n\n\n13. Write short notes on Fuzzy Classification.\n14. Illustrate Different types of Fuzzy sets.\n15. The elements in two sets A and B are given as A={2,4} and B={a,b,c} Find the\nvarious cartesian products of these two sets.\n16. Explain Features of Fuzzy Sets in detail.\n17. Consider two fuzzy sets\nA={\n0.2\n1 +\n0.3\n2 +\n0.4\n3 +\n0.5\n4 } and B={\n0.1\n1 +\n0.2\n2 +\n0.2\n3 +\n\n4}\nCalculate algebric sum, algebraic product,bounded sum and bounded difference.\n18. Defuzzification Techniques.\n19. Two fuzzy relations are given by\nz1 z2 z3\ny1 y2 y1 1\n0.5\n0.3\n0.8\n0.4\n0.7\nR= x1 0.6\n0.3\n0.2\n0.9 and S= y2\nx2\nObtain fuzzy relation T as a composition between given fuzzy relations.\n20. Define cartesian product in Fuzzy Logic.\nFind union, intersection, difference and compliment for the two given fuzzy sets.\nA={\n\n2 +\n0.3\n4 +\n0.5\n6 +\n0.2\n8 } and B={\n0.5\n2 +\n0.4\n4 +\n0.1\n6 +\n\n8 }\n21. Applications of fuzzy logic."
    }
  ]
}