{
  "subject": "Big Data Analytics",
  "files": [
    {
      "filename": "BE_COMP_BDA_Module-3-Week 6_L21-L22.pdf",
      "path": "data/materials\\Big Data Analytics\\BE_COMP_BDA_Module-3-Week 6_L21-L22.pdf",
      "text": "Big Data Analytics\nWeek 6: Module 3: NoSQL\nFaculty Name :\nDr. Tushar Ghorpade\nDr. Poonam Gadge\nIndex - Module :3 NoSQL\n\n\nshared nothing architecture\n\n\nthat NoSQL system handle big data problems.\n\nNoSQL Case Study\n\n-\nCassandra is the most popular wide column store database system on the market.\n-\nInitially developed at Facebook for Facebook’s Inbox search feature, Cassandra was\nopen-sourced in 2008 and subsequently made a top-level project for Apache on\nFebruary 17, 2010.\n-\nWritten in Java\n-\nCassandra offers synchronous and asynchronous replication for each update.\nCase Study on Column Store Database : Cassandra\n\n\n-\nuses a master less “ring” architecture like master-slave architecture\n-\nall nodes in a cluster are treated equally, and a majority of nodes can be used to\nachieve quorum.\n-\nIt stores data in columns and rows like a traditional RDBMS\n-\nCassandra Query Language (CQL), closely resembles the traditional SQL syntax,\n-\nIt offers advanced repair processes for read, write, and entropy (data\nconsistency), which makes its cluster highly available and reliable.\n-\nalso allows for better fault tolerance compared to document stores like\nMongoDB, which might take up to 40 seconds to recover.\nCassandra: Advantages\n\n\n-\nAs the architecture is distributed, replicas can become inconsistent.\n-\nWhile scanning data, Cassandra handles itself well if the primary key is known,\nbut suffers severely if it is not.\n-\nAnother major drawback is its lack of solid official documentation from the\nApache Software Foundation; you must rely on third-party companies like\nDataStax for this.\nCassandra Disadvantages\n\n\n-\nCassandra boasts several large organizations among its users, including\n-\nGitHub,\n-\nFacebook,\n-\nInstagram,\n-\nNetflix, and\n-\nReddit.\nCassandra Users\n\n\nCase Study on Document Store : MongoDB\n\n\n-It is most popular document store on the market and is also one of the leading\nDatabase Management Systems.\n-It was created in 2007 by the team behind DoubleClick (currently owned by Google) to\naddress the scalability and agility issues in serving Internet ads by DoubleClick.\n-\nis a schema-less database and stores data as JSON-like documents (binary\nJSON).\n-\nThis provides flexibility and agility in the type of records that can be stored, and\neven the fields can vary from one document to the other.\n-\nreduce the I/O overload generally associated with database systems\n-\nprovides several enterprise features, like high availability(through replica) and\nhorizontal scalability (clustering).\n-\nMongoDB include a real-time view of your data, mobile applications, IoT\napplications, and content management systems.\nMongoDB Advantages\n\n\n-\ntime-consuming processes\n-\nAlthough it has improved in the newer versions, MapReduce implementations\nstill remain a slow process.\n-\nMongoDB also suffers from memory hog issues as the databases start scaling.\nMongoDB Disadvantages\n\n\n-\nMongoDB counts names like\n-\neBay,\n-\nGoogle,\n-\nCisco,\n-\nSAP, and\n-\nFacebook.\nMongoDB Users\n\n\n-\nGraph databases are based on graph theory from mathematics.\n-\nGraphs are structures that contain vertices (which represent entities, such as\npeople or things) and edges (which represent connections between vertices).\n-\nEdges can have numerical values called weight.\nCase study on Graph Database : Neo4j\n\n\n-\nNeo4j provides its own implementation of graph theory concepts. It has the following\ncomponents:\n-\nNodes (equivalent to vertices in graph theory). These are the main data elements that\nare interconnected through relationships. A node can have one or more labels (that\ndescribe its role) and properties (i.e. attributes).\n-\nRelationships (equivalent to edges in graph theory). A relationship connects two\nnodes that, in turn, can have multiple relationships. Relationships can have one or\nmore properties.\n-\nLabels. These are used to group nodes, and each node can be assigned multiple\nlabels. Labels are indexed to speed up finding nodes in a graph.\n-\nProperties. These are attributes of both nodes and relationships. Neo4j allows for\nstoring data as key-value pairs, which means properties can have any value (string,\nnumber, or boolean).\nNeo4j\n\n\nExample in Neo4j\n\n\nAs you can see, this graph contains two nodes (Alice and Bob) that are connected by\nrelationships. Both nodes share the same label, Person. In the graph, only Bob’s node\nhas properties, but in Neo4j every node and relationship can have properties.\n-\nPerformance : In relational databases, performance suffers as the number and\ndepth of relationships increases. In graph databases like Neo4j, performance\nremains high even if the amount of data grows significantly.\n-\nFlexibility : Neo4j is flexible, as the structure and schema of a graph model can\nbe easily adjusted to the changes in an application. Also, you can easily upgrade\nthe data structure without damaging existing functionality.\n-\nAgility : The structure of a Neo4j database is easy-to-upgrade, so the data store\ncan evolve along with your application.\nNeo4j Advantages\n\n\n-\nFraud detection and analytics\n-\nNetwork and database infrastructure monitoring\n-\nRecommendation engines\n-\nSocial networks\n-\nKnowledge graph\n-\nIdentity and access management\nNeo4j Use Cases\n\n\n-\nIt is a fully managed proprietary NoSQL database service that supports key-value\nand document data structures and is offered by Amazon.com as part of the\nAmazon Web Services portfolio.\n-\nDynamo had a multi-master design requiring the client to resolve version conflicts\nand DynamoDB uses synchronous replication across multiple data centers for\nhigh durability and availability.\n-\nDynamoDB was announced by Amazon CTO Werner Vogels on January 18, 2012,\nand is presented as an evolution of Amazon SimpleDB solution.\nCase Study on key-value store database : Amazon DynamoDB\n\n\n-\nis a fully managed NoSQL database service that provides fast and predictable\nperformance with seamless scalability.\n-\nDynamoDB lets you offload the administrative burdens of operating and scaling\na distributed database so that you don't have to worry about hardware\nprovisioning, setup and configuration, replication, software patching, or cluster\nscaling.\n-\nDynamoDB provides on-demand backup capability.\n-\nWith DynamoDB, you can create database tables that can store and retrieve any\namount of data and serve any level of request traffic.\n-\nDynamoDB allows you to delete expired items from tables automatically to help\nyou reduce storage usage and the cost of storing data that is no longer relevant.\nAdvantages of Amazon DynamoDB\n\n\n-\nThe Key-value store or key-value based database is a database that uses an\nassociative array(such as a map) where each key is associated with one and only\none value in a collection. This kind of relationship is referred to as a key-value\npair.\n-\nIn a Key-value pair, each key value is represented as an arbitrary string such as a\nhash value.\n-\nThe value is stored as a blob.\n-\nThe storage of value as BLOB removes the need to index the data to improve\nperformance so that we cannot control what’s returned from a request by value.\n-\nKey-value stores do not have any query language. They only allow to store,\nretrieve and update data using simple get, put and delete commands and the\ndata can be retrieved by making a direct request to the object in memory or on\ndisk.\nExample on Amazon DynamoDB\n\n\n-\nKey/value stores are used when we have to access the following data –\n-\nsession\n-\nshopping cart info\n-\nFor example – In a shopping mall, information regarding a particular product is\nstored on the basis of a particular key. So, when the product is scanned, on the\nbasis of barcode all the information for a particular product is accessed.\n-\nThis key/value database allows us to read and write values as follows\n-\nGet (key) returns the value associated with the provided key.\n-\nPut (key, value) associates the value with the key.\n-\nMulti-get (key1, key2, .., keyN) returns the list of values associated with the list\nof keys.\n-\nDelete(key), removes the entry for the key from the data store.\nUses of Key-Value Store\n\n\nExample on Key-value Store Database\n\n\n-\nFor Cassandra, Mongodb and Hbase –\n-https://logz.io/blog/nosql-database-comparison/\n-\nFor Neo4j –\n-\nhttps://rubygarage.org/blog/neo4j-database-guide-with-use-cases\n-\nFor Overview on NoSQL --\n-https://www.xenonstack.com/blog/nosql-databases/\n-Amazon DynamoDB\n-https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/In\ntroduction.html\nReferences for NoSQL Case Studies\n\n\nNoSQL systems to handle Big\ndata problems\n\n-\nWhat is Big Data NoSQL Solution ?\n-\nAnalysing Big Data with a Shared Nothing Architecture\n-\nUnderstanding Types of Big Data Problems\n-\nChoosing Distribution Models\n-\nFour ways that NoSQL System Handles Big Data Problems\nUsing NoSQL to manage Big Data\n\n-\nRecommendation\n-\nUser Profile Management\n-\nReal-Time Data Handling\n-\nContent Management\n-\nCatalog Management\n-\n360 Degree Customer view\nWhat is Big Data NoSQL Solution ?\n\n-\nMobile Applications\n-\nInternet of Things\n-\nFraud Detection\nWhat is Big Data NoSQL Solution ?\n\n-\nThe biggest challenge to NOSQL databases is scaling to size.\n-\nThere are two approaches to perform scaling i.e., Vertical Scaling and Horizontal\nScaling.\nAnalysing Big Data with a Shared Nothing Architecture\n\n\nVertical Scaling\n\n\n- Because\nof\nthe\nway\nRDBMS\nare\nstructured,\nrelational\ndatabases usually scale vertically – a single server has to host the\nentire database to ensure acceptable performance for cross-\ntable joins and transactions.\n- This gets expensive quickly, places limits on scale, and creates a\nrelatively\nsmall\nnumber\nof\nfailure\npoints\nfor\ndatabase\ninfrastructure.\n- The solution to support rapidly growing applications is to scale\nhorizontally, by adding servers instead of concentrating more\ncapacity in a single server.\nHorizontal Scaling\n\n\nScaling to size in today‟s world is scaling horizontally, that is adding new\nmachines. There are number of techniques to achieve this.\na. Master Slave replication\nb. Sharding\n-\nIn Master Slave Replication, one server is designated as master and all write\nhappens only to the server. Every update is propagated to all other servers.\n-\nFor example, if there are 1 master and 4 slaves machines, the write can happen\nonly on 1 master server where as there would 5 machines which responds to\nread requests.\nHorizontal Scaling (Master – Slave Replication)\n\n\n-\nThe problem with Master Slave system is, if there more write request than what\none machine can handle. This leads to next system called Sharding)\n-\nIn Sharding, completely isolated structures are put into one server according to\nalphabetical order. For example- figure shows the example of all customer\nnames starting from A-M are put on one server and N-Z are put on another\nserver. This certainly can handle twice the workload. Similarly there can be „n‟\nnumber of machines added to ease the work load.\nHorizontal Scaling (Sharding)\n\n\nSharding\n\n- Sharding is a type of database partitioning that separates very\nlarge databases the into smaller, faster, more easily managed parts\ncalled data shards. The word shard means a small part of a whole.\n-'Sharding' a database across many server instances can be achieved\nwith SQL databases, but usually is accomplished through SANs and\nother complex arrangements for making hardware act as a single\nserver.\n- Application code is developed to distribute the data, distribute queries, and aggregate the\nresults of data across all of the database instances. Additional code must be developed to\nhandle resource failures, to perform joins across the different databases, for data rebalancing,\nreplication, and other requirements.\n\n-\nSharding is the process of storing data records across multiple machines.\n-\nIt provides support to meeting the demands of data growth\nSharding\n\n\n-\nAs the size of data increases, a single machine may not be sufficient to store the data\nnor provide an acceptable read and write throughput.\n-\nSharding solves the problem with horizontal scaling.\n-\nWith sharding, we can add more machines to support data growth and the demands\nof read and write operations.\nSharding\n\n\nFour ways that NoSQL handle big data problems\n1. Moving queries to the data\n-\nWith the exception of large graph databases, most NoSQL systems use\ncommodity processors that each hold a subset of the data on their local\nshared-nothing drives.\n-When a client wants to send a general query to all nodes that hold data, it’s\nmore efficient to send the query to each node than it is to transfer large\ndatasets to a central processor.\n\n\nFour ways that NoSQL handle big data problems\n1. Moving queries to the data\n-This simple rule helps you understand how NoSQL databases can have\ndramatic performance advantages over systems that weren’t designed to\ndistribute queries to the data nodes.\n-Keeping all the data within each data node in the form of logical documents\nmeans that only the query itself and the final result need to be moved over a\nnetwork. This keeps your big data queries fast.\n\n\nFour ways that NoSQL handle big data problems\n2. Using hash rings to evenly distribute data on a cluster\n-\nOne of the most challenging problems with distributed databases is figuring\nout a consistent way of assigning a document to a processing node.\n-\nUsing a hash ring technique to evenly distribute big data loads over many\nservers with a randomly generated 40-character key is a good way to evenly\ndistribute a network load.\n-\nHash rings are common in big data solutions because they consistently\ndetermine how to assign a piece of data to a specific processor.\n\n\nFour ways that NoSQL handle big data problems\n2. Using hash rings to evenly distribute data on a cluster\n-\nHash rings take the leading bits of a document’s hash value and use this to\ndetermine which node the document should be assigned.\n-\nThis allows any node in a cluster to know what node the data lives on and how\nto adapt to new assignment methods as your data grows.\n-\nPartitioning keys into ranges and assigning different key ranges to specific\nnodes is known as keyspace management.\n\n\nFour ways that NoSQL handle big data problems\n3. Using replication to scale reads\n-\nDatabases use replication to make backup copies of data in real time. Using replication allows\nyou to horizontally scale read requests. Figure shows this structure.\n\n\nFour ways that NoSQL handle big data problems\n4. Letting the database distribute queries evenly to handle nodes\n-It takes a single query and distributes it to distinct servers and then it\ncombine the result together to give the user the impression they are\nsearching a single system\n-Here servers are in different geographic location.\n\n\nThank You"
    },
    {
      "filename": "BE_COMP_BDA_Week 1_L1-L4.pdf",
      "path": "data/materials\\Big Data Analytics\\BE_COMP_BDA_Week 1_L1-L4.pdf",
      "text": "Big Data Analytics\nUnit 1: Introduction to Big Data and\nHadoop\nFaculty Name :\nDr.. Tushar G\nDr. Poonam Gadge\nIndex\n\n\n\n\n\n\n\n\n\nModule :1 Introduction to Big Data\nand Hadoop\nIntroduction to Big Data\n\nIntroduction to Big Data\n\n\nWhat is Big\nData?\nHow much\ndata does it\ntake to be\ncalled Big\nData?\nWhere does\nbig data\ncome from?\nWhere is the\nbig data\ntrend going?\nWho are\nsome of the\nBIG DATA\nusers?\nHow will big\ndata impact?\nIntroduction to Big Data\n-\nWhat is Data?\n//Discussion with students\n-\nWhat is Big Data?\n\n\nWhat is\nBig Data?\n“Data that is so large, fast or complex that it’s difficult\nto process using traditional methods.”\nIntroduction to Big Data\n\n\nQualitative Data\nQualitative data is descriptive data.\nIt is non-numerical and is also known as categorical data.\nFor example, imagine someone describing a cake in a taste\ntest:\nThe cake is red in colour\nIt has white buttercream inside it\nIt is decorated with sweets and sprinkles\nThese are all qualitative descriptions.\nQualitative data is important as it allows people to decide on\nhow best to record quantitative data.\nIntroduction to Big Data\nModule :1 Introduction to Big Data\nand Hadoop\n\nCollecting qualitative data\nQualitative data can be collected through:\nSurveys\nPolls\nOne-to-One Interviews\nFocus Groups\nObservation\nCollecting qualitative data is helpful for in-depth analysis and for\nunderstanding audiences and customers, but it is also time-consuming and\nrequires a skilled researcher.\nIntroduction to Big Data\nModule :1 Introduction to Big Data\nand Hadoop\n\nQuantitative Data\nQuantitative data is numerical information,\nand answers questions like 'how many', 'how\nmuch' and 'how often'.\nAny information or answer expressed\nthrough numbers is quantitative.\nMary has 3 children\nThey are between 4 and 12 years old.\nQuantitative Data\nIntroduction to Big Data\nModule :1 Introduction to Big Data\nand Hadoop\n\nIntroduction to Big Data\n\n\nHow much\ndata does it\ntake to be\ncalled Big\nData?\nIntroduction to Big Data\n\n\nExample data of different data sizes:\nIntroduction to Big Data\n\n\nWhere does\nbig data\ncome from?\nSocial media and networks\nScientific instruments\nMobile devices\nSensor technology and networks\nBank /Credit Card\nTransactions\nWeb Data, E-Commerce\nOn an average, 500 million tweet per day.\nOne million customer transactions per hour\nSells 600 items per second\nCommercial airlines make about 5,800\nflights per day.\n74 billion transactions per year\nFacts and Figures\n\n\nAnalysts predict that by 2020, there will be 5,200 gigabytes of data on every\nperson in the world.\nThe New York Stock Exchange generates about one\nterabyte of new trade data per day.\nIntroduction to Big Data\n\nWho are\nsome of the\nBIG DATA\nusers?\n\nStatistic shows that 500+terabytes of new data gets\ningested\ninto\nthe\ndatabases\nof\nsocial\nmedia\nsite Facebook, every day. This data is mainly generated\nin\nterms\nof\nphoto\nand\nvideo\nuploads,\nmessage\nexchanges, putting comments etc.\nIntroduction to Big Data\n\n\nWho are\nsome of the\nBIG DATA\nusers?\nSingle Jet engine can generate 10+terabytes of data\nin 30 minutes of a flight time. With many thousand flights\nper\nday,\ngeneration\nof\ndata\nreaches\nup\nto\nmany Petabytes..\nIntroduction to Big Data\n\n\nWho are\nsome of the\nBIG DATA\nusers?\nWhy is Big Data Important?\nThe importance of big data doesn’t revolve around how\nmuch data you have, but what you do with it. You can take\ndata from any sources and analyze it to find answers that\nenable\nIntroduction to Big Data\n\n\nHow will big\ndata impact?\nIntroduction to Big Data\n\n\nLet’s Summerize today’s learnings:\nLet’s have some fun activity !!!\n\n\n\nBig Data Characteristics\nand Types of Big Data\nBig data' could be found in three forms:\nCategories of Big Data\n\n\nUnstructured\nStructured\nSemi-Structured\nStructured Data\n-\nAny data that can be stored, accessed and processed in the form of\nfixed/well-defined format is termed as a 'structured' data.\n-\nOver the period of time, talent in computer science have achieved greater\nsuccess in developing techniques for working with such kind of data (where the\nformat is well known in advance) and also deriving value out of it.\n-\nHowever, now days, we are foreseeing issues when size of such data\ngrows to a huge extent, typical sizes are being in the rage of multiple zettabyte.\nCategories of Big Data\n\n\nStructured Data\nCategories of Big Data\n\nFixed Schema\n\nExample of Structured Data\nAn 'Employee' table in a database is an example of Structured Data\nCategories of Big Data\n\nEvery Employee record is following the same structure.\n\nUnstructured\n-\nAny data with unknown form or the structure is classified as unstructured\ndata. In addition to the size being huge, un-structured data poses multiple\nchallenges in terms of its processing for deriving value out of it.\n-\nTypical example of unstructured data is, a heterogeneous data source\ncontaining a combination of simple text files, images, videos etc.\n-\nNow a day organizations have wealth of data available with them but\nunfortunately they don't know how to derive value out of it since this data is in\nits raw form or unstructured format.\nCategories of Big Data\n\n\nUnstructured Data\nCategories of Big Data\n\n\nExample of Unstructured Data\nOutput returned by 'Google Search'\nCategories of Big Data\n\n\nSemi-structured\n-\nSemi-structured data can contain both the forms of data.\n-\nWe can see semi-structured data as a structured in form but it is actually\nnot defined with e.g. a table definition in relational DBMS.\n-\nExample of semi-structured data is a data represented in XML file.\nCategories of Big Data\n\n\nSemi-structured\nCategories of Big Data\n\n\nExample of Semi-structured Data\nPersonal data stored in a XML file-\nCategories of Big Data\n\n\nPlease note that web application data, which is unstructured, consists of log\nfiles, transaction history files,text,images etc. OLTP systems are built to work\nwith structured data wherein data is stored in relations (tables).\nData Growth over years\n\n\nCharacterization of Big Data\n\nBig Data is characterized by 5Vs\n\nCharacterization of Big Data: Volume\n\nVolume:\n-\nThe\nname\n'Big\nData'\nitself\nis\nrelated\nto\na\nsize\nwhich\nis\nenormous.\n-\nSize of data plays very crucial\nrole in determining value out of\ndata. Also, whether a particular\ndata can actually be considered\nas a Big Data or not, is dependent\nupon volume of data.\nE.g.\n-\nFacebook alone can generate about\nbillion\nmessages\nand\nover\n\nmillion new posts are uploaded each\nday\n\n-\nA typical PC might have had 10 gigabytes of storage in 2000.\n-\nToday, Facebook ingests 500 terabytes of new data every day.\n-\nBoeing 737 will generate 240 terabytes of flight data during a single flight\nacross the US.\n-\nThe smart phones, the data they create and consume; sensors embedded\ninto everyday objects will soon result in billions of new, constantly-updated\ndata feeds containing environmental, location, and other information,\nincluding video.\nCharacterization of Big Data: Volume….\n\n\nCharacterization of Big Data: Velocity\n\nVelocity\n-\nRefers to the speed of generation of\ndata. How fast the data is generated\nand processed to meet the demands,\ndetermines real potential in the data.\n-\nIt deals with the speed at which data\nflows in from sources like business\nprocesses, application logs, networks\nand\nsocial\nmedia\nsites,\nsensors, Mobile devices, etc. The\nflow\nof\ndata\nis\nmassive\nand\ncontinuous.\nE.g.\n-\nReal\nTime\nPersonalization\nwith\nstreaming analytics\n\n-\nClickstreams and ad impressions capture user behavior at millions of events\nper second\n-\nHigh-frequency stock trading algorithms reflect market changes within\nmicroseconds\n-\nMachine to Machine processes exchange data between billions of devices\n-\nInfrastructure and sensors generate massive log data in real-time\n-\nOn-line gaming systems support millions of concurrent users, each\nproducing multiple inputs per second.\nCharacterization of Big Data: Velocity….\n\n\nCharacterization of Big Data: Variety\n\nVariety\n-\nVariety refers to heterogeneous sources\nand the nature of data, both structured\nand unstructured.\n-\nDuring\nearlier\ndays,\nspreadsheets\nand\ndatabases were the only sources of data\nconsidered by most of the applications.\n-\nNow days, data in the form of emails,\nphotos, videos, PDFs, audio, etc. is also\nbeing\nconsidered\nin\nthe\nanalysis\napplications. This variety of unstructured\ndata\nposes\ncertain\nissues\nfor\nstorage,\nmining and analysing data.\n\n-\nRelational Data (Tables/Transaction/Legacy Data)\n-\nText Data (Web)\n-\nSemi-structured Data (XML)\n-\nGraph Data\nSocial Network, Semantic Web (RDF), …\n-\nStreaming Data\nYou can only scan the data once\n-\nA single application can be generating/collecting many types of data\n-\nBig Public Data (online, weather, finance, etc)\nCharacterization of Big Data: Variety….\n\n\nCharacterization of Big Data: Veracity\n\nVeracity: Trustworthiness of Data\n-\nData involves some uncertainty and\nambiguities\n-\nMistakes\ncan\nbe\nintroduced\nby\nhumans and machines\nPeople sharing accounts\nLike\nit\ntoday,\ndislike\nit\ntomorrorw\nWrong system timestamps\n-\nData Quality is vital!\n-\nAnalytics and conclusions rely on\ngood data quality\nGarbage data + perfect model =>\ngarbage results\nPerfect data + garbage model =>\ngarbage results\n.\n\nCharacterization of Big Data: Volume, Velocity, Variety, Veracity\n\n\nCharacterization of Big Data: Value\n\nValue\n-Raw data of Big Data is of low value\nFor example, single observations\n-Analytics and theory about the data\nincreases the value\n-Analytics transform big data into smart\ndata!\nThe bulk of Data having no Value is of no good, unless you turn it into\nsomething useful.\n\nLet’s have a fun activity !\n\n\nPuzzle\nModule :1 Introduction to Big Data\nand Hadoop\n\n\nTraditional vs Big Data\nbusiness approach\nTraditional Approach Vs Big Data Approach\n\n\n-\nWe are going to discuss about ……….\nTraditional Data Vs Big Data\nTraditional Transactions Vs Big Data Transactions\nTraditional Analytics Vs Big Data Analytics\nTraditional Data and Big Data\n\n\nTraditional Transaction Vs Big Data (NON) Transactions\n\n-\nToday relational DBMS underpin the vast majority of world’s transaction\nsystems supporting on-line transaction processing (OLTP).\n-\nTraditional on-line transaction processing system usually support ACID\nproperties. Traditional ACID properties are typically associated with\nstructured data.\nExample of transaction data\nOrders, Shipments,\nPayments, Returns, Refunds,\nPurchases, Financial adjustments\n\nTraditional Transaction Vs Big Data (NON) Transactions\n-\nSince the emergence of web in 1990s there has been explosive growth in\nthe rate (or velocity) that data is generated. Data velocity has increased\nhas increased both in terms of traditional on-line transaction rates and\nalso in terms of so-called ‘non-transaction data.\n\nExample of non transaction data: Shopping Cart Data\nDeal with large number on concurrent users\nand be highly available for 24*365 nature of\nbusinesses on the web.\nRapidly captures changes to shopping carts\nand serve up-to-date shopping cart data to\nall users many times before a transaction occurs.\n\nTraditional Analytics Vs Big Data Analytics\n\n\nTraditional Data warehouse Analytics vs. Big Data Analytics\n\nTraditional Data warehouse Analytics\nBig Data Analytics\nAnalyzes known data.\nData\nloaded\ninside\na\ndata\nwarehouse\nis\nwell\nunderstood, cleansed and in line with the business\nmetadata\nTargeted at unstructured data.\nData is not well formed and cleaned .\nThus, more challenging but at the same time it gives\na scope for much more insight into the data.\nBuilt\non\ntop\nof\nthe\nrelational\ndata\nmodel.\nRelationships between the subjects of interests have\nbeen created inside the system and the analysis is\ndone based on them.\nDifficult to establish relationship between all the\ninformation in a formal way,\nMost of the big data analytics databases are based\nout Columnar databases.\nBatch oriented and we need to wait for nightly ETL\nand transformation jobs to complete before the\nrequired insight is obtained.\nAimed at near real time analysis of the data using\nthe support of the software meant for it.\n\nTraditional Data warehouse Analytics vs. Big Data Analytics\n\n\nLet’s revise through a video !!\n\nNptel Video Link: Introduction to big data:\nhttps://nptel.ac.in/courses/106/104/106104189/\n\n\nCase Study of Big Data\nSolutions\nApplication of Big Data\n\n\nWe’re Living in a Real Time World…\nHomeland Security\nReal Time Search\nSocial\neCommerce\nUser Tracking &\nEngagement\nFinancial Services\n\n\nThe Flavors of Big Data Analytics\nCounting\nCorrelating\nResearch\n\n\nAnalytics @ Twitter – Counting\n-\nHow many signups, tweets,\nretweets for a topic?\n-\nWhat’s the average latency?\n-\nDemographics\nCountries and cities\nGender\nAge groups\nDevice types\n\n\nAnalytics @ Twitter – Correlating\n-\nWhat devices fail at the same\ntime?\n-\nWhat features get user\nhooked?\n-\nWhat places on the globe are\n“happening”?\n\n\nAnalytics @ Twitter – Research\n-\nSentiment analysis\n“Obama is popular”\n-\nTrends\n“People like to tweet\nafter watching\nAmerican Idol”\n-\nSpam patterns\nHow can you tell when\na user spams?\n\n\nIt’s All about Timing\n“Real time”\n(< few Seconds)\nReasonably Quick\n(seconds - minutes)\nBatch\n(hours/days)\n\n\nIt’s All about Timing\n-\nEvent driven / stream processing\n-\nHigh resolution – every tweet gets counted\n-\nAd-hoc querying\n-\nMedium resolution (aggregations)\n-\nLong running batch jobs (ETL, map/reduce)\n-\nLow resolution (trends & patterns)\n\n\nThis is what we’re\nhere to discuss \nThe 5 Key Big Data Use Cases\n\n\nBig Data Exploration\nFind, visualize,\nunderstand all big data to\nimprove decision making\nEnhanced 360o View\nof the Customer\nExtend existing customer views\n(MDM, CRM, etc) by incorporating\nadditional internal and external\ninformation sources\nSecurity/Intelligence\nExtension\nLower risk, detect fraud\nand monitor cyber\nsecurity in real-time\nData Warehouse Augmentation\nIntegrate big data and data\nwarehouse capabilities to increase\noperational efficiency\nOperations Analysis\nAnalyze a variety of machine\ndata for improved business\nresults\nBig Data Exploration: Customer Example\n\n-Exploring 4 TB to drive point business solutions\n(supplier portal, call center, etc.)\n-Single-point of data fusion for all employees to use\n-Reduced costs & improved operational performance for the business\n-Can you navigate and explore all\nenterprise and external content in a\nsingle user interface?\n-Can you quickly identify areas of\ndata risk?\n-Do you have a logical starting point\nfor your\nbig data initiatives?\nKey Questions to Ask\n-Can you separate the “noise” from\nuseful content?\n-Can you perform data exploration on\nlarge and complex data?\n-Can you find insights in new or\nunstructured data types (e.g. social\nmedia and email)?\nAirline Manufacturer\n\nEnhanced 360º Customer View: Customer Example\n\n-Create “Facebook”\n-Identify 200+ different customer profiles to help in fulfillment & marketing efforts\n-Leverage new data types in customer analysis\nHow are you driving consistency\nacross your information assets\nwhen representing your customer,\nclients, partners etc.?\nHow can a complete view of the\ncustomer enhance your line of\nbusiness users and result in better\nbusiness outcomes?\nKey Questions to Ask\n-Can you identify and deliver all data as it\nrelates to a customer, product,\ncompetitor to those to need it?\n-Can you gathering insights about your\ncustomers from social data, surveys,\nsupport emails, etc.?\n-Can you combine your structured and\nunstructured data to run analytics?\n\n\nAsian Government\nAgency\nCapabilities:\nStream Computing\n- Analyze all Internet traffic (social media, email, etc)\n- Track persons of interest (drug/sex traffickers,\nterrorists, illegal refugees/immigrants) and civil/border\nactivity\nNational Intelligence Platform\nSecurity/Intelligence Extension: Needs\n\n\nOperations Analysis: Needs\nBenefits:\n-Gain real-time visibility into operations, customer experience, transactions and behavior\n-Proactively plan to increase operational efficiency\n-Identify and investigate anomalies\n-Monitor end-to-end infrastructure to proactively avoid service degradation or outages\nAnalyze a variety of machine data for improved\nbusiness results\nBusiness Challenges:\n-Complexity and rapid growth of machine data\n-Difficult to capture small fraction of machine for better decision\n-In-ability to analyze machine data and combine it with enterprise data for a full view\nanalysis\n\n\nOPERATIONAL - ANALYSIS\n-Capabilities:\nHadoop & Stream Computing\n-Intelligent Infrastructure Management: log analytics, energy bill\nforecasting, energy consumption optimization, anomalous\nenergy usage detection, presence-aware energy management\n-Optimized building energy consumption with centralized\nmonitoring; Automated preventive and corrective maintenance\n\nOperations Analysis: Needs\n\nIntegrate big data and data warehouse capabilities to\nincrease operational efficiency\nData Warehouse Augmentation: Needs\nNeed to leverage variety of data\nExtend warehouse infrastructure\n-Optimized storage, maintenance and\nlicensing costs by migrating rarely\nused data to Hadoop\n-Reduced storage costs through smart\nprocessing of streaming data\n-Improved warehouse performance by\ndetermining what data to feed into it\n-Structured, unstructured, and\nstreaming data sources required for\ndeep analysis\n-Low latency requirements\n(hours—not weeks or months)\n-Required query access to data\n\n\nData Warehouse Augmentation: Customer Example\n-\nAre you drowning in very large data sets\n(TBs to PBs) that are difficult and costly to\nstore?\n-\nAre you able to utilize and store new data\ntypes?\n-\nAre you facing rising maintenance/licensing\ncosts?\n-\nDo you use your warehouse environment\nas a repository for all data?\n-Creates pre-processing hub and performs ad hoc analysis\n-Hadoop-based landing zone used to store, manage and analyze structured,\nsemi-structured and multi-structured data before moving to the warehouse\n-Benefits: Data warehouse optimized for workload and performance\n-Utilized InfoSphere BigInsights, InfoSphere DataStage\n-Do you have a lot of cold, or low-touch, data\ndriving up costs or slowing performance?\n-Do you want to perform analysis of data in-\nmotion to determine what should be stored in\nthe warehouse?\n-Do you want to perform data exploration on\nall data?\n-Are you using your data for new types of\nanalytics?\n\n\n-\nBig Data Fundamentals: Concepts, Drivers & Techniques (The Prentice Hall Service\nTechnology Series from Thomas Erl)\nhttp://lab.ilkom.unila.ac.id/ebook/ebooks%20Big%20Data/2016%20Big%20Data%20\nFundamentals%20-%20Thomas%20Erl.pdf\n-\nComparision between types of big data:\nhttps://www.tutorialspoint.com/difference-between-structured-semi-structured-and-\nunstructured-data\nReferences\n\n\nThank You"
    },
    {
      "filename": "BE_COMP_BDA_Week 2_L5-L8.pdf",
      "path": "data/materials\\Big Data Analytics\\BE_COMP_BDA_Week 2_L5-L8.pdf",
      "text": "Big Data Analytics\nUnit 1: Introduction to Big Data and\nHadoop\nFaculty Name :\nMr. Tushar G\nMrs Poonam Gadge\nIndex - Module :1 Introduction to Big Data and Hadoop\n\n\n\n\n\nWeek 2: Lecture 5\nWhat is Hadoop?\nWhat is Hadoop??\n\n\n-Open source framework of tools designed for storage and processing of\nlarge scale data (BigData)\n-Hadoop is maintained by Apache\n-Created by Doug Cutting and Mike Carafella in 2005.\n-Cutting named the program after his son’s toy elephant.\nWhat is Hadoop?\n\n\nHadoop\nFramework of Tools\nis\nHadoop’s Developers\nDoug Cutting\n2005: Doug Cutting and Michael J. Cafarella developed\nHadoop to support distribution for the Nutch search engine\nproject.\nThe project was funded by Yahoo.\n2006: Yahoo gave the project to Apache\nSoftware Foundation.\n\n\nMichael J. Cafarella\nWho Uses Hadoop?\n\n\nand Hadoop\n-What were the limitations of earlier large-scale computing?\n-What requirements should an alternative approach have?\n-How does Hadoop address those requirements?\nMotivations for Hadoop\n\n\nand Hadoop\n-Historically computation was processor-bound\no Data volume has been relatively small\no Complicated computations are performed on that data\n-Advances in computer technology has historically centered around improving\nthe power of a single machine\nEarly Large Scale Computing\n\n\n-\nSingle-core computing can’t scale with current computing needs\n-\nPower consumption limits the speed increase we get from transistor density\nSingle-Core Limitation\n\n\n-\nAllows developers to use multiple\nmachines for a single task\n-\nProblems:\no\nProgramming on a distributed\nsystem is much more complex\no\nSynchronizing data exchanges\no\nManaging a finite bandwidth\no\nControlling computation timing is\ncomplicated\nDistributed Systems\n\n\n-\nTypically divided into Data Nodes and Compute Nodes\n-\nAt compute time, data is copied to the Compute Nodes\n-\nFine for relatively small amounts of data\n-\nModern systems deal with far more data than was gathering in the past\nDistributed System: Data Storage\n\n\nand Hadoop\nTraditional Approach\n\nand Hadoop\n\nBig Data\nPowerful\nComputer\nProcessed BY\nBig Data\nProcessing\nLimit\nOnly so much data\ncould be processed\nHadoop Approach\n\nand Hadoop\n\nBig Data\nbroken into pieces\nHadoop Approach\n\nand Hadoop\n\nComputation\nComputation\nComputation\nComputation\nCombined\nResults\nMove computation to the data\nBig Data\n-\nApplications are written in a high-level programming language\no\nNo network programming or temporal dependency\n-\nNodes should communicate as little as possible\no\nA “shared nothing” architecture\n-\nData is spread among the machines in advance\no\nPerform computation where the data is already stored as often as\npossible\nCore Hadoop Concepts\n\n\nand Hadoop\nHadoop Assumptions\n\nand Hadoop\n\nHadoop was developed with large clusters of computers in mind with the following\nassumptions\n\nHardware will fail, Since it considers a large cluster of computers.\n\nProcessing will be run in batches, so aim at high throughput as opposed to\nlow latency.\n\nApplications that run on Hadoop Distributed File System(HDFS) have large\ndata sets typically from gigabyte to terabytes in size.\n\nPortability is important.\n\nAvailability of high-aggregate data bandwidth and scale to hundreds of\nnodes in a single cluster.\n\nShould support tens of millions of files in a single instance.\n\nApplications need a write-once-read-many access model.\n-\nHadoop works on distributed model.\n-\nAll these are set of low cost computers.\n-\nHadoop works on Linux based machines\nHadoop Architecture\n\nand Hadoop\n\n-\nEach machine consists of two components: Task Tracker and Data Node\n-\nTask Tracker job is to process smaller piece of task given to this particular node\nand Data node component is to manage data that is received on this particular\nnode.\nHadoop Architecture\n\nand Hadoop\n\nSlaves\nHadoop Architecture\n\nand Hadoop\n\nMaster\nSlave\nHadoop Architecture\n\nand Hadoop\n\nMapReduce\nHDFS\nHadoop Architecture\n\nand Hadoop\n\nApplication\nQueue\n-\nJob Tracker\nRole of job tracker is to break the higher bigger task into smaller pieces and\nforward it to the task tracker. Task tracker in turn will perform the computations\nand results are forwarded back to the job tracker. The final result is combined\nby job tracker.\n-\nName Node\nName node running on the master node is responsible to keep an index of\nwhich data resides on which data node. So name node tells application to go to\nthis particular node where data is residing. So application don’t have to depend\non Name node.\nHadoop Architecture\n\nand Hadoop\n\nHadoop Features : Fault Tolerance\n\nand Hadoop\n\nFault Tolerance for data\n- Hardware failures are bound to happen. They\nwill happen.\n- Hadoop has built-in hardware failure\nmanagement.\n- Hadoop maintains 3 copies of each file, so this\nway failure management is carried out with data\nreplication.\nHadoop Features : Data Replication\n\nand Hadoop\n\nDefault Replication is 3-fold\nHadoop Features : Scalable\n\nand Hadoop\n\nScalable\n- Hadoop is higly scalable.\n- It is designed to scale up from single servers to\nthousands of machines, each offering local\ncomputation and storage\nScalable\n1000s\nPhysical Architecture (Hadoop-compatible file system provides\nlocation awareness)\n\nand Hadoop\n\n\nData and Hadoop\n\nPhysical Architecture (Hadoop-compatible file system provides\nlocation awareness)\n\nand Hadoop\n\nPhysical Architecture (Hadoop-compatible file system provides\nlocation awareness)\n\nNever loose all data if entire rack fails\n\nKeep bulky flows in-rack possible\n\nAssumption that in-rack is higher bandwidth and lower latency\nThere is also an assumption that two machines in the same rack have more\nbandwidth and lower latency between each other than two machines in two\ndifferent racks. This is true most of the time.\nEasy Programming\n\nand Hadoop\n\nProgrammers could focus on writing\nscale free programs\nHadoop: Why, Where and Who?\n\nand Hadoop\n\nhttps://www.coursera.org/learn/big-data-introduction/lecture/aHXfE/hadoop-\nwhy-where-and-who\nWeek 2: Lecture 6\nHadoop Components;\nHadoop Ecosystem\nHadoop Core Components\n\nand Hadoop\n\n- Contains Libraries and other modules\nHadoop Common\n- Hadoop Distributed File System\nHDFS\n- Yet Another Resource Negotiator\nHadoop YARN\n- A programming model for large scale data\nprocessing\nHadoop MapReduce\nHadoop Components\n\nand Hadoop\n\n- Hadoop has two main components:\nMapReduce and HDFS File system\n- Hadoop has set of tools and those\nset of tools are represented in\nHadoop Ecosystem\n-\nHDFS is a file system written in Java\n-\nResponsible for storing data on the cluster\n-\nData files are split into blocks and\ndistributed across the nodes in the cluster\n-\nEach block is replicated multiple times\nHadoop Distributed File System(HDFS)\n\nand Hadoop\n\nHDFS Architecture\n\nand Hadoop\n\n-Main Components of HDFS\n- Name Node : keeps track of\nwhich blocks make up a file and\nwhere they are stored\n- Data\nNodes:\nslaves\nwhich\nprovide the actual storage and\nare deployed on each machine.\nHDFS Architecture\n\nand Hadoop\n\n-\nMassive Parallel Processing Technique for\nprocessing data which is distributed on a\ncommodity cluster.\n-\nHow this is achieved?\nA method for distributing computation\nacross multiple nodes\nEasy to Parallelize\nEach node processes the data that is\nstored at that node\nConsists of two main phases\nMap\nReduce\nMapReduce\n\nand Hadoop\n\n-\nReads data as key/value pairs\n-\nOutputs zero or more key/value pairs\nThe Mapper\n\nand Hadoop\n\nShuffling\n\nand Hadoop\n\nThe intermediate result is sorted and redistributed.\nShuffled and sorted data is processed as per key in parallel.\nReducing\n\nand Hadoop\n\nOverall MapReduce word count process\n\nand Hadoop\n\nHadoop Ecosystem\n\nData and Hadoop\n\nHadoop Ecosystem\n\nand Hadoop\n\nHadoop Ecosystem (Flume)\n\nand Hadoop\n\nHadoop Ecosystem (Hive)\n\nand Hadoop\n\nHadoop Ecosystem (HBase)\n\nand Hadoop\n\nHadoop Ecosystem (Mahout)\n\nand Hadoop\n\nHadoop Ecosystem (Pig)\n\nand Hadoop\n\nHadoop Ecosystem (Sqoop)\n\nData and Hadoop\n\n- Scalable\nBreaks data into smaller equal pieces (blocks, typically64/128 Mb)\nBreaks big computation task down into smaller individual tasks\nMore slaves, more processing and storage power\n- Cheap\nCommodity hardware, open source software\n- Extremely fault tolerant\n- “Easy” to use\nWhy Hadoop??????\n\nand Hadoop\n\nSubject Name: Big Data Analytics\nUnit No: 2\nUnit Name: Hadoop HDFS and MapReduce\nFaculty Name :\nMr. Tushar G\nMrs. Poonam Gadge\nIndex : Module 2 : Hadoop HDFS and MapReduce\n\n\nNodes,\n\n\n\nDistributed File System,\nPhysical Organisations of\nComputer nodes\n\nDistributed File System\n-\nA Distributed File System ; DFS; is any file system that allows access to file from\nmultiple hosts sharing via a Computer Network.\n-\nMay include facilities for transparent replication and fault tolerance.\n\nComputer nodes\n\nDifferent types of DFS:\nGoogle File System\nHadoop Distributed File System\nWhat is DFS?\n\nIn DFS data is stored in a cluster. A cluster is a bunch of computers connected to each other\nforming a network.\nWhy DFS?\n\nExample in Scenario 1 : 4GB file takes 4 hours to completely process it.\nExample in Scenario 2: Data is distributed across 4 nodes and file is processed\nsimultaneously. This will reduce time to 1/4th of what was required in scenario 1.\nGoogle File System\n-\nGoogle File System (GFS) is a scalable Distributed File System (DFS) created by\nGoogle Inc. and developed to accommodate Google’s expanding data processing\nrequirements.\n-\nGFS provides\nfault tolerance\n\nreliability\n\nscalability\n\nperformance to large networks and connected nodes.\n\n\nComputer nodes\nGoogle File System Components\nCluster\nGoogle\norganized\nthe\nGFS\ninto\nclusters of computers. A cluster is\nsimply a network of computers. Each\ncluster might contain hundreds or\neven thousands of machines.\nWithin the cluster there are 3\nentities:\nClient\nMaster Server\nChunk Server\n\nof Computer nodes\n\nGoogle File System Components\nClient\n-\nIn the world of GFS, the term “client”\nrefers to any entity that makes a file\nrequest.\n-\nRequest can range from retrieving and\nmanipulating existing files to creating new\nfiles on the system. Clients can be other\ncomputers or computer applications. You\ncan think client as customer of GFS.\n\nof Computer nodes\n\nGoogle File System Components (Chunk Server)\nChunk Server\n-\nChunkservers are the workhorses of the\nGFS..\n-\nThe chunkservers don’t send chunks to the\nmaster\nservers.\nInstead,\nthey\nsend\nrequested chunks directly to the client.\n\n- The GFS copies every chunk multiple times and stores it on different\nchunk servers. Each copy is called replica.\n- By default, the GFS makes three replicas per chunk, but users can\nchange the setting and make more fewer replicas if required.\n\nComputer nodes\nGoogle File System Components (Chunks)\n\nFiles split into chunks\n-\nEach chunk is of 64 MB\n-\nIdentified by 64 bit id\n-\nStored in chunk servers\n\nComputer nodes\nChunks (Replicas)\n\nFiles split into chunks\n- Replica count by client\n(Default is 3 but is configurable\nby client)\n-\nCommodity server failures\n\nComputer nodes\nGoogle File System Components : Master\n\nGFS Master\n-\nThe master server acts as the\ncoordinator for the cluster.\n-\nThe master’s duties include\nmaintaining an operation log,\nwhich\nkeeps\ntrack\nof\nthe\nmaster’s cluster.\n-\nMaster maintains file names\n+ chunk ids + locations\n-\nAlso, Access control details\nas in which client is allowed\nto access which chunk.\n\nComputer nodes\n\nGoogle File System Design\n\nComputer nodes\n\nGoogle File System Design\n\nComputer nodes\nHadoop Distributed File System\n-\nHDFS is very similar to GFS.\n-\nHere,\n-\nSince HDFS is an open-source framework of tools.\n\nGFS\nHDFS\nMaster\nNameNode\nShadow Master\nSecondary NameNode\nChunks\nBlocks\nChunkServer\nDataNode\n\nComputer nodes\nLarge Scale File\nOrganisations\n\nWhat is HDFS????\n\n\nHadoop High-Level Architecture\n\nNameNode:\nCoordinates\nand\nmonitors\nthe\ndata\nstorage\nfunction(HDFS),\nwhile\nJob\nTracker\nCoordinates\nthe\nparallel\nprocessing\nof\ndata\nusing\nMapReduce.\nSlaveNode:\ndoes\nthe\nactual work of storing the\ndata\nand\nrunning\ncomputations.\n\nHadoop High-Level Architecture\n\nMaster\nnode\ngives\ninstructions to their slave\nnode.\nThe Data Node is a slave to\nthe NameNode\nThe TaskTracker is a slave\nto the Job Tracker\n\n-\nHadoop works on distributed model.\n-\nAll these are set of low cost computers.\n-\nHadoop works on Linux based machines\nHadoop Architecture (Revision)\n\n\n-\nEach machine consists of two components: Task Tracker and Data Node\n-\nTask Tracker job is to process smaller piece of task given to this particular node\nand Data node component is to manage data that is received on this particular\nnode.\nHadoop Architecture (Revision)\n\nSlaves\n\nHadoop Architecture (Revision)\n\nMaster\nSlave\n\nHadoop Architecture (Revision)\n\nMapReduce\nHDFS\n\nHadoop Architecture (Revision)\n\nApplication\nQueue\n\nHow Hadoop works? (Example)\n\n4 TB\n1 TB\n1 TB\n1 TB\n1 TB\n- The Hadoop framework, divides the\ndata into smaller chunks and stores each\npart of the data on a separate node\nwithin the cluster.\n-\nLet us say we have around 4\nterabytes of data and a 4 node Hadoop\ncluster.\n-\nThe HDFS would divide this data into 4\nparts of 1 terabyte each. By doing this,\nthe time taken to store this data onto the\ndisk is significantly reduced.\nNode1\nNode2\nNode3\nNode 4\n\nHow Hadoop works? (Example)\n\n\n4 TB\n1 TB\n1 TB\n1 TB\n1 TB\n- In order to provide high availability what\nHadoop does is, it would replicate each\npart of the data onto other machines that\nare present within the cluster. By default\nthe replication factor is set to 3.\n- In order to reduce the bandwidth and\nlatency time, it would store 2 copies of\nthe same part of the data, on the nodes\nthat are present within the same rack,\nand the last copy would be stored on a\nnode, that is present on a different rack.\nNode1\nNode2\nNode3\nNode 4\nRack 1\nRack 2\nHow Files are stored in HDFS?\n\n-\nEach file is stored on HDFS as blocks.\n-\nThe default size of each block is 128 MB in Apache Hadoop 2.x (64 MB in Apache Hadoop\n1.x)\n\nHadoop Data Block\nAll blocks of the file are the same size except the last block, which can be either the\nsame size or smaller. The files are split into 128 MB blocks and then stored into the\nHadoop file system.\n\n\nWhy is HDFS Data Block Size 128MB in Hadoop?\n-\nHDFS have huge data sets, i.e. terabytes and petabytes of data.\n-\nSo like LINUX file system which have 4 KB block size, if we had data block\nsize 4KB for HDFS, then we would be having too many data blocks in\nHadoop HDFS and therefore too much of metadata.\n-\nSo, managing this huge number of blocks and metadata will create huge\noverhead and traffic which is something which we don’t want.\n-\nOn the other hand, data block size can’t be so large that the system is\nwaiting a very long time for one last unit of data processing to finish its\nwork.\n\n\nAdvantages of Hadoop Data Blocks\n-\nSimplicity of storage management: As the size of data blocks is fixed,\nso it is very easy to calculate the number of data blocks that can be stored\non the disk.\n-\nAbility to store very large files: HDFS can store very large files which\ncan be even larger than the size of a single disk as the file is broken into\nhdfs blocks and distributed across various nodes.\n-\nFault tolerance and High Availability of HDFS: Blocks are easy to\nreplicate between the datanodes and thus provide fault tolerance and high\navailability of HDFS.\n\n\nAdvantages of Hadoop Data Blocks\n-\nSimple Storage mechanism for datanodes: HDFS blocks simplify the\nstorage of the datanodes. Metadata of all the blocks is maintained by\nnamenode. The datanode doesn’t need to concern about the block\nmetadata like file permissions etc.\n\n\nThank You"
    },
    {
      "filename": "BE_COMP_BDA_Week 3_L9-L12.pdf",
      "path": "data/materials\\Big Data Analytics\\BE_COMP_BDA_Week 3_L9-L12.pdf",
      "text": "Subject Name: Big Data Analytics\nUnit No: 2\nUnit Name: Hadoop HDFS and MapReduce\nFaculty Name\nMr. Tushar G\nMrs.Poonam Gadge\nIndex\n\n\n\n\nFailures\n\n\nAlgebra Operations,\n\n\n\nMapReduce: The Map tasks,\nGrouping by Key, Reduce tasks\n\nMap Reduce\n-\nMapReduce is a programming model suitable for processing of huge data.\n-\nHadoop is capable of running MapReduce programs written in various languages:\nJava, Ruby, Python, and C++.\n-\nMapReduce programs are parallel in nature, thus are very useful for performing\nlarge-scale data analysis using multiple machines in the cluster.\n-\nMapReduce programs work in two phases:\nMap phase\nReduce phase.\n-\nInput to each phase are key-value pairs. In addition, every programmer needs to\nspecify two functions: map function and reduce function.\n\nby key, reduce tasks\n\nCan we solve this real world problem?\n\nUsing Hash Table (<key, Value>) pairs :\nCity Name -Sales\n<key> -<Value>\n\ntasks\nMapReduce: The Map step\n\nInput\nkey-value pairs\nIntermediate\nkey-value pairs\n\nby key, reduce tasks\nMapReduce: The Reduce Step\n\nk\nv\n…\nk\nv\nk\nv\nk\nv\nIntermediate\nkey-value pairs\nk\nv\n…\nk\nv\nk\nv\nv\nv\nv\nKey-value groups\nk\nv\nk\nv\nk\nv\nGroup\nby key\nreduce\nreduce\nOutput\nkey-value pairs\n\nby key, reduce tasks\nMore Specifically\n-\nInput: a set of key-value pairs\n-\nProgrammer specifies two methods:\n-Map(k, v) <k’, v’>*\n-Takes a key-value pair and outputs a set of key-value pairs\n-E.g., key is the filename, value is a single line in the file\n-There is one Map call for every (k,v) pair\n-Reduce(k’, <v’>*) <k’, v’’>*\n-All values v’ with same key k’ are reduced together\nand processed in v’ order\n-There is one Reduce function call per unique key k’\n\n\nby key, reduce tasks\nHow MapReduce works?\n-\nLets understand this with an example –\n-\nConsider you have following input data for your MapReduce Program\nWelcome to Hadoop Class Hadoop is good Hadoop is bad\n\n\nby key, reduce tasks\nMapReduce Example\n\n\nby key, reduce tasks\nMapReduce Example\n\n\nby key, reduce tasks\nPhases of MapReduce\n-\nThe data goes through following phases\n-\nInput Splits:\nInput to a MapReduce job is divided into fixed-size pieces called input splits Input\nsplit is a chunk of the input that is consumed by a single map\n-\nMapping\nThis is very first phase in the execution of map-reduce program. In this phase data\nin each split is passed to a mapping function to produce output values. In our\nexample, job of mapping phase is to count number of occurrences of each word\nfrom input splits (more details about input-split is given below) and prepare a list in\nthe form of <word, frequency>\n\n\nby key, reduce tasks\nPhases of MapReduce\n-\nShuffling\nThis phase consumes output of Mapping phase. Its task is to consolidate the\nrelevant records from Mapping phase output. In our example, same words are\nclubed together along with their respective frequency.\n-\nReducing\nIn this phase, output values from Shuffling phase are aggregated. This phase\ncombines values from Shuffling phase and returns a single output value. In short,\nthis phase summarizes the complete dataset.\n\n\nby key, reduce tasks\nWord Count Using MapReduce\n\nmap(key, value):\n// key: document name; value: text of the document\nfor each word w in value:\nemit(w, 1)\nreduce(key, values):\n// key: a word; value: an iterator over counts\nresult = 0\nfor each count v in values:\nresult += v\nemit(key, result)\n\nby key, reduce tasks\nMapReduce Organizes Work?\n-\nHadoop divides the job into tasks. There are two types of tasks:\nMap tasks (Spilts & Mapping)\nReduce tasks (Shuffling, Reducing)\n-\nThe complete execution process (execution of Map and Reduce tasks, both) is\ncontrolled by two types of entities called as:\n1.\nJobtracker : Acts like a master (responsible for complete execution of\nsubmitted job)\n2.\nMultiple Task Trackers : Acts like slaves, each of them performing the job\n-\nFor every job submitted for execution in the system, there is one Jobtracker that\nresides\non\nNamenode\nand\nthere\nare\nmultiple\ntasktrackers\nwhich\nreside\non Datanode.\n\n\nby key, reduce tasks\nMapReduce Organizes Work?\n\n\nkey, reduce tasks\nMapReduce : Example 2:\n-\nHow many each movie rating exists?\n\n\nkey, reduce tasks\nMapReduce Example 2:\n\n\nkey, reduce tasks\nMapReduce Example 2 : Solution\n\nMAP each input line to (rating,1)\nREDUCE each rating with the sum of all 1’s\n\nkey, reduce tasks\nMapReduce Organizes Work?\n-\nA job is divided into multiple tasks which are then run onto multiple data nodes in a\ncluster.\n-\nIt is the responsibility of jobtracker to coordinate the activity by scheduling tasks to\nrun on different data nodes.\n-\nExecution of individual task is then look after by tasktracker, which resides on every\ndata node executing part of the job.\n-\nTasktracker's responsibility is to send the progress report to the jobtracker.\n-\nIn addition, tasktracker periodically sends 'heartbeat' signal to the Jobtracker so as\nto notify him of current state of the system.\n-\nThus jobtracker keeps track of overall progress of each job. In the event of task\nfailure, the jobtracker can reschedule it on a different tasktracker.\n\n\nby key, reduce tasks\nMapReduce: Example 3: Simple Social Network Analysis\n\n\nby key, reduce tasks\n\nMapReduce: Example 3: Simple Social Network Analysis - Solution\nREDUCE\n\nby key, reduce tasks\nMap Reduce Example 4 – Words of Longest Length in a file\nProblem Statement :\nTo display the words with longest length in an input file.\n\nInput File\n\nby key, reduce tasks\nMapReduce Example 4 : Step 1 : Input Split\n\nInput File\nThis is a sample file has nothing useful\nYet we are using it for example\nSo we shall use it now\nThis is a sample file has nothing useful\nYet we are using it for example\nSo we shall use it now\n\nby key, reduce tasks\nMapReduce Example 4 : Step 2 : Mapping\n\nThis is a sample file\nhas nothing useful\nYet we are using it for\nexample\nSo we shall use it now\n4, This\n2. Is\n1, a\n6, sample\n4, file\n3, has\n7, nothing\n6, useful\n3, Yet\n2. we\n3, are\n5, using\n2, it\n3, for\n7, example\n2, So\n2. we\n3, shall\n3, use\n2, it\n3, now\n\nby key, reduce tasks\nMapReduce Example 4 : Step 3 : Sorting and Shuffling\n\n4, This\n2. Is\n1, a\n6, sample\n4, file\n3, has\n7, nothing\n6, useful\n3, Yet\n2, we\n3, are\n5, using\n2, it\n3, for\n7, example\n2, So\n2. we\n5, shall\n3, use\n2, it\n3, now\n1, a\n2, is\n2. We\n2, it\n2, so\n2, we\n2, it\n3, has\n3, yet\n3, are\n3, for\n3, use\n3, now\n4, this\n4, file\n5, shall\n5, using\n6, useful\n6, sample\n7, nothing\n7, example\n\nby key, reduce tasks\nMapReduce Example 4 : Step 4 : Reducing\n\n1, a\n2, is\n2. We\n2, it\n2, so\n2, we\n2, it\n3, has\n3, yet\n3, are\n3, for\n3, use\n3, now\n4, this\n4, file\n5, shall\n5, using\n6, useful\n6, sample\n7, nothing\n7, example\n7, nothing\n7, example\n\nby key, reduce tasks\nMap Reduce Example 5 – Average length of all words in a text file\n\n\nby key, reduce tasks\n\nCombiners, Coping with Node\nFailuresCase Study\nMapReduce Internal\nInput\nSplit\nSplit\nSplit\nMap\nMap\nMap\nShuffle\nSort\nReduce\nOutput\nHDFS\nInput\nSplit\nSplit\nSplit\nMap\nMap\nMap\nShuffle\nSort\nReduce\nOutput\nData Node 1\nData Node 1\nProblem here is??\nNetwork Traffic\n\n\nMapReduce Internal\n-\nWhen we run the MapReduce job on very large data sets the mapper processes\nand produces large chunks of intermediate output data which is then send\nto Reducer which causes huge network congestion.\n-\nTo increase the efficiency users can optionally specify a Combiner,\nto perform\nlocal aggregation of the intermediate outputs, which helps to cut down the amount\nof data transferred from the Mapper to the Reducer.\n-\nCombiner acts as a mini-reducer. Combiner processes the output of Mapper and\ndoes local aggregation before passing it to the reducer.\n\nMapReduce Example: Word Count\nDeer Beer River\nCar Car River\nDeer Car Beer\nDear Beer River\nCar Car River\nDeer Car Beer\nDeer, 1\nBeer, 1\nRiver, 1\nCar, 1\nCar, 1\nRiver, 1\nDeer, 1\nCar, 1\nBeer, 1\nBeer, 1\nBeer, 1\nCar, 1\nCar, 1\nCar, 1\nDeer, 1\nDeer, 1\nRiver, 1\nRiver, 1\nBeer, 2\nCar, 3\nDeer, 2\nRiver, 2\nBeer, 2\nCar, 3\nDeer, 2\nRiver, 2\nInput\nSplit\nMap\nShuttle/Sort\nReduce\nOutput\nQ: Do you see any place we can improve the efficiency?\nLocal aggregation at mapper will be able to improve\nMapReduce efficiency.\n\n\nMapReduce: Combiner\n-\nCombiner: do local aggregation/combine task at mapper\n-\nQ: What are the benefits of using combiner:\n– Reduce memory/disk requirement of Map tasks\n– Reduce network traffic\n-\nQ: Can we remove the reduce function?\n– No, reducer still needs to process records with same key but from\ndifferent mappers\n-\nQ: How would you implement combiner?\n– It is the same as Reducer!\n\nCar, 1\nCar, 1\nRiver, 1\nCar, 2\nCar, 1\nCar, 3\nCar, 2\nRiver, 1\n\n-\nOften a Map task will produce many pairs of the form (k,v1), (k,v2), … for the\nsame key k\n-E.g., popular words in the word count example\n-\nCan save network time by pre-aggregating values in the mapper:\n-combine(k, list(v1)) v2\n-Combiner is usually same as the reduce function\n-\nWorks only if reduce function is commutative and associative\nRefinement: Combiners\n\n\nRefinement: Combiners\n\n-\nBack to our word counting example:\n-Combiner combines the values of all keys of a single mapper (single\nmachine):\n-Much less data needs to be copied and shuffled!\n\nMap Reduce Example - Maximum temperature for each year from\nweather reports\nProblem : Find the maximum temperature for each year from weather reports\n-\nInput: A set of records with format as:\n<Year/Month, Average Temperature of that\nmonth>\n- (200707,100), (200706,90)\n- (200508, 90), (200607,100)\n- (200708, 80), (200606,80)\n-\nQuestion: Write down the Map and Reduce function to solve this problem\n– Assume we split the input by line\n\n\n\nMap Reduce Example - Maximum temperature for each year from\nweather reports\nInput\nMap\nShuffle/Sort\nReduce\n(200707,100), (200706,90)\n(200508, 90), (200607,100)\n(200708, 80), (200606,80)\n(2007,100), (2007,90)\n(2005, 90), (2006,100)\n(2007, 80), (2006, 80)\n(2007,[100, 90, 80])\n(2006,[100, 80])\n(2005,[90])\n(2005,90)\n(2006,100)\n(2007,100)\n\n\nMap Reduce Example - Maximum temperature for each year from\nweather reports (With Combiners)\nInput\nMap\nShuffle/Sort\nReduce\n(200707,100), (200706,90)\n(200508, 90), (200607,100)\n(200708, 80), (200606,80)\n(2007,100), (2007,90)\n(2005, 90), (2006,100)\n(2007, 80), (2006, 80)\n(2007,[100, 80])\n(2006,[100, 80])\n(2005,[90])\n(2005,90)\n(2006,100)\n(2007,100)\nCombine\n(2007,100)\n(2007, 80), (2006, 80)\n(2005, 90), (2006,100)\n\nMap Reduce Example - Average temperature for each year from weather\nreports (With Combiners)\n-\nKey-Value Pair of Map and Reduce:\n– Map: (year, temperature)\n– Reduce: (year, maximum temperature of the year)\n-\nQuestion: How to use the above Map Reduce program (that contains the\ncombiner) with slight changes to find the average temperature of the\nyear?\n\n\nMapReduce Example: Average Temperature\nInput\nMap\nShuttle/Sort\nReduce\n(200707,100), (200706,90)\n(200508, 90), (200607,100)\n(200708, 80), (200606,80)\n(2007,100), (2007,90)\n(2005, 90), (2006,100)\n(2007, 80), (2006,80)\nCombine\n(2007,95)\n(2007, 80), (2006,80)\n(2005, 90), (2006,100)\n(2007,[95, 80])\n(2006,[100, 80])\n(2005,[90])\n(2005,90)\n(2006,90)\n(2007,87.5)\nReal average of\n2007: 90\nMap Reduce Example - Average temperature for each year from weather\nreports (With Combiners)\n-\nThe problem is with the combiner!\n-\nHere is a simple counterexample:\n– (2007, 100), (2007,90) -> (2007, 95)\n(2007,80)->(2007,80)\n– Average of the above is: (2007,87.5)\n– However, the real average is: (2007,90)\n-\nHowever, we can do a small trick to get around this\n– Mapper: (2007, 100), (2007,90) -> (2007, <190,2>)\n(2007,80)->(2007,<80,1>)\n– Reducer: (2007,<270,3>)->(2007,90)\n\n\nMapReduce Example: Average Temperature\nInput\nMap\nShuttle/Sort\nReduce\n(200707,100), (200706,90)\n(200508, 90), (200607,100)\n(200708, 80), (200606,80)\n(2007,100), (2007,90)\n(2005, 90), (2006,100)\n(2007, 80), (2006,80)\nCombine\n(2007,<190,2>)\n(2007, <80,1>),\n(2006,<80,1>)\n(2005, <90,1>),\n(2006, <100,1>)\n(2007,[<190,2>, <80,1>])\n(2006,[<100,1>, <80,1>])\n(2005,[<90,1>])\n(2005,90)\n(2006,90)\n(2007,90)\n\nAlgorithms using Map Reduce -\nMatrix Vector Multiplication by\nMapReduce\nAlgorithms Using MapReduce (Matrix Multiplication)\n\nWhere, A, B, C, D indicates websites and matrix indicates adjacency matrix for page rank\nV – indicates Initial Vector i.e., It is initial Page rank assigned to every page\nA B C D\nH=\n\n\n\nAlgorithms Using MapReduce (Matrix Multiplication)\n-\nGoogle mainly implemented MapReduce to execute very large Matrix-\nVector multiplications that are needed for PageRank calculations.\n\nAlgorithms Using MapReduce (Matrix Multiplication)\n\nMatrix Multiplication using MapReduce\n\nj\ni\nk\nj\ni * j\nj * k\n2,2\n2,2\n\nMatrix Multiplication using MapReduce (Input File)\nInput File\nA,0,0,1\nA,0,1,2\nA,1,0,3\nA,1,1,4\nB,0,0,5\nB,0,1,6\nB,1,0,7\nB,1,1,8\n\nj\ni\nk\nj\n2,2\n2,2\n\n\n\n\n\n\n\n\n\nMatrix Multiplication using MapReduce (Mapper)\nMap Function\n(key, value)\n(i/k) (Matrix, j, value)\n\nj\ni\nk\nj\n2,2\n2,2\n\n\n\n\n\n\n\n\nInput\nA,0,0,1\nA,0,1,2\nA,1,0,3\nA,1,1,4\nB,0,0,5\nB,0,1,6\nB,1,0,7\nB,1,1,8\nMapper: (i/k) (Matrix, j, value)\n0 (A,0,1)\n0 (A,1,2)\n1 (A,0,3)\n\n(A,1,4)\n0 (B,0,5)\n0 (B,1,7)\n\n(B,0,6)\n1 (B,1,8)\n\nMatrix Multiplication using MapReduce (Grouping and Shuffling)\n\nInput\nA,0,0,1\nA,0,1,2\nA,1,0,3\nA,1,1,4\nB,0,0,5\nB,0,1,6\nB,1,0,7\nB,1,1,8\nMapper => (i/k) (Matrix, j, value)\n0 (A,0,1)\n0 (A,1,2)\n1 (A,0,3)\n\n(A,1,4)\n0 (B,0,5)\n0 (B,1,7)\n\n(B,0,6)\n1 (B,1,8)\nGrouping and Shuffling\n(i, k)\n(0,0) (A,0,1) (A,1,2)\n(B,0,5) (B,1,7)\n(0,1) (A,0,1) (A,1,2)\n(B,0,6) (B,1,8)\n(1,0) (A,0,3) (A,1,4)\n(B,0,5) (B,1,7)\n(1,1) (A,0,3) (A,1,4)\n(B,0,6) (B,1,8)\n\nMatrix Multiplication using MapReduce (Reduce)\n\nInput\nA,0,0,1\nA,0,1,2\nA,1,0,3\nA,1,1,4\nB,0,0,5\nB,0,1,6\nB,1,0,7\nB,1,1,8\nMapper\n0 (A,0,1)\n0 (A,1,2)\n1 (A,0,3)\n\n(A,1,4)\n0 (B,0,5)\n0 (B,1,7)\n\n(B,0,6)\n1 (B,1,8)\nGrouping and Shuffling\n(i, k)\n(0,0) (A,0,1) (A,1,2)\n(B,0,5) (B,1,7)\n(0,1) (A,0,1) (A,1,2)\n(B,0,6) (B,1,8)\n(1,0) (A,0,3) (A,1,4)\n(B,0,5) (B,1,7)\n(1,1) (A,0,3) (A,1,4)\n(B,0,6) (B,1,8)\n\n\n\n\nReduce\n\nThe Map and Reduce Function for Matrix Multiplication\n\nMatr\nMatrix A\nMatrix B\nMap B\nMap A\nShuffling Reducer\n\nMatrix\nA*B\n\nPractice Problem\n\nMatrix A = 2 3\n4 5\nMatrix B = 6 7\n8 9\n1.\n2.\nMatrix A = 1 2 3\n4 5 6\nMatrix B = 1 2\n3 4\n5 6\n\nMatrix A = 1 2\n2 1\n4 2\n3.\nMatrix B = 1 2\n3 4\n\nINPUT FILE\nMAPPER\n(KEY (I/K)\nVALUE(MATIX\n, J,VALUE)\nGROUPING/SHUFFLER\n(I/K)\nA,0,0,1\nA,0,1,2\nA,1,0,2\nA,1,1,1\nA,2,0,4\nA,2,1,2\nB,0,0,1\nB,0,1,2\nB,1,0,3\nB,1,1,4\n0 A,0,1\n0 A,1,2\n1 A,0,2\n1 A,1,1\n2 A,0,4\n2 A,1,2\n0 B,0,1\n0 B,1,3\n1 B,0,2\n1 B,1,4\n0,0 [(A,0,1), (A,1,2)\n(B,0,1), (B,1,3)]\n0,1 [(A,0,1), (A,1,2)\n(B,0,2),(B,1,4)]\n1,0 [(A,0,2) (A,1,1)\n(B,0,1), (B,1,3)]\n1, 1 [[(A,0,2) (A,1,1)\n(B,0,2),(B,1,4)]\n2,0 [(A,0,4), (A,1,2)\n(B,0,1), (B,1,3)]\n2,1 [(A,0,4), (A,1,2)\n(B,0,2),(B,1,4)]\nReduce\n(1*1+2*3)=7\n\n5 8\n10 16\nSolution of 3rd\nExample\n\nRelational Algebra Operations\nMapReduce and Relational Operators\n-\nMapReduce Algorithm can be used for processing relational data:\n-\nShuffle/Sort\nautomatically\nhandles\ngroup\nby\nsorting\nand\npartitioning\nin\nMapReduce.\n-\nThe following operations are performed either in mapper or in reducer:\n1.\nSelection\n2. Projection\n3. Union, Intersection and difference\n4. Natural Join\n5. Grouping and Aggregation\n\n\nRelational Join\n\n\nRelational Join in MapReduce\n\n\nThank You"
    },
    {
      "filename": "BE_COMP_BDA_Week 5_L17-L20.pdf",
      "path": "data/materials\\Big Data Analytics\\BE_COMP_BDA_Week 5_L17-L20.pdf",
      "text": "Big Data Analytics\nWeek 5: Module 3: NoSQL\nFaculty Name :\nMr. Tushar Ghorpade\nMrs. Poonam Gadge\nIndex - Module :3 NoSQL\n\n\n\n\n\n\nfamily(Bigtable) stores,\n\n\n\n\nIntroduction to NoSQL\n\nHistory of Databases\n\n\nFlat File System\nNo Standard\nDefinition\nRelational\nDatabase\nRelational\nDatabases\nCould not handle\nbig data\nNo SQL Databases\nProblem\nSolution\nProblem\nSolution\n-NoSQL database stands for “Not Only SQL” or “NOT SQL”\n-Traditional RDBMS uses SQL syntax and queries to analyze and get the\ndata for further insights.\n-NoSQL is a Database Management System that provides mechanism for\nstorage and retrieval of massive amount of unstructured data in distributed\nenvironment.\nDefinition\n\n\nDatabase Management Systems\nRDBMS\n(Relational)\nOLAP\nNoSQL\n-\nThe concept of NoSQL databases became popular with Internet giants like\nGoogle, Facebook, Amazon, etc. who deal with huge volumes of data. The\nsystem response time becomes slow when you use RDBMS for massive\nvolumes of data.\n-\nTo resolve this problem, we could \"scale up\" our systems by upgrading our\nexisting hardware. This process is expensive.\n-\nThe alternative for this issue is to distribute database load on multiple hosts\nwhenever the load increases. This method is known as \"scaling out.\"\nWhy NoSQL?\n\n\nWhy NoSQL?\n\n\n-Not optimized for horizontal scaling\nData size has increased tremendously to the range of petabytes.\n-Schema-less data\nMajority of data comes in a semi-structured or unstructured format\n-Cost\nHigh licensing cost for data analysis\n-High Velocity of data ingestion\nRDBMS lacks in high velocity because it is designed for steady data\nretention rather than rapid growth\nFurther Challenges with Traditional RDBMS\n\n\nPerformance\n\n\nDatabase Management Systems\nRDBMS\n(Relational)\nOLAP\nNoSQL\nMore Functionality\nLess Performance\nLess Functionality\nLess Performance\nPerformance\n\n\nDatabase Management Systems\nRDBMS\n(Relational)\nOLAP\nNoSQL\nStructured Data\nStructured Data or\nUnstructured Data\nTables\nCubes\nCollections\n1998- Carlo Strozzi use the term NoSQL for his lightweight, open-source\nrelational database\n2000- Graph database Neo4j is launched\n2004- Google BigTable is launched\n2005- CouchDB is launched\n2007- The research paper on Amazon Dynamo is released\n2008- Facebooks open sources the Cassandra project\n2009- The term NoSQL was reintroduced\nBrief History of NoSQL\n\n\n1. Non-relational\n-NoSQL databases never follow the relational model\n-Never provide tables with flat fixed-column records\n-Work with self-contained aggregates or BLOBs\n-Doesn't require object-relational mapping and data normalization\n-No complex features like query languages, query planners, referential\nintegrity joins, ACID (Atomicity, Consistency, Isolation and Durability).\nFeatures of NoSQL\n\n\n2. Scehma-free\n-NoSQL databases are either schema-free or have relaxed schemas\n-Do not require any sort of definition of the schema of the data\n-Offers heterogeneous structures of data in the same domain\nFeatures of NoSQL\n\n\n3.Simple API\n-\nOffers easy to use interfaces for storage and querying data provided\n-\nAPIs allow low-level data manipulation & selection methods\n-\nText-based protocols mostly used with HTTP REST with JSON\n-\nMostly used no standard based query language\n-\nWeb-enabled databases running as internet-facing services\nFeatures of NoSQL\n\n\n4. Distributed\n-\nMultiple NoSQL databases can be executed in a distributed fashion\n-\nOffers auto-scaling and fail-over capabilities\n-\nOften ACID concept can be sacrificed for scalability and throughput\n-\nShared Nothing Architecture. This enables less coordination and higher\ndistribution.\nFeatures of NoSQL\n\n\nCAP Theorem, BASE\nProperties, NoSQL Business\nDrivers\n\n-\nCAP theorem is also called brewer's theorem. It states that is impossible\nfor a distributed data store to offer more than two out of three guarantees\n1. Consistency\n2. Availability\n3. Partition Tolerance\nConsistency: The data should remain consistent even after the execution of\nan operation. This means once data is written, any future read request should\ncontain that data. For example, after updating the order status, all the clients\nshould be able to see the same data.\nWhat is CAP theorem?\n\n\nAvailability:\nThe database should always be available and responsive. It should not have\nany downtime.\nPartition Tolerance:\nPartition Tolerance means that the system should continue to function even if\nthe communication among the servers is not stable. For example, the servers\ncan be partitioned into multiple groups which may not communicate with each\nother. Here, if part of the database is unavailable, other parts are always\nunaffected.\nWhat is CAP Theorem?\n\n\nNoSQL databases are meant for distributed storage\nCAP Theorem\n\n\nDuplicate Copy of same data is maintained on Multiple Machines. This\nincreases availability, but decreases consistency\nCAP Theorem\n\n\nIf duplicate copy of same data is not maintained, consistency is superior But\navailability decreases.\nCAP Theorem\n\n\nIf data on one machine changes, the update propagates to the other machine,\nsystem is inconsistent, but will become eventually consistent.\nCAP Theorem\n\n\n-\nThe term \"eventual consistency\" means to have copies of data on\nmultiple machines to get high availability and scalability. Thus, changes\nmade to any data item on one machine has to be propagated to other\nreplicas.\n-\nData replication may not be instantaneous as some copies will be updated\nimmediately while others in due course of time.\n-\nThese copies may be mutually, but in due course of time, they become\nconsistent. Hence, the name eventual consistency.\nEventual Consistency\n\n\nCAP Theorem\n\n\nPick\n\nAvailability\nConsistency\nPartition\nTolerance\nThe system works well\ndespite physical\nnetwork partition\nEach client has always\nread and write\nAll clients always have\nthe same view of the\ndata\nBASE:\nBasically\nAvailable,\nSoft\nstate, Eventual consistency\nBasically, available means DB is\navailable all the time as per CAP\ntheorem\nSoft state means even without an\ninput;\nthe\nsystem\nstate\nmay\nchange\nEventual consistency means that\nthe system will become consistent\nover time\nBASE – in NoSQL Systems\n\n\nNoSQL business drivers\n\n-\nVolume\n-\nVelocity\n-\nVariability\n-\nAgility\n\nNoSQL business drivers\nVolume:\nThere are two ways to look into data\nprocessing to improve performance\n-\nIf the key factor is only speed, a\nfaster processor could be used.\n-\nIf the processing involves complex\ncomputations, GPU could be used\nalong with the CPU.\n-\nBut the volume of data is limited to\non board GPU memory\n\n\nNoSQL business drivers\n\nVolume:\n-The main reason for organizations to\nlook at an alternative to their current\nRDBMS’s is the need to query big data\n-The need to horizontal scaling made\norganizations\nto\nmove\nfrom\nserial\nto\ndistributed parallel processing where big\ndata is fragmented and processed using\ncluster of commodity machines.\n-This\nis\nmade\npossible\nby\nthe\ndevelopment of technologies like Apache\nHadoop, MapR ,Hbase etc.\n\nNoSQL business drivers\n\nVelocity\n-Many\nsingle-processor\nRDBMSs\nare\nunable to keep up with the demands of\nreal-time inserts and online queries to the\ndatabase made by public-facing websites.\n-RDBMS\nfrequently\nindex\nmany\ncolumns of every new row, a process\nwhich decreases system performance.\n-When single-processor RDBMSs are\nused as a back end to a web store front,\nthe random bursts in web traffic slow\ndown response for everyone, and tuning\nthese systems can be costly when both\nhigh read and write throughput is desired.\n\nNoSQL business drivers\n\nVariability\n- Companies that want to capture and\nreport on exception data struggle when\nattempting to use rigid database schema\nstructures imposed by RDBMSs.\n- For example, if a business unit wants\nto capture a few custom fields for a\nparticular customer, all customer rows\nwithin the database need to store this\ninformation even though it doesn’t apply.\n- Adding new columns to an RDBMS\nrequires the system be shut down and\nALTER TABLE commands to be run.\nWhen a database is large, this process\ncan impact system availability, costing\ntime and money.\n\nNoSQL business drivers\n\nAgility\n-The\nmost\ncomplex\npart\nof\nbuilding\napplications\nusing\nRDBMSs\nis\nthe\nprocess of putting data into and getting\ndata out of the database.\n-If your data has nested and repeated\nsubgroups of data structures, you need\nto include an object-relational mapping\nlayer.\n-The responsibility of this layer is to\ngenerate the correct combination of\nINSERT,\nUPDATE,\nDELETE,\nand\nSELECT\nSQL\nstatements\nto\nmove\nobject data to and from the RDBMS\npersistence layer.\n\nNoSQL business drivers\n\nAgility\n-This process isn’t simple and is associated\nwith the largest barrier to rapid change\nwhen\ndeveloping\nnew\nor\nmodifying\nexisting applications.\n-Generally,\nobject-relational\nmapping\nrequires experienced software developers\nwho\nare\nfamiliar\nwith\nobject-relational\nframeworks such as Java Hibernate (or\nNHiber-nate for .Net systems).\n-Even\nwith\nexperienced\nstaff,\nsmall\nchange requests can cause slowdowns in\ndevelopment and testing schedules.\n\nNoSQL Data Architecture\nPatterns: Key-Value stores,\nColumn Family stores,\nDocument Stores\n\nTypes of NoSQL Databases\n\n\n-\nRelational databases generally strive toward normalization: making sure\nevery piece of data is stored only once.\nTypes of NoSQL databases\n\n\nTraditional relational databases are row-oriented, with each row having a row-id\nand each field within the row stored together in a table.\nTypes of NoSQL databases : Column-Oriented Database\n\n\n-\nEvery time you look something up in a row-oriented database, every row is\nscanned, regardless of which columns you require. Let’s say you only want a list\nof birthdays in September. The database will scan the table from top to bottom\nand left to right\n\n\nTypes of NoSQL databases : Column-Oriented Database\n-\nColumn databases store each column separately, allowing for quicker scans\nwhen only a small number of columns are involved\nTypes of NoSQL databases : Column-Oriented Database\n\n\nWhen should you use a row-oriented database and when should you use a\ncolumn-oriented database?\nIn a column-oriented database it’s easy to add another column\nbecause none of the existing columns are affected by it. But adding an\nentire record requires adapting all tables. This makes the row-oriented\ndatabase\npreferable\nover\nthe\ncolumn-oriented\ndatabase\nfor\nonline\ntransaction processing (OLTP) because this implies adding or changing\nrecords constantly.\nTypes of NoSQL databases : Column-Oriented Database\n\n\n-\nColumn Family Store:\n\nApache Hbase\n\nFacebook’s Cassandra\n\nHypertable\n\nGoogle BigTable\nTypes of NoSQL databases : Column-Oriented Database\n\n\n-\nKey-value stores are the least complex of the NoSQL databases. They are,\nas the name suggests, a collection of key-value pairs.\n-\nThis simplicity makes them the most scalable of the NoSQL database types,\ncapable of storing huge amounts of data.\nTypes of NoSQL databases : Key-Value Stores\n\n\n-\nThe value in a key-value store can be anything: a string, a number, but also\nan entire new set of key-value pairs encapsulated in an object. Figure,\nshows a slightly more complex key value nested structure.\nTypes of NoSQL databases : Key-Value Stores\n\n\nExamples:\n\nRedis\n\nVoldemort\n\nRiak\n\nAmazon’s Dynamo\nDocument Stores, Graph\nStores\n\n-\nDocument stores are one step up in complexity from key-value stores.\n-\nDocument stores appear the most natural among the NoSQL database\ntypes because they’re designed to store everyday documents as is, and\nthey allow for complex querying and calculations on this often already\naggregated form of data.\n-\nThe way things are stored in a relational database makes sense from a\nnormalization point of view: everything should be stored only once and\nconnected\nvia\nforeign\nkeys.\nDocument\nstores\ncare\nlittle\nabout\nnormalization as long as the data is in a structure that makes sense.\nTypes of NoSQL databases : Document Stores\n\n\n-\nNewspapers or magazines, for example, contain articles. To store these in a\nrelational database, you need to chop them up first: the article text goes in\none table, the author and all the information about the author in another,\nand comments on the article when published on a website go in yet another.\n-\nExamples of document stores are MongoDB and CouchDB.\nTypes of NoSQL databases : Document Stores\n\n\nTypes of NoSQL databases : Document Stores\n\n\nTypes of NoSQL databases : Document Stores\n\n\nTypes of NoSQL databases : Document Stores\n\n\nTypes of NoSQL databases : Document Stores\n\n\ngame::1\n{\n“name”:”Pokemon Red”,\n“price”:”29.99”\n}\ngame::2\n{\n“name”:”Super Smash Bros.”\n“price”:”49.99”\n}\nTypes of NoSQL databases : Document Stores\n\n\nperson::agupta\n{\n“first_name”:”Arun”,\n“last_name”:”Gupta”\n“email”:”arun@test.com”\n}\nTypes of NoSQL databases : Document Stores\n\n\ntransaction::1\n{\n“order_number”:”1234”\n“date”:”07/08/2016”\n“person_id”:”person::nraboy””\n“game_id”:”game::1”\n“quantity”:”1”\n}\ntransaction::2\n{\n“order_number”:”1234”\n“date”:”07/08/2016”\n“person_id”:”person::nraboy””\n“game_id”:”game::2”\n“quantity”:”1”\n}\nTypes of NoSQL databases : Document Stores\n\n\ntransaction::1\n{\n“order_number”:”1234”\n“date”:”07/08/2016”\n“person_id”:”person::nraboy””\n“game_id”:”game::1”\n“quantity”:”1”\n}\ntransaction::2\n{\n“order_number”:”1234”\n“date”:”07/08/2016”\n“person_id”:”person::nraboy””\n“game_id”:”game::2”\n“quantity”:”1”\n}\nTypes of NoSQL databases : Document Stores\n\n\nEmbedded\n-\nThe last big NoSQL database type is the most complex one, geared toward\nstoring relations between entities in an efficient manner.\n-\nWhen the data is highly interconnected, such as for social networks,\nscientific paper citations, or capital asset clusters, graph databases are the\nanswer.\n-\nGraph or network data has two main components:\nNode: The entities themselves. In a social network this could be people.\nEdge:\nThe\nrelationship\nbetween\ntwo\nentities.\nThis\nrelationship\nis\nrepresented by a line and has its own properties. An edge can have a\ndirection, for example, if the arrow indicates who is whose boss.\nTypes of NoSQL databases : Graph Databases\n\n\n-\nGraphs can become incredibly complex given enough relation and entity\ntypes. Figure already shows that complexity with only a limited number of\nentities. Graph databases like Neo4j also claim to uphold ACID, whereas\ndocument stores and key-value stores adhere to BASE.\nTypes of NoSQL databases : Graph Databases\n\n\nThank You"
    }
  ]
}